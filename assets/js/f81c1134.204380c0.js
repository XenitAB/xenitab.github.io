"use strict";(globalThis.webpackChunkhome=globalThis.webpackChunkhome||[]).push([[8130],{7735(e){e.exports=JSON.parse('{"archive":{"blogPosts":[{"id":"/2022/11/21/golang-memory-dump","metadata":{"permalink":"/blog/2022/11/21/golang-memory-dump","source":"@site/blog/2022-11-21-golang-memory-dump.md","title":"Profiling Go in Kubernetes","description":"Profiling Go in Kubernetes using Kubernetes debug.","date":"2022-11-21T00:00:00.000Z","tags":[{"inline":true,"label":"pprof","permalink":"/blog/tags/pprof"},{"inline":true,"label":"kubernetes","permalink":"/blog/tags/kubernetes"},{"inline":true,"label":"debug","permalink":"/blog/tags/debug"}],"readingTime":6.16,"hasTruncateMarker":true,"authors":[{"name":"Edvin Norling","title":"Expert DevOps Engineer","url":"https://github.com/nissessenap","email":"edvin.norling@xenit.se","imageURL":"https://media-exp1.licdn.com/dms/image/C5603AQEtMiyg5yOAqQ/profile-displayphoto-shrink_800_800/0/1580133585786?e=1673481600&v=beta&t=SXFrHWYPkM2jpaKESqhdVIQix65MQP1slsoTBXGOmrY","key":"nissessenap","page":null}],"frontMatter":{"title":"Profiling Go in Kubernetes","description":"Profiling Go in Kubernetes using Kubernetes debug.","authors":"nissessenap","tags":["pprof","kubernetes","debug"],"keywords":["pprof","kubernetes","debug"]},"unlisted":false,"nextItem":{"title":"Improving XKS security using Starboard","permalink":"/blog/2022/05/04/starboard"}},"content":"import useBaseUrl from \'@docusaurus/useBaseUrl\';\\n\\nIn Kubernetes 1.23 the new ephemeral containers API went in to beta and in 1.25 it became stable.\\nEphemeral containers or debug containers as it is also known as, makes it\'s possible to inject a container into an already running pod without restarting it.\\nThis is very useful when you want to debug your application since the ephemeral container can provide tools that you don\'t want in your application container.\\n\\nIn this post I thought we could go trough how to profile a running container in Kubernetes.\\n\\n\x3c!-- truncate --\x3e\\n\\n## Application\\n\\nTo get started lets use a very simple test [application](https://github.com/polarsignals/pprof-example-app-go) written by the team over at [Polar Signals](https://www.polarsignals.com/).\\n\\nThey have been kind enough to publish a container image and a Kubernetes deployment that we will use.\\nThe application is built in such a way that it will continue to use memory and the Kubernetes yaml don\'t contain any request nor limit so don\'t run this for a long time or your system will probably OOM.\\n\\nRun the application on Kubernetes 1.23 or higher.\\n\\n```shell\\nkubectl apply -f https://raw.githubusercontent.com/polarsignals/pprof-example-app-go/main/manifests/deployment.yaml\\n```\\n\\nFor starters lets have a look at the pprof http endpoint using curl.\\nA simple way of doing so is to create a Kubernetes service and reach the pod from another container.\\n\\n```shell\\n# Expose the pprof deployment\\nkubectl expose deployment pprof-example-app-go --port=8080\\n```\\n\\nCreate a curl pod and look at the data inside our app.\\n\\n```shell\\nkubectl run -i -t curl --image=curlimages/curl:latest /bin/sh\\n```\\n\\nYou will be sent directly in to the container and you can run something like:\\n\\n```shell\\ncurl http://pprof-example-app-go:8080/debug/pprof/allocs?debug=1\\n```\\n\\nThis will show you an output that look something like this:\\n\\n```shell\\nheap profile: 4: 47357952 [5473: 554303632] @ heap/1048576\\n1: 46882816 [1: 46882816] @ 0x697585 0x470ce1\\n#\\t0x697584\\tmain.allocMem+0xa4\\t/home/brancz/src/github.com/polarsignals/pprof-example-app-go/main.go:65\\n\\n1: 212992 [1: 212992] @ 0x4e3b6e 0x4e3e6c 0x6974c5 0x470ce1\\n#\\t0x4e3b6d\\tlog.(*Logger).Output+0x38d\\t/usr/local/go/src/log/log.go:180\\n#\\t0x4e3e6b\\tlog.Println+0x6b\\t\\t/usr/local/go/src/log/log.go:329\\n#\\t0x6974c4\\tmain.calculateFib+0xc4\\t\\t/home/brancz/src/github.com/polarsignals/pprof-example-app-go/main.go:55\\n\\n1: 204800 [1: 204800] @ 0x4fb68f 0x4fb256 0x503d3d 0x502c97 0x4f6d5e 0x4f6c92 0x4d22a5 0x4d2625 0x4d70b1 0x4cf3d2 0x4e3e3f 0x6974c5 0x470ce1\\n#\\t0x4fb68e\\tmath/big.nat.make+0x5ee\\t\\t/usr/local/go/src/math/big/nat.go:69\\n#\\t0x4fb255\\tmath/big.nat.sqr+0x1b5\\t\\t/usr/local/go/src/math/big/nat.go:595\\n```\\n\\nSadly this output isn\'t the easiest to read so why not use pprof and while we are at it why not use ephemeral containers.\\n\\n## Debug container/ephemeral container\\n\\nMy original plan for this blog post was to attach a new volume to the ephemeral container so we could save data locally and then copy it out to our client and show some nice flame graphs. But apparently it\'s not supported to attach volumes to the ephemeral container.\\nThe ephemeral container cannot even reach the existing volumes on the pod you attach to.\\n\\nThere is an open [issue](https://github.com/kubernetes/kubectl/issues/1071) to solve this but it\'s not part of the current enhancement [proposal](https://github.com/kubernetes/enhancements/issues/1441) so this is nothing that we will see in the near future.\\n\\nSo instead I will just show how we can debug using pprof from within the container.\\nSince we exposed the endpoint through a service we could of course do this from another pod as well.\\nBut in general you should be very restrictive of what traffic that can reach your pprof endpoint if you expose it at all.\\n\\nSo finally time to use the `kubectl debug` command.\\n\\nThe debug command is used on a specific pod, in my case it\'s `pprof-example-app-go-7c4b6d77d-xw52p`.\\nLet\'s attach a standard golang container to our running pod, in this case i choose golang 1.15 to match the pprof tool with the running application.\\n\\nKubernetes will attach the container for you and give you a shell.\\n\\n```shell\\nkubectl debug -i -t pprof-example-app-go-7c4b6d77d-xw52p --image=golang:1.15-alpine3.14 -- /bin/sh\\n```\\n\\nNow we can point on localhost using pprof.\\n\\n```shell\\ngo tool pprof http://localhost:8080/debug/pprof/allocs\\n```\\n\\nThis will provide you with a pprof terminal inside the container.\\n\\nI\'m no pprof pro but there are some easy commands to get you started.\\n`top 10 -cum` will show you the resource consumption it takes to call a function including all function it calls\\n\\n```pprof\\ntop 10 -cum\\nShowing nodes accounting for 27.63GB, 99.80% of 27.69GB total\\nDropped 26 nodes (cum <= 0.14GB)\\nShowing top 10 nodes out of 17\\n      flat  flat%   sum%        cum   cum%\\n         0     0%     0%    25.66GB 92.69%  main.calculateFib\\n   25.59GB 92.43% 92.43%    25.59GB 92.43%  math/big.nat.make (inline)\\n         0     0% 92.43%    25.41GB 91.78%  github.com/polarsignals/pprof-example-app-go/fib.Fibonacci\\n         0     0% 92.43%    25.41GB 91.78%  math/big.(*Int).Add\\n         0     0% 92.43%    25.41GB 91.78%  math/big.nat.add\\n    2.02GB  7.30% 99.73%     2.02GB  7.30%  main.allocMem\\n    0.02GB 0.073% 99.80%     0.25GB  0.91%  fmt.Sprintln\\n         0     0% 99.80%     0.25GB  0.91%  log.Println\\n         0     0% 99.80%     0.23GB  0.84%  fmt.(*pp).doPrintln\\n         0     0% 99.80%     0.23GB  0.84%  fmt.(*pp).handleMethods\\n```\\n\\n`top 10 -flat` will show you the resource consumption it takes to call a function excluding all function it calls\\n\\n```pprof\\nActive filters:\\n   ignore=flat\\nShowing nodes accounting for 27.66GB, 99.90% of 27.69GB total\\nDropped 10 nodes (cum <= 0.14GB)\\nShowing top 10 nodes out of 17\\n      flat  flat%   sum%        cum   cum%\\n   25.59GB 92.43% 92.43%    25.59GB 92.43%  math/big.nat.make (inline)\\n    2.02GB  7.30% 99.73%     2.02GB  7.30%  main.allocMem\\n    0.03GB   0.1% 99.83%     0.21GB  0.76%  math/big.nat.itoa\\n    0.02GB 0.073% 99.90%     0.25GB  0.91%  fmt.Sprintln\\n         0     0% 99.90%     0.23GB  0.84%  fmt.(*pp).doPrintln\\n         0     0% 99.90%     0.23GB  0.84%  fmt.(*pp).handleMethods\\n         0     0% 99.90%     0.23GB  0.84%  fmt.(*pp).printArg\\n         0     0% 99.90%    25.41GB 91.78%  github.com/polarsignals/pprof-example-app-go/fib.Fibonacci\\n         0     0% 99.90%     0.25GB  0.91%  log.Println\\n         0     0% 99.90%    25.66GB 92.69%  main.calculateFib\\n```\\n\\nBy looking at the output we can see that it\'s `math/big.nat.make` that takes the most resources.\\n\\nJust for fun I also generated a flame graph using pprof by port-forwarding to the application and running\\n\\n```shell\\ngo tool pprof -http=: http://localhost:8080/debug/pprof/allocs\\n```\\n\\n<img alt=\\"pprof flame\\" src={useBaseUrl(\\"img/assets/blog/pprof_flame.png\\")} />\\n\\n## Cleanup\\n\\nTo cleanup the resources we created run:\\n\\n```shell\\nkubectl delete -f https://raw.githubusercontent.com/polarsignals/pprof-example-app-go/main/manifests/deployment.yaml\\nkubectl delete svc pprof-example-app-go\\nkubectl delete pod curl\\n```\\n\\n## Conclusion\\n\\nThe `kubectl debug` command is extremely useful when you want to debug your application and you\\ndon\'t have access to a shell or the tools that you need in your normal container.\\n\\nIt saves us from having to install unneeded applications in our container which lowers the amount of potential CVE:s and the time it takes to start your container by lower container size.\\nKubectl debug isn\'t perfect and it won\'t work for all your uses cases especially since you can\'t use it to interact with existing volumes but it\'s a great start.\\n\\nWhen it comes to continues profiling it\'s probably better to look at tool specifically written for it like [Grafana Phlare](https://grafana.com/oss/phlare/) or [Parca](https://www.parca.dev/docs/overview).\\nBut using a tool like pprof locally can be a good start. Hopefully we will get time to write a blog about continues profiling in the future."},{"id":"/2022/05/04/starboard","metadata":{"permalink":"/blog/2022/05/04/starboard","source":"@site/blog/2022-05-04-starboard.md","title":"Improving XKS security using Starboard","description":"Xenit evolves Starboard for continuous scanning of production workloads.","date":"2022-05-04T00:00:00.000Z","tags":[{"inline":true,"label":"security","permalink":"/blog/tags/security"},{"inline":true,"label":"kubernetes","permalink":"/blog/tags/kubernetes"},{"inline":true,"label":"starboard","permalink":"/blog/tags/starboard"},{"inline":true,"label":"trivy","permalink":"/blog/tags/trivy"}],"readingTime":6.67,"hasTruncateMarker":true,"authors":[{"name":"Edvin Norling","title":"Expert DevOps Engineer","url":"https://github.com/nissessenap","email":"edvin.norling@xenit.se","imageURL":"https://media-exp1.licdn.com/dms/image/C5603AQEtMiyg5yOAqQ/profile-displayphoto-shrink_800_800/0/1580133585786?e=1673481600&v=beta&t=SXFrHWYPkM2jpaKESqhdVIQix65MQP1slsoTBXGOmrY","key":"nissessenap","page":null}],"frontMatter":{"title":"Improving XKS security using Starboard","description":"Xenit evolves Starboard for continuous scanning of production workloads.","authors":"nissessenap","tags":["security","kubernetes","starboard","trivy"],"keywords":["security","kubernetes","starboard","trivy"]},"unlisted":false,"prevItem":{"title":"Profiling Go in Kubernetes","permalink":"/blog/2022/11/21/golang-memory-dump"},"nextItem":{"title":"Designing RESTful APIs for cloud services","permalink":"/blog/2022/04/25/designing-restful-apis-for-cloud-services"}},"content":"import useBaseUrl from \'@docusaurus/useBaseUrl\';\\n\\nHow sure are you that you have close to 0 critical CVEs in your Kubernetes cluster?\\n\\nJust like for all companies today security is top of mind for Xenit and we try to come with a solution for this question.\\n\\nAt the time we were already scanning our images in our CI/CD pipeline at creation time using Trivy, but what about new CVEs that gets disclosed after the initial image build?\\n\\nThe increasing rates of cyber crime (by some measures, cyber crimes now outnumber all other crimes put together) which makes it harder for companies to protect themselves.\\nThe faster we can fix relatively simple problems like patching a CVE on container level the more secure we will be.\\n\\nXenit is hosting a number of Kubernetes clusters for our customers and we want a quick way of visualizing CVEs on a platform and a per customer basis.\\nWe want to achieve this without having to jumping around to different clusters and run some script to find out the answer to this question.\\n\\n\x3c!-- truncate --\x3e\\n\\n## How can we do better?\\n\\nCould one solution be to scan the container images continuously that are running in the Kubernetes clusters?\\nOr should we just continuously scan the images that are stored in our private image registry? But what about the images that isn\'t our private image registry?\\nWhat about the third-party images that we use from places like [Quay](https://quay.io/) or [Docker hub](https://hub.docker.com/)? Some people would argue that these images are the most important ones to scan.\\n\\nAt Xenit we already have a central monitoring solution that we use to monitor the status of all our clusters why not think of CVEs just like another metric?\\n\\nTo be able to solve all our questions we decided to go with continuously scanning the images that is running in the Kubernetes clusters.\\n\\nAs mentioned earlier we already scan our images in our CI/CD pipeline and there we use [Trivy](https://github.com/aquasecurity/trivy/).\\nSo it was a natural fit for us to got with [Aqua Securitys](https://www.aquasec.com/) [Starboard](https://github.com/aquasecurity/starboard).\\nStarboard is a reporting tool that supports multiple Aqua Security tools like Trivy for vulnerability report, but it also supports [conftest](https://aquasecurity.github.io/starboard/latest/configuration-auditing/pluggable-scanners/conftest/) and [kube-bench](https://github.com/aquasecurity/kube-bench) among others.\\nIn this post we will only focus on the vulnerability reports generated from the image scanning.\\n\\nWhen starting to use Starboard we noticed a few features that where missing and we really needed. Since Starboard is open source we thought: why not help to implement these features?\\n\\n## Improving the ecosystem\\n\\nThe first issue we found was that the Starboard operator is only able to scan the images and show the result as a CR (Custom Resource) but no metrics of how many CVEs we have per container image.\\n\\nAs a part of Xenit Kubernetes Service ([XKS](https://xenit.se/it-tjanster/kubernetes-eng/)) we supply our customers with monthly reports and we wanted to be able to provide them with simple visualization to see the number of critical CVEs on their applications.\\nIt turns out that we weren\'t the only ones that thought missing metrics was a problem and the great people over at Giantswarm had implemented a solution for this called [starboard-exporter](https://github.com/giantswarm/starboard-exporter).\\n\\nAfter helping out to clean up their [Helm chart](https://github.com/giantswarm/starboard-exporter/pull/27) a bit we started to use starboard-exporter in production.\\n\\nThis made it possible for us to start generating metrics but after some time we realized that the data was strange, this was due to duplicate metrics for the same CVE.\\n\\n### Starboard current revisions\\n\\nBy default Starboard generates a vulnerability report per Kubernetes replica set, instead of one per Kubernetes deployment. This can be a good thing to be able to simply compare the number of CVEs between versions of your application.\\nBut when trying to get a overview of the number of CVEs in a Kubernetes cluster it created a few issues.\\n\\nTo solve this we introduced a new environment variable `OPERATOR_VULNERABILITY_SCANNER_SCAN_ONLY_CURRENT_REVISIONS` to Starboard in [#870](https://github.com/aquasecurity/starboard/pull/870) and if set to true it will only create vulnerability report for the latest replica set in a deployment.\\n\\nGreat now we had metrics that we can trust, but wasn\'t the whole point of this work to continuously scan for new CVEs in the cluster?\\n\\n### Vulnerability TTL\\n\\nThe problem was that once a vulnerability report got generated it didn\'t get updated unless the current vulnerability report was deleted and this created issues for long-running deployments.\\nTo solve this we introduce a TTL(Time To Live) for vulnerability reports [#879](https://github.com/aquasecurity/starboard/pull/879) by implementing a new controller.\\nThe controller currently only supports managing TTL for vulnerability reports but could easily add the same feature to other Starboard reports.\\nSetting the following config in the operator `OPERATOR_VULNERABILITY_SCANNER_REPORT_TT=24h` will automatically delete any vulnerability report older then 24 hours.\\n\\nSo now we were able to scan our public images, we could get metrics and updated scans as often as we want. But what about privates repositories?\\n\\nSupport in Trivy for AWS [ECR](https://aws.amazon.com/ecr/) (Elastic Container Registry) have been around for a long time and same thing with starboard.\\nThe problem was until recently there was no support for Azure [ACR](https://azure.microsoft.com/en-us/services/container-registry) (Azure Container Registry).\\n\\nLuckily just as we were thinking of starting to work on this feature someone else from the community did the heavy [lifting](https://github.com/aquasecurity/fanal/pull/371) and added support for Azure in Fanal.\\nFanal is the library that Trivy uses to scan images, and by fixing this together with a number of other commits to both Starboard and Trivy the problem was solved.\\n\\nOr so we thought...\\n\\n### Add ACR support\\n\\nThere had been a PR to add to the possibility of setting a custom label on your Starboard [jobs](https://github.com/aquasecurity/starboard/pull/902) thus giving us the possibility to of using [aad-pod-identity](https://github.com/Azure/aad-pod-identity) per Starboard job.\\nAad-pod-identity makes it possible to talk to Azure from a container without having to worry about passwords so it\'s something we definitely want to use.\\nBut instead of running Starboards vulnerability scan jobs in server mode (which is the default), we are running Starboard in client mode.\\nWe have deployed a separate Trivy instance in our cluster to cache all the images that we scan and this saves us allot of time per image that is used in the cluster.\\n\\nThe problem was that the Trivy helm chart didn\'t support setting custom labels on the Trivy stateful set so we created a [PR](https://github.com/aquasecurity/trivy/pull/1767) to solve it.\\n\\n## Future of XKS security scanning\\n\\nThanks to this we are now able to quickly generate dashboards with the amount of critical CVEs in our clusters for both public and private images.\\nAnd we can easily show them to our customers through our report, for example:\\n<img alt=\\"XKS Overview\\" src={useBaseUrl(\\"img/assets/blog/starboard_vulnerability.png\\")} />\\n\\nShort term the feature I\'m mostly looking forward in Starboard is the possibility to cache the result of scanned images cluster wide.\\nThis would reduce the amount of scans allot especially when it comes to images like Linkerd side-cars.\\nThere is already a design [PR](https://github.com/aquasecurity/starboard/pull/740) open and the discussion is ongoing, so feel free to jump in to the discussion.\\n\\nWe at Xenit love open-source and think it\'s really important to be able to give back to the community when we can.\\nA big thanks to the maintainers over at Aqua Security and Giantswarm for there great job and being extremely helpful getting these new features merged quickly.\\nIt\'s amazing to always be able to stand on the shoulders of giants."},{"id":"/2022/04/25/designing-restful-apis-for-cloud-services","metadata":{"permalink":"/blog/2022/04/25/designing-restful-apis-for-cloud-services","source":"@site/blog/2022-04-25designing-restful-apis-for-cloud-services.md","title":"Designing RESTful APIs for cloud services","description":"This blog post provides guidelines for RESTful APIs for software-as-a-service offerings, with the goal to maximize their life span and support rapid evolution throughout it.\\n","date":"2022-04-25T00:00:00.000Z","tags":[{"inline":true,"label":"api","permalink":"/blog/tags/api"},{"inline":true,"label":"rest","permalink":"/blog/tags/rest"},{"inline":true,"label":"architecture","permalink":"/blog/tags/architecture"}],"readingTime":10.51,"hasTruncateMarker":true,"authors":[{"name":"Anders Qvist","title":"Expert Engineer","url":"https://github.com/bittrance","email":"anders.qvist@xenit.se","imageURL":"https://media-exp1.licdn.com/dms/image/C5603AQETsc7IPBMeZg/profile-displayphoto-shrink_800_800/0/1516275947412?e=1673481600&v=beta&t=zo3AriMbbg_yN4uAFtsvkSL3aPaGiOl5UZLbCa53WV4","key":"andersq","page":null}],"frontMatter":{"title":"Designing RESTful APIs for cloud services","authors":"andersq","description":"This blog post provides guidelines for RESTful APIs for software-as-a-service offerings, with the goal to maximize their life span and support rapid evolution throughout it.\\n","tags":["api","rest","architecture"],"keywords":["api","rest","architecture"]},"unlisted":false,"prevItem":{"title":"Improving XKS security using Starboard","permalink":"/blog/2022/05/04/starboard"},"nextItem":{"title":"Kubernetes Ephemeral Container Security","permalink":"/blog/2022/04/12/ephemeral-container-security"}},"content":"HTTP has become the de-facto standard transport protocol for programmatic communication in software-as-a-service offerings. This mostly entails publishing request-response style APIs. We often refer to these APIs as \\"RESTful APIs\\":\\n\\n> _[When a] request is made via a RESTful API, [the response is] a representation of the state of the resource[.]_ -- https://www.redhat.com/en/topics/api/what-is-a-rest-api\\n\\nLet\'s decrypt that: \\"representation\\" has come to mean JSON, while \\"state\\" refers to those ubiquitous (and unwieldy) relational databases and \\"resource\\" is an object from our domain model.\\n\\nThe literature on API design will exhort you to analyze the problem space and consider your design choices carefully. Indeed, design choices will significantly impact the lifecycle of APIs that are part of a software-as-a-service offering. However, when designing a software-as-a-service API, we do not really know the details of future usage. The goal must therefore be to maximize our freedom to evolve the APIs without having to change the formal or informal contracts that regulate their usage. The API should become a facade behind which we are free to evolve the implementation.\\n\\nThis post proposes and motivates a set of guidelines for RESTful APIs (and by extension their contracts) intended to maximize their life span and support rapid evolution throughout it. The post focuses on organizations that want to provide commercial software-as-a-service offerings, though many of the guidelines have wider application.\\n\\n\x3c!-- truncate --\x3e\\n\\n## Start with the basics\\n\\nThe guidelines below should be considered in addition to established good practice, so if you are new to REST, you may want to start by reading the literature. Here are some good good articles about RESTful API practices:\\n\\n- http://www.restfulwebapis.org/\\n- https://restfulapi.net/rest-api-design-tutorial-with-example/\\n- https://docs.microsoft.com/en-us/azure/architecture/best-practices/api-design\\n\\nNote that these articles and the recommendations are not in total harmony, for example when it comes to the extent to which use cases should be allowed to influence API design.\\n\\nAlso, please remember that HTTP is a very rich transport protocol which provides solutions to many common API needs. For example, [content negotiation](https://tools.ietf.org/html/rfc7231#section-5.3) and [conditional requests](https://tools.ietf.org/html/rfc7232) can help solve various problems.\\n\\n## Practices for rapid evolution\\n\\nAdditionally, there are a number of good practices which are relevant when building software-as-a-service RESTful APIs.\\n\\n### Your API is a collection of nouns\\n\\n_The most central tenet of REST bears repeating: your API is expressed in nouns, each of which is a class of resources. (If expressing your API in terms of nouns feels contrived, you may want to consider an RPC style API instead.) Those resources are queried and manipulated using basic [\\"CRUD\\"](https://developer.mozilla.org/en-US/docs/Glossary/CRUD) operations. A car sharing service might expose `GET /vehicles` for finding available vehicles and an individual vehicle would be `GET /vehicles/:id`._\\n\\n**Motivation**: Focusing on resources reduces the risk of implementation details bleeding into the API, which means that it becomes easier to change the backing implementation. This is akin to Kant\'s [Der ding an sich](https://en.wikipedia.org/wiki/Thing-in-itself), in that we are trying to discover what properties a resource should reasonably have to match the sum of all observations.\\n\\nAn important consequence of realizing a service as a series of nouns is that in order to be able to keep to CRUD operations, we may need to introduce new nouns (i.e. sub-resource), for example giving cars \\"services\\" so that we have `POST /vehicles/:id/services/heater` for activating the car\'s heater. With this design, the developer using the API knows that discovery will be `GET /vehicles/:id/services` and heater status can be checked with `GET /vehicles/:id/services/heater`. \\"CRUD plus noun\\" allows us to builda a contract taxonomy.\\n\\n### The caller is responsible for the use case\\n\\n_The RESTful API concerns itself with effective access to the data model (the \\"resources\\"). Generally speaking, the use case is the caller\'s concern. For example, the caller may be required to combine data from numerous API calls, read more data than it needs and perform its own sorting._\\n\\n**Motivation**: An endpoint that is optimized for one use case will be hard pressed to accommodate a second use case. It will be hard to avoid adding a second similar endpoint, risking divergence. Furthermore, use cases will evolve over time and if too much of processing, filtering and sorting quirks is handled by the endpoint, it is very easy to end up in a situation where we have to spend our time optimizing specific database queries for individual use cases rather than improving the performance for all callers by refactoring storage.\\n\\nProperly implemented RESTful APIs have a good chance of ageing gracefully. By pushing parts of the business logic to the caller (e.g. a batch job or a [backend-for-frontend](https://samnewman.io/patterns/architectural/bff/)) it ensures that the RESTful API can be reused across many use cases.\\n\\nOne of the most common mistakes with RESTful APIs is to treat the backend as a layer that translates API calls into SQL. Under this fallacy, as APIs evolve, their queries grow more complex (joining, sorting and complex mutations are common examples) making it ever harder to maintain response times. An extreme version of this is \\"passthru-sql\\" (e.g. a query parameter like `?filter=\\"username eq \'bittrance\'\\"`). When developers try to follow the precepts of REST but retain an RPC mindset, they frequently create endpoints that allow the caller to pass query fragments that are appended to the resulting database query more or less verbatim.\\n\\n### A published API is an eternal promise\\n\\n_As long as we have paying customers on a particular API, we maintain that API. We may refuse access by new customers and we may cancel entire services, but as long as a customer uses a service, all APIs on that service are maintained. Where an API in use needs to be decommissioned, that is a commercial decision._\\n\\n**Motivation**: There is no good time to decommission an API. Any change you force on a customer will incur costs for that customer, with limited benefit. Furthermore, in many cases your success will come through partners using your APIs to design new services on top of yours. Adding customers to a well-designed multi-tenant service has very low marginal cost, potentially enabling different business models. Strong sun-setting clauses will constrain our partners\' business models.\\n\\nYou may still want to retain the right to decommission APIs in your contracts; sometimes runaway success may incur unacceptable operational costs and you are forced to redesign. Just be aware that regular use of that clause will damage your reputation. Google Ads ability to [regularly decommission](https://developers.google.com/google-ads/api/docs/sunset-dates) their APIs is a strong indicator of its undue marketing power. For a counter-point look at [retiring EC2 Classic](https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-classic-platform.html). It took over 10 years from the decision was made to retire EC2 Classic until AWS decided it was commercially acceptable to evict the last stragglers in late 2022.\\n\\n### Endpoints make no assumptions about the URL space\\n\\n_We frequently use HTTP load balancers (and API gateways) to compose our URL space. They may direct any arbitrary part of that space to a particular process. Thus, `POST /customers` may be one service (which writes to the master database) and `GET /customers/:id` goes to read replica: a particular endpoint or process must not assume that it \\"owns\\" the customer resources, for example by assuming that it will see all writes. Similarly, endpoints should minimize the part of the object model that it requires to present its resource. For example, `GET /users/:id` should not include additional company information in order to be useful, since users and companies may need to be split across different services tomorrow._\\n\\n**Motivation**: Our users will be successful by creating innovative things on top of our APIs. Almost by definition, they will use our APIs in ways we did not forsee, thus creating unexpected loads. Therefore, a large part of evolving a cloud service is about changing how data is partitioned and what storage systems are used. Therefore, it is very important to retain flexibility in this regard.\\n\\nA service typically starts small, as a single process exposing all your endpoints. However, as the service grows in popularity and scope, simple horizontal scaling is often not possible and you need to diversify: you may add new overlapping (micro-)services or you may want to split reads and writes into separate processes (i.e. go [CQRS](https://docs.microsoft.com/en-us/azure/architecture/patterns/cqrs)).\\n\\n### JSON objects are maps\\n\\n_Adding properties to any returned object is considered a non-breaking change. API docs should point out that properties are new. Similarly, an API can start accepting new optional query parameters on the URL or properties in the input body or add HTTP headers in either direction without being considered breaking._\\n\\n**Motivation**: REST fundamentally limits us to CRUD and behavior will be implicit from the resource state. In order to implement new behavior it follows that we will over time introduce new properties which controls that behavior.\\n\\n### Versioning is part of the URI\\n\\n_The URI should contain a version number. In [semver](https://semver.org/) terms, this is a \\"major\\" version and we use it to signal breaking changes. Given that we have the ability to extend input and output (see [JSON objects are maps](#json-objects-are-maps)), it should be possible to accommodate most \\"minor\\" changes within existing APIs. Ideally, resources with different versions have an implicit relation. For example, if we serve both `GET /v1/customers/acme` and `GET /v2/customers/acme`, they refer to the same customer._\\n\\n**Motivation**: Versions in the URI serve two purposes. First, it signals that one resource should be preferred over another. Second, enables us to write new implementations of a service incrementally.\\n\\n### Authorization is based on method + resource\\n\\n_Client authorization should depend only on the HTTP request method and the URI (and in some cases on headers). It should preferably not on depend on the request body and particularly not on the state of the resource._\\n\\n**Motivation**: Ideally, both authentication and authorization should be handled outside of your endpoint. This may be by middleware in your API or by a load balancer or API gateway. Furthermore, having bespoke authorization logic in your endpoints invites security bugs. It is also hard to document and understand for the caller. Loading the underlying resource to know whether the caller is permitted to perform the request risks being expensive - if you deactivate the caller\'s credentials you don\'t want to continue accruing the cost of those calls (or higher: the client may well retry several times). Finally, filtering lists on permission defeats caching.\\n\\nA consequence of this rule is that you should avoid APIs that use permissions as filter criteria for resource listings; everyone who calls `GET /users` should get the same list. If the user list is secret, you introduce a `GET /departments/:id/users` that has only the relevant users. Similarly, if you have restricted parts of a resource, you can make it into a sub-resource, e.g. `GET /users/:id/access_tokens`.\\n\\n### Be tough on clients\\n\\n_Clients are expected to:_\\n\\n- implement HTTP properly. For example, if a resource was missing a correct Content-Type, a client that breaks when this is rectified is at fault.\\n- be reasonably parallel. A client should be able to make thousands of requests in a reasonable time frame. For example, a search operation may return summaries and if the client wants more information, it is expected to request the full object for each returned item.\\n- do local caching. A client that excessively requests resources that has caching directives should be considered as misbehaving.\\n\\n**Motivation**: We are building REST APIs to be used by many different callers and our situation would quickly be untenable if we had to respect quirky clients and inexperienced developers. For example, it is relatively straight-forward to horizontally scale a service that can answer 1 million GET/s, but very tricky to answer one GET request which is supposed to return 1 million entries per second.\\n\\nSomeone may protest that browsers only allow 6-8 concurrent HTTP sessions against one host and that data must therefore be aggregated or pre-processed for clients to be performant. Normally, introducing [HTTP/2.0 multiplexing](https://datatracker.ietf.org/doc/html/rfc7540#section-5) and ensuring observed response times of \\\\<50ms will do the trick just as well."},{"id":"/2022/04/12/ephemeral-container-security","metadata":{"permalink":"/blog/2022/04/12/ephemeral-container-security","source":"@site/blog/2022-04-12-ephemeral-container-security.md","title":"Kubernetes Ephemeral Container Security","description":"Ephemeral containers is a new concept in Kubernetes which allows attaching  containers to already running Pods. It also introduces new security concerns which have to be resolved before it can be enabled.\\n","date":"2022-04-12T00:00:00.000Z","tags":[{"inline":true,"label":"kubernetes","permalink":"/blog/tags/kubernetes"},{"inline":true,"label":"security","permalink":"/blog/tags/security"},{"inline":true,"label":"ephemeral containers","permalink":"/blog/tags/ephemeral-containers"}],"readingTime":9.35,"hasTruncateMarker":true,"authors":[{"name":"Philip Laine","title":"Expert DevOps Engineer","url":"https://github.com/phillebaba/","email":"philip.laine@xenit.se","imageURL":"https://media-exp1.licdn.com/dms/image/C4E03AQG31KQ2xlivRA/profile-displayphoto-shrink_800_800/0/1598524888194?e=1673481600&v=beta&t=lIA0UixRG6tq22FXe9IuxYKjiINqgVgkLw9hWMdqD_w","key":"phillebaba","page":null}],"frontMatter":{"title":"Kubernetes Ephemeral Container Security","description":"Ephemeral containers is a new concept in Kubernetes which allows attaching  containers to already running Pods. It also introduces new security concerns which have to be resolved before it can be enabled.\\n","authors":"phillebaba","tags":["kubernetes","security","ephemeral containers"],"keywords":["kubernetes","security","ephemeral containerss"]},"unlisted":false,"prevItem":{"title":"Designing RESTful APIs for cloud services","permalink":"/blog/2022/04/25/designing-restful-apis-for-cloud-services"},"nextItem":{"title":"Twelve-factor app anno 2022","permalink":"/blog/2022/02/23/12factor"}},"content":"Attempting to debug a Pod and realizing that you can\'t install curl due to security settings has to be a meme at this point. Good security practices are always nice but it often comes at the cost of usability. To the point where some may even solve this problem by installing debug tools into their production images. Shudders.\\n\\n\x3c!-- truncate --\x3e\\n\\n<img src=\\"https://i.imgflip.com/6cczqi.jpg\\" title=\\"made at imgflip.com\\"/>\\n\\nKubernetes has introduced a new concept called [ephemeral containers](https://kubernetes.io/docs/concepts/workloads/pods/ephemeral-containers/) to deal with this problem. Ephemeral containers are temporary containers that can be attached after a Pod has been created. Rejoice! We can now attach a temporary container with all the tools which we desire. While the applications container may have \\"annoying security features\\" like a read only file system the ephemeral container can enjoy all the freedom which writing files entails. I love this feature so I need to upgrade my cluster immediately!\\n\\n## Digging Deeper\\n\\nNow that we have the new feature we can start a ephemeral container in any Pod we like.\\n\\n```shell\\nkubectl run ephemeral-demo --image=k8s.gcr.io/pause:3.1 --restart=Never\\nkubectl debug -it ephemeral-demo --image=busybox:1.28\\n```\\n\\nWe get a shell and life is now much simpler, but wait a minute. This post is not about how to use ephemeral containers, there are enough of those already, but rather the security implications of enabling ephemeral containers. Let\'s have a look at the YAML for the Pod that we created the ephemeral container in.\\n\\n```yaml\\napiVersion: v1\\nkind: Pod\\nmetadata:\\n  name: ephemeral-demo\\nspec:\\n  ...\\n  ephemeralContainers:\\n  - name: debugger-r59b7\\n    image: busybox:1.28\\n    imagePullPolicy: IfNotPresent\\n    stdin: true\\n    terminationMessagePath: /dev/termination-log\\n    terminationMessagePolicy: File\\n    tty: true\\n```\\n\\nInteresting, there is a new field called `ephemeralContainers` in the Pod definition. This new field contains a list of containers similar to `initContainers` and `containers`. It is not identical as there are certain options which are not available, refer to the [API documentation](https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.23/#ephemeralcontainer-v1-core) for more information. It does however allow configuration of the container security context, which could in theory allow a bad actor to escalate the container\'s privileges. This should not affect those of us who use a policy enforcement tool right? The answer is yes and no depending on the tool and version that is being used. It also depends on if you are using policies from the project\'s library or policies developed in house.\\n\\n### OPA Gatekeeper\\n\\n[OPA Gatekeeper](https://github.com/open-policy-agent/gatekeeper) does not require any code changes as all of its policies are written in [rego](https://www.openpolicyagent.org/docs/latest/policy-language/). It\'s sub project [Gateekper Library](https://github.com/open-policy-agent/gatekeeper-library/) does however have to be updated. The library contains an implementation of the common Pod Security Policies. This includes policies like not allowing containers in privileged mode. The issue with the all of the policies is that they currently only check containers specified in `initContainers` and `containers`, analyze the [following](https://github.com/open-policy-agent/gatekeeper-library/blob/275a1628694dcdf9daf5f6dda1373de6af78e7da/library/pod-security-policy/privileged-containers/template.yaml#L49-L55) rego as an example.\\n\\nThe good news is that this is a pretty easy fix, the bad news is that it requires end users to update the policies pulled from the library.\\n\\n### Kyverno\\n\\n[Kyverno](https://github.com/kyverno/kyverno) seems to have resolved the issues faster. Compared to OPA Gatekeeper however it did require a small code change which means that version [1.5.3](https://github.com/kyverno/kyverno/releases/tag/v1.5.3) or later is needed to write policies for ephemeral containers. They have also [updated their policy library](https://github.com/kyverno/policies/pull/241) to include checking ephemeral containers. Kyverno has done a great job solving these issues quickly. It does still require end users to update however.\\n\\n\\n### Pod Security Policies\\n\\n[Pod Security Policies](https://kubernetes.io/docs/concepts/security/pod-security-policy/) used to be the default policy tool for Kubernetes, and a lot of projects have rules based on Pod Security Policies (PSP). However if you are relying on PSP in a modern cluster you should really start looking for other options like OPA Gatekeeper or Kyverno. PSP has been deprecated since Kubernetes v1.21 and will be removed in v1.25.\\n\\nIf PSP is your only policy tool and you are planning to upgrade to v1.23, don\'t. As PSP is deprecated no new features have been added, and that includes policy enforcement on ephemeral containers. Which means that any security context in an ephemeral container is allowed no matter the PSP in the cluster.  The PSP below will have no affect when adding an ephemeral container to a Pod which is privileged.\\n\\n```yaml\\napiVersion: policy/v1beta1\\nkind: PodSecurityPolicy\\nmetadata:\\n  name: default\\nspec:\\n  privileged: false\\n  seLinux:\\n    rule: RunAsAny\\n  supplementalGroups:\\n    rule: RunAsAny\\n  runAsUser:\\n    rule: RunAsAny\\n  fsGroup:\\n    rule: RunAsAny\\n  volumes:\\n  - \'*\'\\n```\\n\\n### RBAC\\n\\nDisallowing ephemeral containers with RBAC could be an option if the feature is not needed and it is not possible to disable the feature completely. The [KEP-277: Ephemeral Containers](https://github.com/kubernetes/enhancements/blob/master/keps/sig-node/277-ephemeral-containers/README.md) state the following about using RBAC to disable ephemeral containers.\\n\\n> Cluster administrators will be expected to choose from one of the following mechanisms for restricting usage of ephemeral containers:\\n> * Use RBAC to control which users are allowed to access the /ephemeralcontainers subresource.\\n> * Write or use a third-party admission controller to allow or reject Pod updates that modify ephemeral containers based on the content of the update.\\n> * Disable the feature using the EphemeralContainers feature gate.\\n\\nRBAC is additive which means that it is not possible to remove permissions from a role. This type of mitigation obviously does not matter if all users a cluster admin, which they should not be, so we assume that new roles are created for the cluster consumers. In this case having a look at the existing roles can be enough to make sure that the subresource `/ephemeralcontainers` is not included in the role.\\n\\n```yaml\\napiVersion: rbac.authorization.k8s.io/v1\\nkind: ClusterRole\\nmetadata:\\n  name: edit\\nrules:\\n- apiGroups:\\n  - \\"\\"\\n  resources:\\n  - pods\\n  - pods/attach\\n  - pods/exec\\n  - pods/portforward\\n  - pods/proxy\\n  verbs:\\n  - create\\n  - delete\\n  - deletecollection\\n  - patch\\n  - update\\n```\\n\\n## Checking Policy Enforcement\\n\\nLet\'s say that you upgraded your cluster and informed all end users of the great new feature. How do you know that the correct policies are enforced in accordance to your security practices. You may have been aware of the API changes and taken the correct precautionary steps. Or you just updated Kyverno and it\'s policies out of pure happenstance. Either way it is good to trust but verify that it is not for example possible to create a privileged ephemeral container. Annoyingly the debug command does not expose any options to set any security context configuration, so we need another option. Ephemeral containers cannot be defined in a Pod when it is created and it can neither be added with an update. We need some other method to create these specific ephemeral containers.\\n\\n> Ephemeral containers are created using a special ephemeralcontainers handler in the API rather than by adding them directly to pod.spec, so it\'s not possible to add an ephemeral container using `kubectl edit`.\\n\\nThe simplest method to add an ephemeral container with a security context to a Pod is to use the Go client. A couple of lines of code can add a new ephemeral container running as privileged or use any other security context setting which is to your liking.\\n\\n```go\\npackage main\\n\\nimport (\\n\\t\\"context\\"\\n\\t\\"fmt\\"\\n\\t\\"os\\"\\n\\n\\tcorev1 \\"k8s.io/api/core/v1\\"\\n\\tmetav1 \\"k8s.io/apimachinery/pkg/apis/meta/v1\\"\\n\\t\\"k8s.io/client-go/kubernetes\\"\\n\\t\\"k8s.io/client-go/tools/clientcmd\\"\\n)\\n\\nfunc main() {\\n\\tif len(os.Args) != 4 {\\n\\t\\tpanic(\\"expected three args\\")\\n\\t}\\n\\tpodNamespace := os.Args[1]\\n\\tpodName := os.Args[2]\\n\\tkubeconfigPath := os.Args[3]\\n\\n\\t// Create the client\\n\\tclient, err := getKubernetesClients(kubeconfigPath)\\n\\tif err != nil {\\n    panic(fmt.Errorf(\\"could not create client: %w\\", err))\\n\\t}\\n\\tctx := context.Background()\\n\\n\\t// Get the Pod\\n\\tpod, err := client.CoreV1().Pods(podNamespace).Get(ctx, podName, metav1.GetOptions{})\\n\\tif err != nil {\\n    panic(fmt.Errorf(\\"could not get pod: %w\\", err))\\n\\t}\\n\\n\\t// Add a new ephemeral container\\n\\ttrueValue := true\\n\\tephemeralContainer := corev1.EphemeralContainer{\\n\\t\\tEphemeralContainerCommon: corev1.EphemeralContainerCommon{\\n\\t\\t\\tName:  \\"debug\\",\\n\\t\\t\\tImage: \\"busybox\\",\\n\\t\\t\\tTTY:   true,\\n\\t\\t\\tSecurityContext: &corev1.SecurityContext{\\n\\t\\t\\t\\tPrivileged:               &trueValue,\\n\\t\\t\\t\\tAllowPrivilegeEscalation: &trueValue,\\n\\t\\t\\t},\\n\\t\\t},\\n\\t}\\n\\tpod.Spec.EphemeralContainers = append(pod.Spec.EphemeralContainers, ephemeralContainer)\\n\\tpod, err = client.CoreV1().Pods(pod.Namespace).UpdateEphemeralContainers(ctx, pod.Name, pod, metav1.UpdateOptions{})\\n\\tif err != nil {\\n    panic(fmt.Errorf(\\"could not add ephemeral container: %w\\", err))\\n\\t}\\n}\\n\\nfunc getKubernetesClients(path string) (kubernetes.Interface, error) {\\n\\tcfg, err := clientcmd.BuildConfigFromFlags(\\"\\", path)\\n\\tif err != nil {\\n\\t\\treturn nil, err\\n\\t}\\n\\tclient, err := kubernetes.NewForConfig(cfg)\\n\\tif err != nil {\\n\\t\\treturn nil, err\\n\\t}\\n\\treturn client, nil\\n}\\n```\\n\\nRun the program and pass the namespace, pod name, and path to a kube config file. We assume that the ephemeral-demo Pod is still running. \\n\\n```shell\\ngo run main.go default ephemeral-demo $KUBECONFIG\\n```\\n\\nIf it completes with no error a privileged ephemeral container should have been added to the Pod. Exec into it and list the host\'s devices to prove that it is a privileged container.\\n\\n```shell\\nkubectl exec -it ephemeral-demo -c debug -- sh\\nls /dev\\n```\\n\\n## Conclusion\\n\\nIf there is one takeaway from this post, it is that any policy tool that has not been updated in the last couple of months will not enforce rules on ephemeral containers. This also includes all policies written in house! It is not enough to update the community policies.\\n\\nSome may argue that this type of oversight is not really an issue. Ephemeral containers can\'t mount [host paths](https://kubernetes.io/docs/concepts/storage/volumes/#hostpath), or access the [hosts namespaces](https://kubernetes.io/docs/concepts/security/pod-security-policy/#host-namespaces). All it can do is set the common container security context. That is a fair comment, because it\'s true. Being able to create a privileged container is however still not ideal, and there are [methods to escalate privileges](https://bishopfox.com/blog/kubernetes-pod-privilege-escalation#Pod3) when this is possible. Either way it is important to be aware of how policies are enforced and the security contexts which are allowed.\\n\\nI am still not sure how much of an issue this will be short term. Cloud providers are currently in the process of rolling out Kubernetes v1.23 in their SaaS offerings. In these solutions it is still a possibility that they chose to disable ephemeral containers. Those rolling their own clusters may have already upgraded to v1.23 and not be aware of the new feature. That is the biggest issue really, that the platform administrator has to be aware of the existence of ephemeral containers. The fact that kubectl does not expose the option to set a security context will make even less people aware that it is still possible to set one with other means. Investing in a security audit 6 months ago will only be valuable as long as the same Kubernetes version is used. Kubernetes is by design **not** secure by default, so each new feature introduced has to be analyzed. The fact that upgrading from Kubernetes v1.22 to v.23 could make your cluster less secure is part of the difficulties of working with Kubernetes, requiring platform administrators to always stay on top of things. The reality is that these types of things are easy to miss, so hopefully this post has helped someone make their cluster a bit more secure."},{"id":"/2022/02/23/12factor","metadata":{"permalink":"/blog/2022/02/23/12factor","source":"@site/blog/2022-02-23-12factor.md","title":"Twelve-factor app anno 2022","description":"The Twelve-factor app methodology turns 10. This blog posts re-evaluates  the original factors against a decade of experience with  software-as-a-service development and the maturing of serverless  development.\\n","date":"2022-02-23T00:00:00.000Z","tags":[{"inline":true,"label":"12-factor","permalink":"/blog/tags/12-factor"},{"inline":true,"label":"devops","permalink":"/blog/tags/devops"},{"inline":true,"label":"serverless","permalink":"/blog/tags/serverless"}],"readingTime":19.23,"hasTruncateMarker":true,"authors":[{"name":"Anders Qvist","title":"Expert Engineer","url":"https://github.com/bittrance","email":"anders.qvist@xenit.se","imageURL":"https://media-exp1.licdn.com/dms/image/C5603AQETsc7IPBMeZg/profile-displayphoto-shrink_800_800/0/1516275947412?e=1673481600&v=beta&t=zo3AriMbbg_yN4uAFtsvkSL3aPaGiOl5UZLbCa53WV4","key":"andersq","page":null}],"frontMatter":{"title":"Twelve-factor app anno 2022","description":"The Twelve-factor app methodology turns 10. This blog posts re-evaluates  the original factors against a decade of experience with  software-as-a-service development and the maturing of serverless  development.\\n","authors":"andersq","tags":["12-factor","devops","serverless"],"keywords":["12-factor","devops","serverless"]},"unlisted":false,"prevItem":{"title":"Kubernetes Ephemeral Container Security","permalink":"/blog/2022/04/12/ephemeral-container-security"}},"content":"[The Twelve-factor app](https://12factor.net/) is a methodology for building software-as-a-service apps that was first formulated by developers associated with Heroku. It\'s been ten years since the first presentation of this methodology. Despite the criticism that it is only applicable to Heroku and similar webapp services, it remains a relevant yard stick for software-as-a-service development. Some of its tenets have been incorporated into Docker and thence into OCI, effectively making them the law of container-land. This blog post looks at each of the twelve factors and tries to evaluate whether they remain relevant or whether they need updating.\\n\\n\x3c!-- truncate --\x3e\\n\\nIn one respect, the criticism of being Heroku-centric is relevant. Heroku (and Google App Engine and similar services) offer a single packaging model which we today might refer to as \\"IaC-less\\": you provide an HTTP server and Heroku runs it for you (or a war file in Google App Engine\'s case). Any non-trivial software-as-a-service offering require composing many apps into a service, where each has a distinct role: authentication, caching, API, serving static files, et.c., with some infrastructure-as-code that describes how these apps are exposed and interconnect. We end up with an \\"app\\" level and a \\"service\\" level and we have to be careful when considering the original text, since it talks only about the app level, but some of its concerns now reside at the service level.\\n\\n## Factor I: Codebase\\n\\n> A codebase is any single repository. [...] The codebase is the same across all deploys, although different versions may be active in each deploy.\\n\\nThis factor is first and foremost an argument for expressing as much as possible of your service as code that can be put under version control. This was probably already a strawman attack ten years ago. Nevertheless, the latest incarnation of mandating version control is [GitOps](https://www.weave.works/technologies/gitops/), namely the idea that your infrastructure maintains itself by reading your version control system and automatically applies changes as necessary - remarkably similar to the original Heroku model.\\n\\n> There is always a one-to-one correlation between the codebase and the app. If there are multiple codebases, it\u2019s not an app \u2013 it\u2019s a distributed system. Each component in a distributed system is an app, and each can individually comply with twelve-factor.\\n\\nThis part of the factor remains relevant at the app level, but modern public cloud providers\' infrastructure-as-code tooling and platforms like Kubernetes allow us to describe a set of apps and their supporting resources (e.g. secrets) as one package or service. Some organizations separate infrastructure-as-code and app code into separate repositories while others keep the app and its supporting IaC together; neither of these can be said unconditionally to be best practice. Similarly, [Single-page apps](https://en.wikipedia.org/wiki/Single-page_application) are often deployed to a [Content Distribution Network](https://www.cloudflare.com/learning/cdn/what-is-a-cdn/), while their backend may be deployed to a public cloud provider or to a [Kubernetes](https://kubernetes.io/) cluster. Whether these should be kept in the same repository or in different repositories depends on how tightly coupled they are.\\n\\nThe 2022 developer considers the relationship between repositories and artifacts carefully. Pertinent aspects include:\\n\\n- branching strategy\\n- continuous integration completeness and run times\\n- continuous delivery pipelines\\n- infrastructure-as-code maintainability\\n- configuration management\\n- automated deployments\\n\\nExpect to reorganize your sources as your apps evolve.\\n\\n## Factor II: Dependencies\\n\\n> A twelve-factor app never relies on implicit existence of system-wide packages. It declares all dependencies, completely and exactly, via a dependency declaration manifest. [...] Twelve-factor apps also do not rely on the implicit existence of any system tools.\\n\\nIn the container and function-as-a-service worlds, this factor has been elevated to fact. These execution environments provide virtually no implicit dependencies.\\n\\nModern apps tend to have more than one dependency declaration manifest, namely its project manifest(s) (e.g. `go.mod` or `package.json`) and a `Dockerfile`. A consequence of this factor is that you should use Docker base images that are as bare-bone as they come, forcing the explicit installation of supporting libs and tools.\\n\\nThe up-to-date interpretation of this factor is that upgrading dependency versions should always be a conscious action. This slightly shifts the interpretation of the original factor\'s \\"exactly\\". The various ecosystems and tool chains (maven, npm, cargo, et.c.) work differently in when they resolve dependencies. Some resolve dependencies when the developer performs a build and some require an explicit \\"upgrade\\" operation to change what goes into a build. It is therefore vital to have a codified workflow for updating dependencies. For example, when using Node.js and npm, a developer should normally do [npm ci](https://blog.npmjs.org/post/171556855892/introducing-npm-ci-for-faster-more-reliable) and only use the traditional `npm install` (or `npm update`) when the intent is to modernize dependencies.\\n\\n> it uses a dependency isolation tool during execution to ensure that no implicit dependencies \u201cleak in\u201d from the surrounding system. The full and explicit dependency specification is applied uniformly to both production and development.\\n\\nOne of the innovations introduced by Docker is that this factor is enforced already at build time, making it easy to ensure uniformity across dev and prod. Run your automated tests with the built container and there is very little space for execution environment differences.\\n\\n## Factor III: Config\\n\\n> An app\u2019s config is everything that is likely to vary between deploys (staging, production, developer environments, etc). The twelve-factor app stores config in environment variables. Env vars are easy to change between deploys without changing any code; unlike config files, there is little chance of them being checked into the code repo accidentally; and unlike custom config files, [...] they are a language- and OS-agnostic standard. [...] A litmus test for whether an app has all config correctly factored out of the code is whether the codebase could be made open source at any moment, without compromising any credentials.\\n\\nThis factor remains mostly relevant as written, but there are some nuances to consider.\\n\\nInfrastructure-as-code tools like Terraform allows us to create files and database entries. Kubernetes allows us to create [ConfigMaps](https://kubernetes.io/docs/concepts/configuration/configmap/) which will be made available to a container as a file. In both cases, the source configuration can be put under version control, any manual edits will be overwritten on the next deploy and the mechanism is easy for a developer to emulate locally. Thus, they achieve the same result as using environment variables by different means.\\n\\nAlso, while environment variables are operations-friendly, they are problematic when writing tests, since they are global state. In ecosystems that default to parallel test execution (e.g. Rust) environment variables cannot be used. Thus, while an environment variable remains the preferred way to accept simple configuration values, a 12-factor app should convert them to internal state as early as possible.\\n\\nThis factor is now obsolete in one respect. As much as possible, secrets (passwords, private keys, et c) should be stored using a secrets management system such as Hashicorp Vault or Azure Key Vault. Particularly where we can rely on the infrastructure to authenticate the calling process (e.g. via a Kubernetes service account) access to secrets will not directly require credentials. The existence of the secret is under version control, but the actual secret content is immaterial.\\n\\nFurthermore, with platforms such as Kubernetes, service discovery means that some aspects need no configuration at all. Additionally, some forms of configuration can better be managed as references between IaC-controlled resources, which removes them from direct configuration management consideration.\\n\\n## Factor IV: Backing services\\n\\n> A backing service is any service the app consumes over the network as part of its normal operation. The code for a twelve-factor app makes no distinction between local and third party services. To the app, both are attached resources, accessed via a URL or other locator/credentials stored in the config. [...] Resources can be attached to and detached from deploys at will.\\n\\nThis factor remains relevant as written. Its current iteration is sometimes referred to as [\\"API first\\"](https://swagger.io/resources/articles/adopting-an-api-first-approach/) which can be described as the principle that all services you create should be able to act as a backing service. More generally, with the advent of [Zero Trust](https://www.crowdstrike.com/cybersecurity-101/zero-trust-security/) and the proliferation of cloud services, the logical end result of this factor is that any service can interact with any other service on the planet.\\n\\nEven your orchestration platform itself is a backing service, not just the services that run inside it. A service can leverage the Kubernetes control plane to run a one-off job or provision cloud resources to serve a new customer.\\n\\nThe original text focuses a lot on relational databases. It is worth pointing out that you can create a service which scalably serves long-lived state without violating the 12 factors: as long as there is a procedure to claim or negotiate access to a particular shard of the state (e.g. backups stored in an Azure storage account), the actual process can remain stateless. Contemporary thinking in this matter is still coloured by software that predates the cloud era (e.g. MySQL, Elasticsearch, RabbitMQ). For an example of what is possible in 2022, we can look at [Loki](https://github.com/grafana/loki).\\n\\n## Factor V: Build, release, run\\n\\n> The twelve-factor app uses strict separation between the build, release, and run stages. The build stage is a transform which converts a code repo into an executable bundle known as a build.\\n\\nThis factor is more or less a prerequisite for developing software-as-a-service in 2022, but we need to complement this factor with a requirement for automating these stages. The maturing of CI/CD software-as-a-service providers such as GitHub, [ACR Tasks](https://docs.microsoft.com/en-us/azure/container-registry/container-registry-tasks-overview) and [Circle CI](https://circleci.com/) means that it is now relatively easy to automate this process.\\n\\nTypically, the build stage will push a container image to some container registry or upload a function-as-a-service zip files to cloud storage.\\n\\n> The release stage takes the build produced by the build stage and combines it with the deploy\u2019s current config\\n\\nThe normal practice today is for a pipeline to push the release to the runtime environment. This is very useful early in the lifecycle of an app since the release process typically evolves with the app. To achieve stronger separation between the build and release, you might want to consider going GitOps. In Kubernetes-land, you would use a service such as [Flux](https://fluxcd.io).\\n\\n## Factor VI: Processes\\n\\n> Twelve-factor processes are stateless and share-nothing. Any data that needs to persist must be stored in a stateful backing service.\\n\\nThis factor remains relevant as written. In the world of REST APIs, this effectively means that we should hold no domain state in memory between HTTP requests - it should always be handed over to a caching service. This is the main enabler for scale-out in a software-as-a-service.\\n\\nAdhering to this rule is also a good way to avoid memory leaks, which tend to plague garbage-collected ecosystems such as Java, Node, Python and Ruby. You will still get leaks after you out-source your caching to Redis, but it will be much easier to measure and the incitement to properly architecture the caching is stronger.\\n\\n## Factor VII: Port binding\\n\\n> The twelve-factor app is completely self-contained and does not rely on runtime injection of a webserver into the execution environment to create a web-facing service. The web app exports HTTP as a service by binding to a port, and listening to requests coming in on that port.\\n\\nThis factor is now standard in containerized scenarios. Widespread adoption of port binding has enabled a whole ecosystem of supporting proxies (e.g. [Envoy](https://www.envoyproxy.io/), [Traefik](https://traefik.io/), [Toxiproxy](https://github.com/Shopify/toxiproxy)) which (ironically) means that a typical app today is often not self-contained, but depends on other containerized apps to perform e.g. authentication and tracing. This is a improves [separation of concerns](https://deviq.com/principles/separation-of-concerns) and consequently, in 2022 we consider this factor at the service level.\\n\\n> The port-binding approach means that one app can become the backing service for another app, by providing the URL to the backing app as a resource handle in the config for the consuming app.\\n\\nThe original text focuses on network protocols such as HTTP and [XMPP](https://xmpp.org/extensions/). In order to become a backing service in 2022, the app should also adhere to an [API contract](https://apievangelist.com/2019/07/15/what-is-an-api-contract/) of some sort, defining the backing service\'s intended role.\\n\\nMany developers implicitly assume that using high-level protocols like HTTP incurs latency. The overhead of a REST call over a local network (e.g. within a cloud provider) is typically 2-4 ms so you need to get a significant number of non-parallelizable requests before this overhead is noticeable over RDBMS operations and latency towards the consumer.\\n\\n## Factor VIII: Concurrency\\n\\n> In the twelve-factor app, processes are a first class citizen. Processes in the twelve-factor app take strong cues from the unix process model for running service daemons. [...] This does not exclude individual processes from handling their own internal multiplexing. But an individual VM can only grow so large (vertical scale), so the [app] must also be able to span multiple processes running on multiple physical machines.\\n\\nThis factor is now more or less written into law. Function-as-a-service platforms typically provide transparent horizontal scaling. In Kubernetes deployments you just give the number of pods you expect.\\n\\nThe mechanics of horizontal scalability is thus addressed in 2022. The central challenge for any successful app is to distribute work across multiple processes and backing services in such a way that it actually achieves meaningful horizontal scalability. The typical RDBMS-backed web app usually has its scalability completely constrained by its backing database, forcing vertical scaling of the RDBMS - a matter of allocating additional CPUs. This is often expensive and tends to yield only marginal improvement.\\n\\nIs short, horizontal scalability is much preferable over vertical scalability but it is strictly a result of software architecture. It is therefore vital to identify early those apps that will actually require significant scale-out so that they can be properly architected.\\n\\nDespite the dominance of the serverless paradigm in the software-as-a-service realm, there is still an overhang from the era of vertical scaling which the Twelve Factor App tries to break with. For example, the virtual machines of Java, Node, Python and Ruby maintain large volumes of reflection metadata and are very reluctant to de-allocate memory, leading to significant inefficiency on scale-out. A new generation of ecosystems, lead by Go and Rust, are more frugal in this respect.\\n\\n## Factor IX: Disposability\\n\\n> The twelve-factor app\u2019s processes are disposable, meaning they can be started or stopped at a moment\u2019s notice. [...] Processes should strive to minimize startup time. [...] Processes shut down gracefully when they receive a SIGTERM signal, [...] allowing any current requests to finish. [...] A twelve-factor app is architected to handle unexpected, non-graceful terminations.\\n\\nThis factor remains relevant as written and remains nearly as elusive today as it was ten years ago. For example, the HTTP server included in Node.js does [not by default perform graceful shutdown](https://blog.dashlane.com/implementing-nodejs-http-graceful-shutdown/) (see also [nodejs issue 2642](https://github.com/nodejs/node/issues/2642)).\\n\\nFulfilling this factor on an existing code base is much harder than it sounds. It means mapping all the (often implicit) state machines involved in the app and coordinating them so that there are no undefined transitions. For example, the database connection pool must be ready before we bring up our HTTP listener and must not shut down until the HTTP listener is down _and_ all in-flight HTTP requests are done. Workers must \\"hand back\\" in-progress work items so that replacement workers do not have to wait for timeout to process those work items. This difficulty is compounded with distributed systems as a conceptual \\"transaction\\" may span more than one app or backing service (e.g. writing to file storage and sending mail), requiring [two-phase commit](https://en.wikipedia.org/wiki/Two-phase_commit_protocol) semantics.\\n\\nThis factor is nevertheless the key to the always-on experience that we take for granted in large cloud services. A service with working graceful shutdown and comprehensive health checks can be updated at any time and builds developer and operations confidence. This can result in significant productivity gains, particularly when combined with automated testing.\\n\\n## Factor X: Dev/prod parity\\n\\n> Historically, there have been substantial gaps between development [...] and production [...], the time gap, the personnel gap [and] the tools gap. [...] The twelve-factor app is designed for continuous deployment by keeping the gap between development and production small.\\n\\nThis factor is now colloquially known as [DevOps](https://aws.amazon.com/devops/what-is-devops/) and remains as relevant as ever, but has still not established itself fully in software-as-a-service development: many production environments are hard to squeeze onto a developer\'s laptop. Generally speaking, the public cloud providers put too little effort into supporting development use cases for their services. Kubernetes goes furthest in this respect: [Kind](https://kind.sigs.k8s.io/) deserves mentioning for its heroic effort to achieve a dev-friendly, multi-node Kuberentes cluster using only Docker.\\n\\nDocker has introduced a borderland where it is possible to develop a container using just Docker Engine for dev environment and still be reasonably confident that it will execute properly in e.g. Kubernetes. Still, some provisioning of backend services is still needed and time is wasted maintaining two different sets of instrumentation. For example, apps often have a Docker Compose file to get developers started, and a Kubernetes manifest for test/prod. When these desynch, nasty surprises can occur at deployment.\\n\\n> The twelve-factor developer resists the urge to use different backing services between development and production\\n\\nMany full-stack development setups includes \\"dev\\" servers whose role is to automatically reload the app as its source code change. Similarly, tools like Docker Desktop and [Tilt](https://tilt.dev/) provide capabilities and environments that are subtly different from e.g. Azure Container instances or Kubernetes. All these will color developers\' choices and risk introducing issues that will not be discovered until late in the process.\\n\\nThe 2022 developer considers both developer experience, continuous integration/delivery/deployment and operability when choosing tools.\\n\\n## Factor XI: Logs\\n\\n> Logs provide visibility into the behavior of a running app. [...] A twelve-factor app never concerns itself with routing or storage of its output stream. It should not attempt to write to or manage logfiles. Instead, each running process writes its event stream, unbuffered, to stdout. [...] Destinations are not visible to or configurable by the app, and instead are completely managed by the execution environment.\\n\\nInterestingly, this factor does not actually advise on the use of logging. Rather it treats them much as pre-contraceptive times viewed children: as something that inevitably accumulates as a result of marriage.\\n\\nThe factor should thus be updated to mandate that an app should be \\"observable\\", meaning that it should volunteer information on its performance and behavior. We normally break this down into logging, metrics, tracing and audit trails. The relative merit of these differ greatly between apps, but all apps should have an observability strategy. Often, the need changes as an app evolve: early in the lifecycle, logging may dominate, but as usage grows, focus shifts to metrics. The apps in a service are considered as a unit and typically provide different observability needs.\\n\\n## Factor XII: Admin processes\\n\\n> One-off admin processes should be run in an identical environment as the regular long-running processes of the app. They run against a release, using the same codebase and config as any process run against that release.\\n\\nThis factor captures a practice that is common in the [Rails](https://rubyonrails.org/) and [Drupal](https://www.drupal.org/) ecosystems, among others. These are based on interpreted languages where it is relatively easy to give scripting or interactive access to the app\'s internals: the main reason is to ensure that database access occurs using the same models that are used during runtime. In compiled languages, this requires a modularization (e.g. making the data model a separate library) that would complicate development.\\n\\nHowever, the factor has two underlying principles which holds even today. First, that tools used to administer an app should be versioned and released with the same discipline as your app is. Second, that operating and upgrading an app is part of its ordinary usage and there is nothing strange with adding endpoints for internal administrative use. Put differently, at least factors I - IV should apply to the app\'s tooling just as it does to the app itself.\\n\\n## What else is there?\\n\\nVarious additional factors have been proposed over the years, for example [Beyond the Twelve-factor app](https://raw.githubusercontent.com/ffisk/books/master/beyond-the-twelve-factor-app.pdf) and [Seven missing factors from the Twelve-factor app](https://www.ibm.com/cloud/blog/7-missing-factors-from-12-factor-applications). The appeal of the original methodology springs from the universality of its factors. These contributions have relevance, but often only for a subsection of all apps that could (should) strive to live up to the original twelve factors. However, there is two aspects that are clearly missing: security and automated testing.\\n\\nThe good people at WhiteHat Security has written a good analysis called [Security and the twelve-factor app](https://www.devopsdigest.com/security-and-12-factor-app-step-1) which analyses each factor from a security perspective and provides recommendations for securing your app. Their main point is that rather than being an additional factor, security needs to permeate all the twelve factors. The charge that security is underrepresented in the original methodology har merit, and the 2022 developer no longer has the luxury of treating security as an afterthought.\\n\\nFinally, much of the benefit of adhering to the methodology is lost without extensive and automated testing. Version control hygiene and horizontal scalability matters little if apps break as soon as they are deployed. Ten years ago, there was still a discussion about whether writing programmatic tests was worth the effort. That discussion is now settled and in 2022, the discussion is about what the automated testing strategy should look like for a particular app or service. The discerning developer considers:\\n\\n- when to apply unit, component, integration and end-to-end tests at app and/or service level\\n- when to use in-memory implementations of backing services\\n- what long-lived testing environments are needed\\n- how much automated [static analysis](https://en.wikipedia.org/wiki/Static_program_analysis) to include\\n\\nThese additions notwithstanding, the Twelve-factor app methodology remains remarkably relevant today. All developers producing software-as-a-service offerings can benefit from adhering to its factors."}]}}')}}]);