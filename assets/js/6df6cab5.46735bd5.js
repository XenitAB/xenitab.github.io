"use strict";(globalThis.webpackChunkhome=globalThis.webpackChunkhome||[]).push([[3314],{8453(e,n,s){s.d(n,{R:()=>o,x:()=>i});var t=s(6540);const r={},a=t.createContext(r);function o(e){const n=t.useContext(a);return t.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function i(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(r):e.components||r:o(e.components),t.createElement(a.Provider,{value:n},e.children)}},9717(e,n,s){s.r(n),s.d(n,{assets:()=>d,contentTitle:()=>i,default:()=>h,frontMatter:()=>o,metadata:()=>t,toc:()=>l});const t=JSON.parse('{"id":"xks/operator-guide/kubernetes/eks","title":"EKS","description":"Xenit Kubernetes Framework supports both AKS and EKS.","source":"@site/docs/xks/operator-guide/kubernetes/eks.md","sourceDirName":"xks/operator-guide/kubernetes","slug":"/xks/operator-guide/kubernetes/eks","permalink":"/docs/xks/operator-guide/kubernetes/eks","draft":false,"unlisted":false,"editUrl":"https://github.com/xenitab/xenitab.github.io/edit/main/docs/xks/operator-guide/kubernetes/eks.md","tags":[],"version":"current","frontMatter":{"id":"eks","title":"EKS"},"sidebar":"docs","previous":{"title":"AKS","permalink":"/docs/xks/operator-guide/kubernetes/aks"},"next":{"title":"Ingress Nginx Retiring","permalink":"/docs/xks/operator-guide/ingress-nginx deprecation/ingress-nginx-retiring"}}');var r=s(4848),a=s(8453);s(8180);const o={id:"eks",title:"EKS"},i=void 0,d={},l=[{value:"Differences",id:"differences",level:2},{value:"Repo structure",id:"repo-structure",level:3},{value:"EKS",id:"eks",level:3},{value:"IRSA",id:"irsa",level:3},{value:"Bootstrap",id:"bootstrap",level:2},{value:"Tenants account peering",id:"tenants-account-peering",level:2},{value:"Update cluster version",id:"update-cluster-version",level:2},{value:"Update the control plane using AWS CLI",id:"update-the-control-plane-using-aws-cli",level:3},{value:"Update the control plane using Terraform",id:"update-the-control-plane-using-terraform",level:3},{value:"Update the nodes",id:"update-the-nodes",level:3},{value:"Command examples",id:"command-examples",level:3},{value:"Break glass",id:"break-glass",level:2},{value:"EKS resources",id:"eks-resources",level:2}];function c(e){const n={a:"a",code:"code",h2:"h2",h3:"h3",li:"li",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,a.R)(),...e.components};return(0,r.jsxs)(r.Fragment,{children:[(0,r.jsx)(n.p,{children:"Xenit Kubernetes Framework supports both AKS and EKS.\nIn this document we will describe how to setup XKF on EKS and how it differs from AKS."}),"\n",(0,r.jsx)(n.h2,{id:"differences",children:"Differences"}),"\n",(0,r.jsx)(n.p,{children:"To setup XKF using EKS you still need an Azure environment."}),"\n",(0,r.jsxs)(n.p,{children:["XKF is heavily relying on Azure AD (AAD) and we have developed our own tool to\nmanage access to our clusters called ",(0,r.jsx)(n.a,{href:"https://github.com/XenitAB/azad-kube-proxy",children:"azad-kube-proxy"}),"."]}),"\n",(0,r.jsx)(n.p,{children:"Our governance solution is still fully located in Azure together with our Terraform state."}),"\n",(0,r.jsx)(n.h3,{id:"repo-structure",children:"Repo structure"}),"\n",(0,r.jsx)(n.p,{children:"This is how an AWS repo structure can look like:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-txt",children:"\u251c\u2500\u2500 Makefile\n\u251c\u2500\u2500 README.md\n\u251c\u2500\u2500 aws-core\n\u2502\xa0\xa0 \u251c\u2500\u2500 main.tf\n\u2502\xa0\xa0 \u251c\u2500\u2500 outputs.tf\n\u2502\xa0\xa0 \u251c\u2500\u2500 variables\n\u2502\xa0\xa0 \u2502\xa0\xa0 \u251c\u2500\u2500 common.tfvars\n\u2502\xa0\xa0 \u2502\xa0\xa0 \u251c\u2500\u2500 dev.tfvars\n\u2502\xa0\xa0 \u2502\xa0\xa0 \u251c\u2500\u2500 prod.tfvars\n\u2502\xa0\xa0 \u2502\xa0\xa0 \u2514\u2500\u2500 qa.tfvars\n\u2502\xa0\xa0 \u2514\u2500\u2500 variables.tf\n\u251c\u2500\u2500 aws-eks\n\u2502\xa0\xa0 \u251c\u2500\u2500 main.tf\n\u2502\xa0\xa0 \u251c\u2500\u2500 outputs.tf\n\u2502\xa0\xa0 \u251c\u2500\u2500 variables\n\u2502\xa0\xa0 \u2502\xa0\xa0 \u251c\u2500\u2500 common.tfvars\n\u2502\xa0\xa0 \u2502\xa0\xa0 \u251c\u2500\u2500 dev.tfvars\n\u2502\xa0\xa0 \u2502\xa0\xa0 \u251c\u2500\u2500 prod.tfvars\n\u2502\xa0\xa0 \u2502\xa0\xa0 \u2514\u2500\u2500 qa.tfvars\n\u2502\xa0\xa0 \u2514\u2500\u2500 variables.tf\n\u251c\u2500\u2500 azure-governance\n\u2502\xa0\xa0 \u251c\u2500\u2500 main.tf\n\u2502\xa0\xa0 \u251c\u2500\u2500 outputs.tf\n\u2502\xa0\xa0 \u251c\u2500\u2500 variables\n\u2502\xa0\xa0 \u2502\xa0\xa0 \u251c\u2500\u2500 common.tfvars\n\u2502\xa0\xa0 \u2502\xa0\xa0 \u251c\u2500\u2500 dev.tfvars\n\u2502\xa0\xa0 \u2502\xa0\xa0 \u251c\u2500\u2500 prod.tfvars\n\u2502\xa0\xa0 \u2502\xa0\xa0 \u2514\u2500\u2500 qa.tfvars\n\u2502\xa0\xa0 \u2514\u2500\u2500 variables.tf\n\u251c\u2500\u2500 global.tfvars\n"})}),"\n",(0,r.jsx)(n.h3,{id:"eks",children:"EKS"}),"\n",(0,r.jsx)(n.p,{children:"Just like in AKS we use Calico as our CNI."}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"AWS CNI does not support network policies"}),"\n",(0,r.jsx)(n.li,{children:"AWS CNI heavily limits how many pods we can run on a single node"}),"\n",(0,r.jsx)(n.li,{children:"We want to be consistent with AKS"}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:"Just after setting up the EKS cluster we use a null_resource to first delete\nthe AWS CNI daemon set and then install calico.\nThis is all done before we add a single node to the cluster."}),"\n",(0,r.jsx)(n.p,{children:"After this we add an EKS node group and Calico starts."}),"\n",(0,r.jsx)(n.h3,{id:"irsa",children:"IRSA"}),"\n",(0,r.jsx)(n.p,{children:"In AKS we use AAD Pod Identity to support access to Azure resources.\nWe support the same thing in EKS but use IAM roles for service accounts IRSA."}),"\n",(0,r.jsxs)(n.p,{children:["To make it easier to use IRSA we have developed a small terraform ",(0,r.jsx)(n.a,{href:"https://github.com/XenitAB/terraform-modules/blob/main/modules/aws/irsa/README.md",children:"module"}),"."]}),"\n",(0,r.jsx)(n.h2,{id:"bootstrap",children:"Bootstrap"}),"\n",(0,r.jsxs)(n.p,{children:["By default AWS CNI limits the amount of pods that you can have on a single node.\nSince we are using Calico we do not have this limit,\nbut when setting up a default EKS environment the EKS ",(0,r.jsx)(n.a,{href:"https://github.com/awslabs/amazon-eks-ami/blob/master/files/bootstrap.sh",children:"bootstrap script"}),"\ndefines a pod limit. To remove this limit we have created our own AWS launch template for our EKS node group. It sets ",(0,r.jsx)(n.code,{children:"--use-max-pods false"}),' and some needed Kubernetes node labels. If these labels are not set the EKS cluster is unable to "find" the nodes in the node group.']}),"\n",(0,r.jsx)(n.h2,{id:"tenants-account-peering",children:"Tenants account peering"}),"\n",(0,r.jsx)(n.p,{children:"In Azure we separates XKF and our tenants by using Resource Groups, in AWS we use separate accounts."}),"\n",(0,r.jsx)(n.p,{children:"To setup a VPC peering you need to know the target VPC id, this creates a chicken and egg problem.\nTo workaround this problem we sadly have to run the eks/core module multiple times on both the XKF side and the tenant side."}),"\n",(0,r.jsx)(n.p,{children:"Run Terraform in the following order:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["XKF core without any ",(0,r.jsx)(n.code,{children:"vpc_peering_config_requester"})," defined."]}),"\n",(0,r.jsxs)(n.li,{children:["Tenant core without any ",(0,r.jsx)(n.code,{children:"vpc_peering_config_accepter"})," defined."]}),"\n",(0,r.jsxs)(n.li,{children:["XKF core defines ",(0,r.jsx)(n.code,{children:"vpc_peering_config_requester"}),", manually getting the needed information from the tenant account."]}),"\n",(0,r.jsxs)(n.li,{children:["Tenant core defines ",(0,r.jsx)(n.code,{children:"vpc_peering_config_accepter"}),", manually getting the needed information from the XKF account."]}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:"Make sure that you only have one peering request open at the same time, else the accepter side will not be able to find a unique request.\nNow you should be able to see the VPC peering connected on both sides."}),"\n",(0,r.jsx)(n.h2,{id:"update-cluster-version",children:"Update cluster version"}),"\n",(0,r.jsxs)(n.p,{children:["Updating the EKS cluster version can not be done by updating Terraform code only, it also involves the AWS CLI and kubectl. Find your EKS version to upgrade to here: ",(0,r.jsx)(n.a,{href:"https://docs.aws.amazon.com/eks/latest/userguide/kubernetes-versions.html",children:"EKS versions"})]}),"\n",(0,r.jsxs)(n.p,{children:["For further information on the AWS CLI commands used in this section, please refer to the ",(0,r.jsx)(n.a,{href:"https://docs.aws.amazon.com/cli/latest/reference/eks/index.html",children:"AWS EKS CLI"})," documentation."]}),"\n",(0,r.jsx)(n.h3,{id:"update-the-control-plane-using-aws-cli",children:"Update the control plane using AWS CLI"}),"\n",(0,r.jsx)(n.p,{children:"Get the name of the cluster to update:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"aws eks list-clusters --region eu-west-1\n"})}),"\n",(0,r.jsx)(n.p,{children:"Update the control plane version by running the following command:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"aws eks update-cluster-version --region eu-west-1 --name <cluster-name> --kubernetes-version <version>\n"})}),"\n",(0,r.jsx)(n.p,{children:"The above command provides an id that can be use to check the status of the update:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"aws eks describe-update --region eu-west-1 --name <cluster-name> --update-id <id>\n"})}),"\n",(0,r.jsxs)(n.p,{children:["The update is finished when status is ",(0,r.jsx)(n.code,{children:"Successful"}),". Previous updates have taken approximately ",(0,r.jsx)(n.strong,{children:"45 minutes"}),"."]}),"\n",(0,r.jsxs)(n.p,{children:["In the ",(0,r.jsx)(n.code,{children:"aws-eks/variables/<environment>.tfvars"})," Terraform file that corresponds to the actual environment, update the ",(0,r.jsx)(n.code,{children:"kubernetes_version"})," in ",(0,r.jsx)(n.code,{children:"eks_config"})," and make a ",(0,r.jsx)(n.code,{children:"terraform plan"}),". No difference in the plan output is expected. Also perform a ",(0,r.jsx)(n.code,{children:"terraform apply"})," just to make sure state the state is updated (might not be needed)."]}),"\n",(0,r.jsx)(n.h3,{id:"update-the-control-plane-using-terraform",children:"Update the control plane using Terraform"}),"\n",(0,r.jsx)(n.p,{children:"TBD"}),"\n",(0,r.jsx)(n.h3,{id:"update-the-nodes",children:"Update the nodes"}),"\n",(0,r.jsxs)(n.p,{children:["In the ",(0,r.jsx)(n.code,{children:"aws-eks/variables/<environment>.tfvars"})," Terraform file that corresponds to the actual environment, add a new node group in ",(0,r.jsx)(n.code,{children:"eks_config"}),". The example below shows a node upgrade from ",(0,r.jsx)(n.code,{children:"1.20"})," to ",(0,r.jsx)(n.code,{children:"1.21"})," where ",(0,r.jsx)(n.code,{children:"standard2"})," is the new node group. The value of ",(0,r.jsx)(n.code,{children:"release_version"})," must match an AMI version (preferrably the latest) for the actual Kubernetes version (can be found in the ",(0,r.jsx)(n.a,{href:"https://docs.aws.amazon.com/eks/latest/userguide/eks-linux-ami-versions.html",children:"EKS Linux AMI versions documentation"}),"):"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-terraform",children:'eks_config = {\n  kubernetes_version = "1.21"\n  cidr_block         = "10.100.64.0/18"\n  node_groups = [\n    {\n      name            = "standard"\n      release_version = "1.20.4-20210621"\n      min_size        = 3\n      max_size        = 4\n      instance_types  = ["t3.large"]\n    },\n    {\n      name            = "standard2"\n      release_version = "1.21.5-20220123"\n      min_size        = 3\n      max_size        = 4\n      instance_types  = ["t3.large"]\n    },\n  ]\n}\n'})}),"\n",(0,r.jsx)(n.p,{children:"When this change is applied, there will be a new set of nodes running the new version added to the cluster. The following command will show all nodes and their versions:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-terraform",children:"kubectl get nodes\n"})}),"\n",(0,r.jsx)(n.p,{children:"Now it is time to drain the old nodes one by one with:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"kubectl drain <node-name> --ignore-daemonsets --delete-emptydir-data\n"})}),"\n",(0,r.jsxs)(n.p,{children:["When all nodes are drained, remove the old node group in ",(0,r.jsx)(n.code,{children:"eks_config"}),". From the example above:"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-terraform",children:'eks_config = {\n  kubernetes_version = "1.21"\n  cidr_block         = "10.100.64.0/18"\n  node_groups = [\n    {\n      name            = "standard2"\n      release_version = "1.21.5-20220123"\n      min_size        = 3\n      max_size        = 4\n      instance_types  = ["t3.large"]\n    },\n  ]\n}\n'})}),"\n",(0,r.jsx)(n.p,{children:"When applied, the old nodes are removed. The update is now complete."}),"\n",(0,r.jsx)(n.h3,{id:"command-examples",children:"Command examples"}),"\n",(0,r.jsx)(n.p,{children:"The following AWS CLI commands are an example of an update from 1.20 to 1.21:"}),"\n",(0,r.jsx)(n.p,{children:"Control plane:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"aws eks list-clusters --region eu-west-1\naws eks update-cluster-version --region eu-west-1  --name qa-eks2  --kubernetes-version 1.21\naws eks describe-update --region eu-west-1 --name qa-eks2 --update-id 25b9f04f-0be3-40ca-bc37-aaf841070012\n"})}),"\n",(0,r.jsx)(n.h2,{id:"break-glass",children:"Break glass"}),"\n",(0,r.jsx)(n.p,{children:"We are very dependent on azad-proxy to work but if something happens with the\ningress, azad-proxy or the AAD we need to have ways of reaching the cluster:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"aws eks --region eu-west-1 update-kubeconfig --name dev-eks1 --alias dev-eks1 --role-arn arn:aws:iam::111111111111:role/xkf-eu-west-1-dev-eks-admin\n"})}),"\n",(0,r.jsx)(n.h2,{id:"eks-resources",children:"EKS resources"}),"\n",(0,r.jsxs)(n.p,{children:["To get a quick overview of what is happening in EKS you can look at its ",(0,r.jsx)(n.a,{href:"https://github.com/awslabs/amazon-eks-ami/blob/master/CHANGELOG.md#changelog",children:"changelog"}),"."]}),"\n",(0,r.jsxs)(n.p,{children:["When upgrading node groups you need to correlate with your Kubernetes release, you can find which node group is available to ",(0,r.jsx)(n.a,{href:"https://docs.aws.amazon.com/eks/latest/userguide/eks-linux-ami-versions.html",children:"which node group"}),"."]}),"\n",(0,r.jsxs)(n.p,{children:["AWS general ",(0,r.jsx)(n.a,{href:"https://aws.amazon.com/security/security-bulletins/",children:"security information"})]}),"\n",(0,r.jsxs)(n.p,{children:["Public ",(0,r.jsx)(n.a,{href:"https://github.com/aws/containers-roadmap",children:"containers roadmap"}),"."]})]})}function h(e={}){const{wrapper:n}={...(0,a.R)(),...e.components};return n?(0,r.jsx)(n,{...e,children:(0,r.jsx)(c,{...e})}):c(e)}}}]);