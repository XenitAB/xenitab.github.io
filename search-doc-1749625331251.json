[{"title":"Improving XKS security using Starboard","type":0,"sectionRef":"#","url":"blog/2022/05/04/starboard","content":"","keywords":"security kubernetes starboard trivy"},{"title":"How can we do better?​","type":1,"pageTitle":"Improving XKS security using Starboard","url":"blog/2022/05/04/starboard#how-can-we-do-better","content":"Could one solution be to scan the container images continuously that are running in the Kubernetes clusters? Or should we just continuously scan the images that are stored in our private image registry? But what about the images that isn't our private image registry? What about the third-party images that we use from places like Quay or Docker hub? Some people would argue that these images are the most important ones to scan. At Xenit we already have a central monitoring solution that we use to monitor the status of all our clusters why not think of CVEs just like another metric? To be able to solve all our questions we decided to go with continuously scanning the images that is running in the Kubernetes clusters. As mentioned earlier we already scan our images in our CI/CD pipeline and there we use Trivy. So it was a natural fit for us to got with Aqua Securitys Starboard. Starboard is a reporting tool that supports multiple Aqua Security tools like Trivy for vulnerability report, but it also supports conftest and kube-bench among others. In this post we will only focus on the vulnerability reports generated from the image scanning. When starting to use Starboard we noticed a few features that where missing and we really needed. Since Starboard is open source we thought: why not help to implement these features? "},{"title":"Improving the ecosystem​","type":1,"pageTitle":"Improving XKS security using Starboard","url":"blog/2022/05/04/starboard#improving-the-ecosystem","content":"The first issue we found was that the Starboard operator is only able to scan the images and show the result as a CR (Custom Resource) but no metrics of how many CVEs we have per container image. As a part of Xenit Kubernetes Service (XKS) we supply our customers with monthly reports and we wanted to be able to provide them with simple visualization to see the number of critical CVEs on their applications. It turns out that we weren't the only ones that thought missing metrics was a problem and the great people over at Giantswarm had implemented a solution for this called starboard-exporter. After helping out to clean up their Helm chart a bit we started to use starboard-exporter in production. This made it possible for us to start generating metrics but after some time we realized that the data was strange, this was due to duplicate metrics for the same CVE. "},{"title":"Starboard current revisions​","type":1,"pageTitle":"Improving XKS security using Starboard","url":"blog/2022/05/04/starboard#starboard-current-revisions","content":"By default Starboard generates a vulnerability report per Kubernetes replica set, instead of one per Kubernetes deployment. This can be a good thing to be able to simply compare the number of CVEs between versions of your application. But when trying to get a overview of the number of CVEs in a Kubernetes cluster it created a few issues. To solve this we introduced a new environment variable OPERATOR_VULNERABILITY_SCANNER_SCAN_ONLY_CURRENT_REVISIONS to Starboard in #870 and if set to true it will only create vulnerability report for the latest replica set in a deployment. Great now we had metrics that we can trust, but wasn't the whole point of this work to continuously scan for new CVEs in the cluster? "},{"title":"Vulnerability TTL​","type":1,"pageTitle":"Improving XKS security using Starboard","url":"blog/2022/05/04/starboard#vulnerability-ttl","content":"The problem was that once a vulnerability report got generated it didn't get updated unless the current vulnerability report was deleted and this created issues for long-running deployments. To solve this we introduce a TTL(Time To Live) for vulnerability reports #879 by implementing a new controller. The controller currently only supports managing TTL for vulnerability reports but could easily add the same feature to other Starboard reports. Setting the following config in the operator OPERATOR_VULNERABILITY_SCANNER_REPORT_TT=24h will automatically delete any vulnerability report older then 24 hours. So now we were able to scan our public images, we could get metrics and updated scans as often as we want. But what about privates repositories? Support in Trivy for AWS ECR (Elastic Container Registry) have been around for a long time and same thing with starboard. The problem was until recently there was no support for Azure ACR (Azure Container Registry). Luckily just as we were thinking of starting to work on this feature someone else from the community did the heavy lifting and added support for Azure in Fanal. Fanal is the library that Trivy uses to scan images, and by fixing this together with a number of other commits to both Starboard and Trivy the problem was solved. Or so we thought... "},{"title":"Add ACR support​","type":1,"pageTitle":"Improving XKS security using Starboard","url":"blog/2022/05/04/starboard#add-acr-support","content":"There had been a PR to add to the possibility of setting a custom label on your Starboard jobs thus giving us the possibility to of using aad-pod-identity per Starboard job. Aad-pod-identity makes it possible to talk to Azure from a container without having to worry about passwords so it's something we definitely want to use. But instead of running Starboards vulnerability scan jobs in server mode (which is the default), we are running Starboard in client mode. We have deployed a separate Trivy instance in our cluster to cache all the images that we scan and this saves us allot of time per image that is used in the cluster. The problem was that the Trivy helm chart didn't support setting custom labels on the Trivy stateful set so we created a PR to solve it. "},{"title":"Future of XKS security scanning​","type":1,"pageTitle":"Improving XKS security using Starboard","url":"blog/2022/05/04/starboard#future-of-xks-security-scanning","content":"Thanks to this we are now able to quickly generate dashboards with the amount of critical CVEs in our clusters for both public and private images. And we can easily show them to our customers through our report, for example:  Short term the feature I'm mostly looking forward in Starboard is the possibility to cache the result of scanned images cluster wide. This would reduce the amount of scans allot especially when it comes to images like Linkerd side-cars. There is already a design PR open and the discussion is ongoing, so feel free to jump in to the discussion. We at Xenit love open-source and think it's really important to be able to give back to the community when we can. A big thanks to the maintainers over at Aqua Security and Giantswarm for there great job and being extremely helpful getting these new features merged quickly. It's amazing to always be able to stand on the shoulders of giants. "},{"title":"Designing RESTful APIs for cloud services","type":0,"sectionRef":"#","url":"blog/2022/04/25/designing-restful-apis-for-cloud-services","content":"","keywords":"api rest architecture"},{"title":"Start with the basics​","type":1,"pageTitle":"Designing RESTful APIs for cloud services","url":"blog/2022/04/25/designing-restful-apis-for-cloud-services#start-with-the-basics","content":"The guidelines below should be considered in addition to established good practice, so if you are new to REST, you may want to start by reading the literature. Here are some good good articles about RESTful API practices: http://www.restfulwebapis.org/https://restfulapi.net/rest-api-design-tutorial-with-example/https://docs.microsoft.com/en-us/azure/architecture/best-practices/api-design Note that these articles and the recommendations are not in total harmony, for example when it comes to the extent to which use cases should be allowed to influence API design. Also, please remember that HTTP is a very rich transport protocol which provides solutions to many common API needs. For example, content negotiation and conditional requests can help solve various problems. "},{"title":"Practices for rapid evolution​","type":1,"pageTitle":"Designing RESTful APIs for cloud services","url":"blog/2022/04/25/designing-restful-apis-for-cloud-services#practices-for-rapid-evolution","content":"Additionally, there are a number of good practices which are relevant when building software-as-a-service RESTful APIs. "},{"title":"Your API is a collection of nouns​","type":1,"pageTitle":"Designing RESTful APIs for cloud services","url":"blog/2022/04/25/designing-restful-apis-for-cloud-services#your-api-is-a-collection-of-nouns","content":"The most central tenet of REST bears repeating: your API is expressed in nouns, each of which is a class of resources. (If expressing your API in terms of nouns feels contrived, you may want to consider an RPC style API instead.) Those resources are queried and manipulated using basic &quot;CRUD&quot; operations. A car sharing service might expose GET /vehicles for finding available vehicles and an individual vehicle would be GET /vehicles/:id. Motivation: Focusing on resources reduces the risk of implementation details bleeding into the API, which means that it becomes easier to change the backing implementation. This is akin to Kant's Der ding an sich, in that we are trying to discover what properties a resource should reasonably have to match the sum of all observations. An important consequence of realizing a service as a series of nouns is that in order to be able to keep to CRUD operations, we may need to introduce new nouns (i.e. sub-resource), for example giving cars &quot;services&quot; so that we have POST /vehicles/:id/services/heater for activating the car's heater. With this design, the developer using the API knows that discovery will be GET /vehicles/:id/services and heater status can be checked with GET /vehicles/:id/services/heater. &quot;CRUD plus noun&quot; allows us to builda a contract taxonomy. "},{"title":"The caller is responsible for the use case​","type":1,"pageTitle":"Designing RESTful APIs for cloud services","url":"blog/2022/04/25/designing-restful-apis-for-cloud-services#the-caller-is-responsible-for-the-use-case","content":"The RESTful API concerns itself with effective access to the data model (the &quot;resources&quot;). Generally speaking, the use case is the caller's concern. For example, the caller may be required to combine data from numerous API calls, read more data than it needs and perform its own sorting. Motivation: An endpoint that is optimized for one use case will be hard pressed to accommodate a second use case. It will be hard to avoid adding a second similar endpoint, risking divergence. Furthermore, use cases will evolve over time and if too much of processing, filtering and sorting quirks is handled by the endpoint, it is very easy to end up in a situation where we have to spend our time optimizing specific database queries for individual use cases rather than improving the performance for all callers by refactoring storage. Properly implemented RESTful APIs have a good chance of ageing gracefully. By pushing parts of the business logic to the caller (e.g. a batch job or a backend-for-frontend) it ensures that the RESTful API can be reused across many use cases. One of the most common mistakes with RESTful APIs is to treat the backend as a layer that translates API calls into SQL. Under this fallacy, as APIs evolve, their queries grow more complex (joining, sorting and complex mutations are common examples) making it ever harder to maintain response times. An extreme version of this is &quot;passthru-sql&quot; (e.g. a query parameter like ?filter=&quot;username eq 'bittrance'&quot;). When developers try to follow the precepts of REST but retain an RPC mindset, they frequently create endpoints that allow the caller to pass query fragments that are appended to the resulting database query more or less verbatim. "},{"title":"A published API is an eternal promise​","type":1,"pageTitle":"Designing RESTful APIs for cloud services","url":"blog/2022/04/25/designing-restful-apis-for-cloud-services#a-published-api-is-an-eternal-promise","content":"As long as we have paying customers on a particular API, we maintain that API. We may refuse access by new customers and we may cancel entire services, but as long as a customer uses a service, all APIs on that service are maintained. Where an API in use needs to be decommissioned, that is a commercial decision. Motivation: There is no good time to decommission an API. Any change you force on a customer will incur costs for that customer, with limited benefit. Furthermore, in many cases your success will come through partners using your APIs to design new services on top of yours. Adding customers to a well-designed multi-tenant service has very low marginal cost, potentially enabling different business models. Strong sun-setting clauses will constrain our partners' business models. You may still want to retain the right to decommission APIs in your contracts; sometimes runaway success may incur unacceptable operational costs and you are forced to redesign. Just be aware that regular use of that clause will damage your reputation. Google Ads ability to regularly decommission their APIs is a strong indicator of its undue marketing power. For a counter-point look at retiring EC2 Classic. It took over 10 years from the decision was made to retire EC2 Classic until AWS decided it was commercially acceptable to evict the last stragglers in late 2022. "},{"title":"Endpoints make no assumptions about the URL space​","type":1,"pageTitle":"Designing RESTful APIs for cloud services","url":"blog/2022/04/25/designing-restful-apis-for-cloud-services#endpoints-make-no-assumptions-about-the-url-space","content":"We frequently use HTTP load balancers (and API gateways) to compose our URL space. They may direct any arbitrary part of that space to a particular process. Thus, POST /customers may be one service (which writes to the master database) and GET /customers/:id goes to read replica: a particular endpoint or process must not assume that it &quot;owns&quot; the customer resources, for example by assuming that it will see all writes. Similarly, endpoints should minimize the part of the object model that it requires to present its resource. For example, GET /users/:id should not include additional company information in order to be useful, since users and companies may need to be split across different services tomorrow. Motivation: Our users will be successful by creating innovative things on top of our APIs. Almost by definition, they will use our APIs in ways we did not forsee, thus creating unexpected loads. Therefore, a large part of evolving a cloud service is about changing how data is partitioned and what storage systems are used. Therefore, it is very important to retain flexibility in this regard. A service typically starts small, as a single process exposing all your endpoints. However, as the service grows in popularity and scope, simple horizontal scaling is often not possible and you need to diversify: you may add new overlapping (micro-)services or you may want to split reads and writes into separate processes (i.e. go CQRS). "},{"title":"JSON objects are maps​","type":1,"pageTitle":"Designing RESTful APIs for cloud services","url":"blog/2022/04/25/designing-restful-apis-for-cloud-services#json-objects-are-maps","content":"Adding properties to any returned object is considered a non-breaking change. API docs should point out that properties are new. Similarly, an API can start accepting new optional query parameters on the URL or properties in the input body or add HTTP headers in either direction without being considered breaking. Motivation: REST fundamentally limits us to CRUD and behavior will be implicit from the resource state. In order to implement new behavior it follows that we will over time introduce new properties which controls that behavior. "},{"title":"Versioning is part of the URI​","type":1,"pageTitle":"Designing RESTful APIs for cloud services","url":"blog/2022/04/25/designing-restful-apis-for-cloud-services#versioning-is-part-of-the-uri","content":"The URI should contain a version number. In semver terms, this is a &quot;major&quot; version and we use it to signal breaking changes. Given that we have the ability to extend input and output (see JSON objects are maps), it should be possible to accommodate most &quot;minor&quot; changes within existing APIs. Ideally, resources with different versions have an implicit relation. For example, if we serve both GET /v1/customers/acme and GET /v2/customers/acme, they refer to the same customer. Motivation: Versions in the URI serve two purposes. First, it signals that one resource should be preferred over another. Second, enables us to write new implementations of a service incrementally. "},{"title":"Authorization is based on method + resource​","type":1,"pageTitle":"Designing RESTful APIs for cloud services","url":"blog/2022/04/25/designing-restful-apis-for-cloud-services#authorization-is-based-on-method--resource","content":"Client authorization should depend only on the HTTP request method and the URI (and in some cases on headers). It should preferably not on depend on the request body and particularly not on the state of the resource. Motivation: Ideally, both authentication and authorization should be handled outside of your endpoint. This may be by middleware in your API or by a load balancer or API gateway. Furthermore, having bespoke authorization logic in your endpoints invites security bugs. It is also hard to document and understand for the caller. Loading the underlying resource to know whether the caller is permitted to perform the request risks being expensive - if you deactivate the caller's credentials you don't want to continue accruing the cost of those calls (or higher: the client may well retry several times). Finally, filtering lists on permission defeats caching. A consequence of this rule is that you should avoid APIs that use permissions as filter criteria for resource listings; everyone who calls GET /users should get the same list. If the user list is secret, you introduce a GET /departments/:id/users that has only the relevant users. Similarly, if you have restricted parts of a resource, you can make it into a sub-resource, e.g. GET /users/:id/access_tokens. "},{"title":"Be tough on clients​","type":1,"pageTitle":"Designing RESTful APIs for cloud services","url":"blog/2022/04/25/designing-restful-apis-for-cloud-services#be-tough-on-clients","content":"Clients are expected to: implement HTTP properly. For example, if a resource was missing a correct Content-Type, a client that breaks when this is rectified is at fault.be reasonably parallel. A client should be able to make thousands of requests in a reasonable time frame. For example, a search operation may return summaries and if the client wants more information, it is expected to request the full object for each returned item.do local caching. A client that excessively requests resources that has caching directives should be considered as misbehaving. Motivation: We are building REST APIs to be used by many different callers and our situation would quickly be untenable if we had to respect quirky clients and inexperienced developers. For example, it is relatively straight-forward to horizontally scale a service that can answer 1 million GET/s, but very tricky to answer one GET request which is supposed to return 1 million entries per second. Someone may protest that browsers only allow 6-8 concurrent HTTP sessions against one host and that data must therefore be aggregated or pre-processed for clients to be performant. Normally, introducing HTTP/2.0 multiplexing and ensuring observed response times of &lt;50ms will do the trick just as well. "},{"title":"Twelve-factor app anno 2022","type":0,"sectionRef":"#","url":"blog/2022/02/23/12factor","content":"","keywords":"12-factor devops serverless"},{"title":"Factor I: Codebase​","type":1,"pageTitle":"Twelve-factor app anno 2022","url":"blog/2022/02/23/12factor#factor-i-codebase","content":"A codebase is any single repository. [...] The codebase is the same across all deploys, although different versions may be active in each deploy. This factor is first and foremost an argument for expressing as much as possible of your service as code that can be put under version control. This was probably already a strawman attack ten years ago. Nevertheless, the latest incarnation of mandating version control is GitOps, namely the idea that your infrastructure maintains itself by reading your version control system and automatically applies changes as necessary - remarkably similar to the original Heroku model. There is always a one-to-one correlation between the codebase and the app. If there are multiple codebases, it’s not an app – it’s a distributed system. Each component in a distributed system is an app, and each can individually comply with twelve-factor. This part of the factor remains relevant at the app level, but modern public cloud providers' infrastructure-as-code tooling and platforms like Kubernetes allow us to describe a set of apps and their supporting resources (e.g. secrets) as one package or service. Some organizations separate infrastructure-as-code and app code into separate repositories while others keep the app and its supporting IaC together; neither of these can be said unconditionally to be best practice. Similarly, Single-page apps are often deployed to a Content Distribution Network, while their backend may be deployed to a public cloud provider or to a Kubernetes cluster. Whether these should be kept in the same repository or in different repositories depends on how tightly coupled they are. The 2022 developer considers the relationship between repositories and artifacts carefully. Pertinent aspects include: branching strategycontinuous integration completeness and run timescontinuous delivery pipelinesinfrastructure-as-code maintainabilityconfiguration managementautomated deployments Expect to reorganize your sources as your apps evolve. "},{"title":"Factor II: Dependencies​","type":1,"pageTitle":"Twelve-factor app anno 2022","url":"blog/2022/02/23/12factor#factor-ii-dependencies","content":"A twelve-factor app never relies on implicit existence of system-wide packages. It declares all dependencies, completely and exactly, via a dependency declaration manifest. [...] Twelve-factor apps also do not rely on the implicit existence of any system tools. In the container and function-as-a-service worlds, this factor has been elevated to fact. These execution environments provide virtually no implicit dependencies. Modern apps tend to have more than one dependency declaration manifest, namely its project manifest(s) (e.g. go.mod or package.json) and a Dockerfile. A consequence of this factor is that you should use Docker base images that are as bare-bone as they come, forcing the explicit installation of supporting libs and tools. The up-to-date interpretation of this factor is that upgrading dependency versions should always be a conscious action. This slightly shifts the interpretation of the original factor's &quot;exactly&quot;. The various ecosystems and tool chains (maven, npm, cargo, et.c.) work differently in when they resolve dependencies. Some resolve dependencies when the developer performs a build and some require an explicit &quot;upgrade&quot; operation to change what goes into a build. It is therefore vital to have a codified workflow for updating dependencies. For example, when using Node.js and npm, a developer should normally do npm ci and only use the traditional npm install (or npm update) when the intent is to modernize dependencies. it uses a dependency isolation tool during execution to ensure that no implicit dependencies “leak in” from the surrounding system. The full and explicit dependency specification is applied uniformly to both production and development. One of the innovations introduced by Docker is that this factor is enforced already at build time, making it easy to ensure uniformity across dev and prod. Run your automated tests with the built container and there is very little space for execution environment differences. "},{"title":"Factor III: Config​","type":1,"pageTitle":"Twelve-factor app anno 2022","url":"blog/2022/02/23/12factor#factor-iii-config","content":"An app’s config is everything that is likely to vary between deploys (staging, production, developer environments, etc). The twelve-factor app stores config in environment variables. Env vars are easy to change between deploys without changing any code; unlike config files, there is little chance of them being checked into the code repo accidentally; and unlike custom config files, [...] they are a language- and OS-agnostic standard. [...] A litmus test for whether an app has all config correctly factored out of the code is whether the codebase could be made open source at any moment, without compromising any credentials. This factor remains mostly relevant as written, but there are some nuances to consider. Infrastructure-as-code tools like Terraform allows us to create files and database entries. Kubernetes allows us to create ConfigMaps which will be made available to a container as a file. In both cases, the source configuration can be put under version control, any manual edits will be overwritten on the next deploy and the mechanism is easy for a developer to emulate locally. Thus, they achieve the same result as using environment variables by different means. Also, while environment variables are operations-friendly, they are problematic when writing tests, since they are global state. In ecosystems that default to parallel test execution (e.g. Rust) environment variables cannot be used. Thus, while an environment variable remains the preferred way to accept simple configuration values, a 12-factor app should convert them to internal state as early as possible. This factor is now obsolete in one respect. As much as possible, secrets (passwords, private keys, et c) should be stored using a secrets management system such as Hashicorp Vault or Azure Key Vault. Particularly where we can rely on the infrastructure to authenticate the calling process (e.g. via a Kubernetes service account) access to secrets will not directly require credentials. The existence of the secret is under version control, but the actual secret content is immaterial. Furthermore, with platforms such as Kubernetes, service discovery means that some aspects need no configuration at all. Additionally, some forms of configuration can better be managed as references between IaC-controlled resources, which removes them from direct configuration management consideration. "},{"title":"Factor IV: Backing services​","type":1,"pageTitle":"Twelve-factor app anno 2022","url":"blog/2022/02/23/12factor#factor-iv-backing-services","content":"A backing service is any service the app consumes over the network as part of its normal operation. The code for a twelve-factor app makes no distinction between local and third party services. To the app, both are attached resources, accessed via a URL or other locator/credentials stored in the config. [...] Resources can be attached to and detached from deploys at will. This factor remains relevant as written. Its current iteration is sometimes referred to as &quot;API first&quot; which can be described as the principle that all services you create should be able to act as a backing service. More generally, with the advent of Zero Trust and the proliferation of cloud services, the logical end result of this factor is that any service can interact with any other service on the planet. Even your orchestration platform itself is a backing service, not just the services that run inside it. A service can leverage the Kubernetes control plane to run a one-off job or provision cloud resources to serve a new customer. The original text focuses a lot on relational databases. It is worth pointing out that you can create a service which scalably serves long-lived state without violating the 12 factors: as long as there is a procedure to claim or negotiate access to a particular shard of the state (e.g. backups stored in an Azure storage account), the actual process can remain stateless. Contemporary thinking in this matter is still coloured by software that predates the cloud era (e.g. MySQL, Elasticsearch, RabbitMQ). For an example of what is possible in 2022, we can look at Loki. "},{"title":"Factor V: Build, release, run​","type":1,"pageTitle":"Twelve-factor app anno 2022","url":"blog/2022/02/23/12factor#factor-v-build-release-run","content":"The twelve-factor app uses strict separation between the build, release, and run stages. The build stage is a transform which converts a code repo into an executable bundle known as a build. This factor is more or less a prerequisite for developing software-as-a-service in 2022, but we need to complement this factor with a requirement for automating these stages. The maturing of CI/CD software-as-a-service providers such as GitHub, ACR Tasks and Circle CI means that it is now relatively easy to automate this process. Typically, the build stage will push a container image to some container registry or upload a function-as-a-service zip files to cloud storage. The release stage takes the build produced by the build stage and combines it with the deploy’s current config The normal practice today is for a pipeline to push the release to the runtime environment. This is very useful early in the lifecycle of an app since the release process typically evolves with the app. To achieve stronger separation between the build and release, you might want to consider going GitOps. In Kubernetes-land, you would use a service such as Flux. "},{"title":"Factor VI: Processes​","type":1,"pageTitle":"Twelve-factor app anno 2022","url":"blog/2022/02/23/12factor#factor-vi-processes","content":"Twelve-factor processes are stateless and share-nothing. Any data that needs to persist must be stored in a stateful backing service. This factor remains relevant as written. In the world of REST APIs, this effectively means that we should hold no domain state in memory between HTTP requests - it should always be handed over to a caching service. This is the main enabler for scale-out in a software-as-a-service. Adhering to this rule is also a good way to avoid memory leaks, which tend to plague garbage-collected ecosystems such as Java, Node, Python and Ruby. You will still get leaks after you out-source your caching to Redis, but it will be much easier to measure and the incitement to properly architecture the caching is stronger. "},{"title":"Factor VII: Port binding​","type":1,"pageTitle":"Twelve-factor app anno 2022","url":"blog/2022/02/23/12factor#factor-vii-port-binding","content":"The twelve-factor app is completely self-contained and does not rely on runtime injection of a webserver into the execution environment to create a web-facing service. The web app exports HTTP as a service by binding to a port, and listening to requests coming in on that port. This factor is now standard in containerized scenarios. Widespread adoption of port binding has enabled a whole ecosystem of supporting proxies (e.g. Envoy, Traefik, Toxiproxy) which (ironically) means that a typical app today is often not self-contained, but depends on other containerized apps to perform e.g. authentication and tracing. This is a improves separation of concerns and consequently, in 2022 we consider this factor at the service level. The port-binding approach means that one app can become the backing service for another app, by providing the URL to the backing app as a resource handle in the config for the consuming app. The original text focuses on network protocols such as HTTP and XMPP. In order to become a backing service in 2022, the app should also adhere to an API contract of some sort, defining the backing service's intended role. Many developers implicitly assume that using high-level protocols like HTTP incurs latency. The overhead of a REST call over a local network (e.g. within a cloud provider) is typically 2-4 ms so you need to get a significant number of non-parallelizable requests before this overhead is noticeable over RDBMS operations and latency towards the consumer. "},{"title":"Factor VIII: Concurrency​","type":1,"pageTitle":"Twelve-factor app anno 2022","url":"blog/2022/02/23/12factor#factor-viii-concurrency","content":"In the twelve-factor app, processes are a first class citizen. Processes in the twelve-factor app take strong cues from the unix process model for running service daemons. [...] This does not exclude individual processes from handling their own internal multiplexing. But an individual VM can only grow so large (vertical scale), so the [app] must also be able to span multiple processes running on multiple physical machines. This factor is now more or less written into law. Function-as-a-service platforms typically provide transparent horizontal scaling. In Kubernetes deployments you just give the number of pods you expect. The mechanics of horizontal scalability is thus addressed in 2022. The central challenge for any successful app is to distribute work across multiple processes and backing services in such a way that it actually achieves meaningful horizontal scalability. The typical RDBMS-backed web app usually has its scalability completely constrained by its backing database, forcing vertical scaling of the RDBMS - a matter of allocating additional CPUs. This is often expensive and tends to yield only marginal improvement. Is short, horizontal scalability is much preferable over vertical scalability but it is strictly a result of software architecture. It is therefore vital to identify early those apps that will actually require significant scale-out so that they can be properly architected. Despite the dominance of the serverless paradigm in the software-as-a-service realm, there is still an overhang from the era of vertical scaling which the Twelve Factor App tries to break with. For example, the virtual machines of Java, Node, Python and Ruby maintain large volumes of reflection metadata and are very reluctant to de-allocate memory, leading to significant inefficiency on scale-out. A new generation of ecosystems, lead by Go and Rust, are more frugal in this respect. "},{"title":"Factor IX: Disposability​","type":1,"pageTitle":"Twelve-factor app anno 2022","url":"blog/2022/02/23/12factor#factor-ix-disposability","content":"The twelve-factor app’s processes are disposable, meaning they can be started or stopped at a moment’s notice. [...] Processes should strive to minimize startup time. [...] Processes shut down gracefully when they receive a SIGTERM signal, [...] allowing any current requests to finish. [...] A twelve-factor app is architected to handle unexpected, non-graceful terminations. This factor remains relevant as written and remains nearly as elusive today as it was ten years ago. For example, the HTTP server included in Node.js does not by default perform graceful shutdown (see also nodejs issue 2642). Fulfilling this factor on an existing code base is much harder than it sounds. It means mapping all the (often implicit) state machines involved in the app and coordinating them so that there are no undefined transitions. For example, the database connection pool must be ready before we bring up our HTTP listener and must not shut down until the HTTP listener is down and all in-flight HTTP requests are done. Workers must &quot;hand back&quot; in-progress work items so that replacement workers do not have to wait for timeout to process those work items. This difficulty is compounded with distributed systems as a conceptual &quot;transaction&quot; may span more than one app or backing service (e.g. writing to file storage and sending mail), requiring two-phase commit semantics. This factor is nevertheless the key to the always-on experience that we take for granted in large cloud services. A service with working graceful shutdown and comprehensive health checks can be updated at any time and builds developer and operations confidence. This can result in significant productivity gains, particularly when combined with automated testing. "},{"title":"Factor X: Dev/prod parity​","type":1,"pageTitle":"Twelve-factor app anno 2022","url":"blog/2022/02/23/12factor#factor-x-devprod-parity","content":"Historically, there have been substantial gaps between development [...] and production [...], the time gap, the personnel gap [and] the tools gap. [...] The twelve-factor app is designed for continuous deployment by keeping the gap between development and production small. This factor is now colloquially known as DevOps and remains as relevant as ever, but has still not established itself fully in software-as-a-service development: many production environments are hard to squeeze onto a developer's laptop. Generally speaking, the public cloud providers put too little effort into supporting development use cases for their services. Kubernetes goes furthest in this respect: Kind deserves mentioning for its heroic effort to achieve a dev-friendly, multi-node Kuberentes cluster using only Docker. Docker has introduced a borderland where it is possible to develop a container using just Docker Engine for dev environment and still be reasonably confident that it will execute properly in e.g. Kubernetes. Still, some provisioning of backend services is still needed and time is wasted maintaining two different sets of instrumentation. For example, apps often have a Docker Compose file to get developers started, and a Kubernetes manifest for test/prod. When these desynch, nasty surprises can occur at deployment. The twelve-factor developer resists the urge to use different backing services between development and production Many full-stack development setups includes &quot;dev&quot; servers whose role is to automatically reload the app as its source code change. Similarly, tools like Docker Desktop and Tilt provide capabilities and environments that are subtly different from e.g. Azure Container instances or Kubernetes. All these will color developers' choices and risk introducing issues that will not be discovered until late in the process. The 2022 developer considers both developer experience, continuous integration/delivery/deployment and operability when choosing tools. "},{"title":"Factor XI: Logs​","type":1,"pageTitle":"Twelve-factor app anno 2022","url":"blog/2022/02/23/12factor#factor-xi-logs","content":"Logs provide visibility into the behavior of a running app. [...] A twelve-factor app never concerns itself with routing or storage of its output stream. It should not attempt to write to or manage logfiles. Instead, each running process writes its event stream, unbuffered, to stdout. [...] Destinations are not visible to or configurable by the app, and instead are completely managed by the execution environment. Interestingly, this factor does not actually advise on the use of logging. Rather it treats them much as pre-contraceptive times viewed children: as something that inevitably accumulates as a result of marriage. The factor should thus be updated to mandate that an app should be &quot;observable&quot;, meaning that it should volunteer information on its performance and behavior. We normally break this down into logging, metrics, tracing and audit trails. The relative merit of these differ greatly between apps, but all apps should have an observability strategy. Often, the need changes as an app evolve: early in the lifecycle, logging may dominate, but as usage grows, focus shifts to metrics. The apps in a service are considered as a unit and typically provide different observability needs. "},{"title":"Factor XII: Admin processes​","type":1,"pageTitle":"Twelve-factor app anno 2022","url":"blog/2022/02/23/12factor#factor-xii-admin-processes","content":"One-off admin processes should be run in an identical environment as the regular long-running processes of the app. They run against a release, using the same codebase and config as any process run against that release. This factor captures a practice that is common in the Rails and Drupal ecosystems, among others. These are based on interpreted languages where it is relatively easy to give scripting or interactive access to the app's internals: the main reason is to ensure that database access occurs using the same models that are used during runtime. In compiled languages, this requires a modularization (e.g. making the data model a separate library) that would complicate development. However, the factor has two underlying principles which holds even today. First, that tools used to administer an app should be versioned and released with the same discipline as your app is. Second, that operating and upgrading an app is part of its ordinary usage and there is nothing strange with adding endpoints for internal administrative use. Put differently, at least factors I - IV should apply to the app's tooling just as it does to the app itself. "},{"title":"What else is there?​","type":1,"pageTitle":"Twelve-factor app anno 2022","url":"blog/2022/02/23/12factor#what-else-is-there","content":"Various additional factors have been proposed over the years, for example Beyond the Twelve-factor app and Seven missing factors from the Twelve-factor app. The appeal of the original methodology springs from the universality of its factors. These contributions have relevance, but often only for a subsection of all apps that could (should) strive to live up to the original twelve factors. However, there is two aspects that are clearly missing: security and automated testing. The good people at WhiteHat Security has written a good analysis called Security and the twelve-factor app which analyses each factor from a security perspective and provides recommendations for securing your app. Their main point is that rather than being an additional factor, security needs to permeate all the twelve factors. The charge that security is underrepresented in the original methodology har merit, and the 2022 developer no longer has the luxury of treating security as an afterthought. Finally, much of the benefit of adhering to the methodology is lost without extensive and automated testing. Version control hygiene and horizontal scalability matters little if apps break as soon as they are deployed. Ten years ago, there was still a discussion about whether writing programmatic tests was worth the effort. That discussion is now settled and in 2022, the discussion is about what the automated testing strategy should look like for a particular app or service. The discerning developer considers: when to apply unit, component, integration and end-to-end tests at app and/or service levelwhen to use in-memory implementations of backing serviceswhat long-lived testing environments are neededhow much automated static analysis to include These additions notwithstanding, the Twelve-factor app methodology remains remarkably relevant today. All developers producing software-as-a-service offerings can benefit from adhering to its factors. "},{"title":"Kubernetes Ephemeral Container Security","type":0,"sectionRef":"#","url":"blog/2022/04/12/ephemeral-container-security","content":"","keywords":"kubernetes security ephemeral containerss"},{"title":"Digging Deeper​","type":1,"pageTitle":"Kubernetes Ephemeral Container Security","url":"blog/2022/04/12/ephemeral-container-security#digging-deeper","content":"Now that we have the new feature we can start a ephemeral container in any Pod we like. kubectl run ephemeral-demo --image=k8s.gcr.io/pause:3.1 --restart=Never kubectl debug -it ephemeral-demo --image=busybox:1.28 Copy We get a shell and life is now much simpler, but wait a minute. This post is not about how to use ephemeral containers, there are enough of those already, but rather the security implications of enabling ephemeral containers. Let's have a look at the YAML for the Pod that we created the ephemeral container in. apiVersion: v1 kind: Pod metadata: name: ephemeral-demo spec: ... ephemeralContainers: - name: debugger-r59b7 image: busybox:1.28 imagePullPolicy: IfNotPresent stdin: true terminationMessagePath: /dev/termination-log terminationMessagePolicy: File tty: true Copy Interesting, there is a new field called ephemeralContainers in the Pod definition. This new field contains a list of containers similar to initContainers and containers. It is not identical as there are certain options which are not available, refer to the API documentation for more information. It does however allow configuration of the container security context, which could in theory allow a bad actor to escalate the container's privileges. This should not affect those of us who use a policy enforcement tool right? The answer is yes and no depending on the tool and version that is being used. It also depends on if you are using policies from the project's library or policies developed in house. "},{"title":"OPA Gatekeeper​","type":1,"pageTitle":"Kubernetes Ephemeral Container Security","url":"blog/2022/04/12/ephemeral-container-security#opa-gatekeeper","content":"OPA Gatekeeper does not require any code changes as all of its policies are written in rego. It's sub project Gateekper Library does however have to be updated. The library contains an implementation of the common Pod Security Policies. This includes policies like not allowing containers in privileged mode. The issue with the all of the policies is that they currently only check containers specified in initContainers and containers, analyze the following rego as an example. The good news is that this is a pretty easy fix, the bad news is that it requires end users to update the policies pulled from the library. "},{"title":"Kyverno​","type":1,"pageTitle":"Kubernetes Ephemeral Container Security","url":"blog/2022/04/12/ephemeral-container-security#kyverno","content":"Kyverno seems to have resolved the issues faster. Compared to OPA Gatekeeper however it did require a small code change which means that version 1.5.3 or later is needed to write policies for ephemeral containers. They have also updated their policy library to include checking ephemeral containers. Kyverno has done a great job solving these issues quickly. It does still require end users to update however. "},{"title":"Pod Security Policies​","type":1,"pageTitle":"Kubernetes Ephemeral Container Security","url":"blog/2022/04/12/ephemeral-container-security#pod-security-policies","content":"Pod Security Policies used to be the default policy tool for Kubernetes, and a lot of projects have rules based on Pod Security Policies (PSP). However if you are relying on PSP in a modern cluster you should really start looking for other options like OPA Gatekeeper or Kyverno. PSP has been deprecated since Kubernetes v1.21 and will be removed in v1.25. If PSP is your only policy tool and you are planning to upgrade to v1.23, don't. As PSP is deprecated no new features have been added, and that includes policy enforcement on ephemeral containers. Which means that any security context in an ephemeral container is allowed no matter the PSP in the cluster. The PSP below will have no affect when adding an ephemeral container to a Pod which is privileged. apiVersion: policy/v1beta1 kind: PodSecurityPolicy metadata: name: default spec: privileged: false seLinux: rule: RunAsAny supplementalGroups: rule: RunAsAny runAsUser: rule: RunAsAny fsGroup: rule: RunAsAny volumes: - '*' Copy "},{"title":"RBAC​","type":1,"pageTitle":"Kubernetes Ephemeral Container Security","url":"blog/2022/04/12/ephemeral-container-security#rbac","content":"Disallowing ephemeral containers with RBAC could be an option if the feature is not needed and it is not possible to disable the feature completely. The KEP-277: Ephemeral Containers state the following about using RBAC to disable ephemeral containers. Cluster administrators will be expected to choose from one of the following mechanisms for restricting usage of ephemeral containers: Use RBAC to control which users are allowed to access the /ephemeralcontainers subresource.Write or use a third-party admission controller to allow or reject Pod updates that modify ephemeral containers based on the content of the update.Disable the feature using the EphemeralContainers feature gate. RBAC is additive which means that it is not possible to remove permissions from a role. This type of mitigation obviously does not matter if all users a cluster admin, which they should not be, so we assume that new roles are created for the cluster consumers. In this case having a look at the existing roles can be enough to make sure that the subresource /ephemeralcontainers is not included in the role. apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRole metadata: name: edit rules: - apiGroups: - &quot;&quot; resources: - pods - pods/attach - pods/exec - pods/portforward - pods/proxy verbs: - create - delete - deletecollection - patch - update Copy "},{"title":"Checking Policy Enforcement​","type":1,"pageTitle":"Kubernetes Ephemeral Container Security","url":"blog/2022/04/12/ephemeral-container-security#checking-policy-enforcement","content":"Let's say that you upgraded your cluster and informed all end users of the great new feature. How do you know that the correct policies are enforced in accordance to your security practices. You may have been aware of the API changes and taken the correct precautionary steps. Or you just updated Kyverno and it's policies out of pure happenstance. Either way it is good to trust but verify that it is not for example possible to create a privileged ephemeral container. Annoyingly the debug command does not expose any options to set any security context configuration, so we need another option. Ephemeral containers cannot be defined in a Pod when it is created and it can neither be added with an update. We need some other method to create these specific ephemeral containers. Ephemeral containers are created using a special ephemeralcontainers handler in the API rather than by adding them directly to pod.spec, so it's not possible to add an ephemeral container using kubectl edit. The simplest method to add an ephemeral container with a security context to a Pod is to use the Go client. A couple of lines of code can add a new ephemeral container running as privileged or use any other security context setting which is to your liking. package main import ( &quot;context&quot; &quot;fmt&quot; &quot;os&quot; corev1 &quot;k8s.io/api/core/v1&quot; metav1 &quot;k8s.io/apimachinery/pkg/apis/meta/v1&quot; &quot;k8s.io/client-go/kubernetes&quot; &quot;k8s.io/client-go/tools/clientcmd&quot; ) func main() { if len(os.Args) != 4 { panic(&quot;expected three args&quot;) } podNamespace := os.Args[1] podName := os.Args[2] kubeconfigPath := os.Args[3] // Create the client client, err := getKubernetesClients(kubeconfigPath) if err != nil { panic(fmt.Errorf(&quot;could not create client: %w&quot;, err)) } ctx := context.Background() // Get the Pod pod, err := client.CoreV1().Pods(podNamespace).Get(ctx, podName, metav1.GetOptions{}) if err != nil { panic(fmt.Errorf(&quot;could not get pod: %w&quot;, err)) } // Add a new ephemeral container trueValue := true ephemeralContainer := corev1.EphemeralContainer{ EphemeralContainerCommon: corev1.EphemeralContainerCommon{ Name: &quot;debug&quot;, Image: &quot;busybox&quot;, TTY: true, SecurityContext: &amp;corev1.SecurityContext{ Privileged: &amp;trueValue, AllowPrivilegeEscalation: &amp;trueValue, }, }, } pod.Spec.EphemeralContainers = append(pod.Spec.EphemeralContainers, ephemeralContainer) pod, err = client.CoreV1().Pods(pod.Namespace).UpdateEphemeralContainers(ctx, pod.Name, pod, metav1.UpdateOptions{}) if err != nil { panic(fmt.Errorf(&quot;could not add ephemeral container: %w&quot;, err)) } } func getKubernetesClients(path string) (kubernetes.Interface, error) { cfg, err := clientcmd.BuildConfigFromFlags(&quot;&quot;, path) if err != nil { return nil, err } client, err := kubernetes.NewForConfig(cfg) if err != nil { return nil, err } return client, nil } Copy Run the program and pass the namespace, pod name, and path to a kube config file. We assume that the ephemeral-demo Pod is still running. go run main.go default ephemeral-demo $KUBECONFIG Copy If it completes with no error a privileged ephemeral container should have been added to the Pod. Exec into it and list the host's devices to prove that it is a privileged container. kubectl exec -it ephemeral-demo -c debug -- sh ls /dev Copy "},{"title":"Conclusion​","type":1,"pageTitle":"Kubernetes Ephemeral Container Security","url":"blog/2022/04/12/ephemeral-container-security#conclusion","content":"If there is one takeaway from this post, it is that any policy tool that has not been updated in the last couple of months will not enforce rules on ephemeral containers. This also includes all policies written in house! It is not enough to update the community policies. Some may argue that this type of oversight is not really an issue. Ephemeral containers can't mount host paths, or access the hosts namespaces. All it can do is set the common container security context. That is a fair comment, because it's true. Being able to create a privileged container is however still not ideal, and there are methods to escalate privileges when this is possible. Either way it is important to be aware of how policies are enforced and the security contexts which are allowed. I am still not sure how much of an issue this will be short term. Cloud providers are currently in the process of rolling out Kubernetes v1.23 in their SaaS offerings. In these solutions it is still a possibility that they chose to disable ephemeral containers. Those rolling their own clusters may have already upgraded to v1.23 and not be aware of the new feature. That is the biggest issue really, that the platform administrator has to be aware of the existence of ephemeral containers. The fact that kubectl does not expose the option to set a security context will make even less people aware that it is still possible to set one with other means. Investing in a security audit 6 months ago will only be valuable as long as the same Kubernetes version is used. Kubernetes is by design not secure by default, so each new feature introduced has to be analyzed. The fact that upgrading from Kubernetes v1.22 to v.23 could make your cluster less secure is part of the difficulties of working with Kubernetes, requiring platform administrators to always stay on top of things. The reality is that these types of things are easy to miss, so hopefully this post has helped someone make their cluster a bit more secure. "},{"title":"Profiling Go in Kubernetes","type":0,"sectionRef":"#","url":"blog/2022/11/21/golang-memory-dump","content":"","keywords":"pprof kubernetes debug"},{"title":"Application​","type":1,"pageTitle":"Profiling Go in Kubernetes","url":"blog/2022/11/21/golang-memory-dump#application","content":"To get started lets use a very simple test application written by the team over at Polar Signals. They have been kind enough to publish a container image and a Kubernetes deployment that we will use. The application is built in such a way that it will continue to use memory and the Kubernetes yaml don't contain any request nor limit so don't run this for a long time or your system will probably OOM. Run the application on Kubernetes 1.23 or higher. kubectl apply -f https://raw.githubusercontent.com/polarsignals/pprof-example-app-go/main/manifests/deployment.yaml Copy For starters lets have a look at the pprof http endpoint using curl. A simple way of doing so is to create a Kubernetes service and reach the pod from another container. # Expose the pprof deployment kubectl expose deployment pprof-example-app-go --port=8080 Copy Create a curl pod and look at the data inside our app. kubectl run -i -t curl --image=curlimages/curl:latest /bin/sh Copy You will be sent directly in to the container and you can run something like: curl http://pprof-example-app-go:8080/debug/pprof/allocs?debug=1 Copy This will show you an output that look something like this: heap profile: 4: 47357952 [5473: 554303632] @ heap/1048576 1: 46882816 [1: 46882816] @ 0x697585 0x470ce1 # 0x697584 main.allocMem+0xa4 /home/brancz/src/github.com/polarsignals/pprof-example-app-go/main.go:65 1: 212992 [1: 212992] @ 0x4e3b6e 0x4e3e6c 0x6974c5 0x470ce1 # 0x4e3b6d log.(*Logger).Output+0x38d /usr/local/go/src/log/log.go:180 # 0x4e3e6b log.Println+0x6b /usr/local/go/src/log/log.go:329 # 0x6974c4 main.calculateFib+0xc4 /home/brancz/src/github.com/polarsignals/pprof-example-app-go/main.go:55 1: 204800 [1: 204800] @ 0x4fb68f 0x4fb256 0x503d3d 0x502c97 0x4f6d5e 0x4f6c92 0x4d22a5 0x4d2625 0x4d70b1 0x4cf3d2 0x4e3e3f 0x6974c5 0x470ce1 # 0x4fb68e math/big.nat.make+0x5ee /usr/local/go/src/math/big/nat.go:69 # 0x4fb255 math/big.nat.sqr+0x1b5 /usr/local/go/src/math/big/nat.go:595 Copy Sadly this output isn't the easiest to read so why not use pprof and while we are at it why not use ephemeral containers. "},{"title":"Debug container/ephemeral container​","type":1,"pageTitle":"Profiling Go in Kubernetes","url":"blog/2022/11/21/golang-memory-dump#debug-containerephemeral-container","content":"My original plan for this blog post was to attach a new volume to the ephemeral container so we could save data locally and then copy it out to our client and show some nice flame graphs. But apparently it's not supported to attach volumes to the ephemeral container. The ephemeral container cannot even reach the existing volumes on the pod you attach to. There is an open issue to solve this but it's not part of the current enhancement proposal so this is nothing that we will see in the near future. So instead I will just show how we can debug using pprof from within the container. Since we exposed the endpoint through a service we could of course do this from another pod as well. But in general you should be very restrictive of what traffic that can reach your pprof endpoint if you expose it at all. So finally time to use the kubectl debug command. The debug command is used on a specific pod, in my case it's pprof-example-app-go-7c4b6d77d-xw52p. Let's attach a standard golang container to our running pod, in this case i choose golang 1.15 to match the pprof tool with the running application. Kubernetes will attach the container for you and give you a shell. kubectl debug -i -t pprof-example-app-go-7c4b6d77d-xw52p --image=golang:1.15-alpine3.14 -- /bin/sh Copy Now we can point on localhost using pprof. go tool pprof http://localhost:8080/debug/pprof/allocs Copy This will provide you with a pprof terminal inside the container. I'm no pprof pro but there are some easy commands to get you started.top 10 -cum will show you the resource consumption it takes to call a function including all function it calls top 10 -cum Showing nodes accounting for 27.63GB, 99.80% of 27.69GB total Dropped 26 nodes (cum &lt;= 0.14GB) Showing top 10 nodes out of 17 flat flat% sum% cum cum% 0 0% 0% 25.66GB 92.69% main.calculateFib 25.59GB 92.43% 92.43% 25.59GB 92.43% math/big.nat.make (inline) 0 0% 92.43% 25.41GB 91.78% github.com/polarsignals/pprof-example-app-go/fib.Fibonacci 0 0% 92.43% 25.41GB 91.78% math/big.(*Int).Add 0 0% 92.43% 25.41GB 91.78% math/big.nat.add 2.02GB 7.30% 99.73% 2.02GB 7.30% main.allocMem 0.02GB 0.073% 99.80% 0.25GB 0.91% fmt.Sprintln 0 0% 99.80% 0.25GB 0.91% log.Println 0 0% 99.80% 0.23GB 0.84% fmt.(*pp).doPrintln 0 0% 99.80% 0.23GB 0.84% fmt.(*pp).handleMethods Copy top 10 -flat will show you the resource consumption it takes to call a function excluding all function it calls Active filters: ignore=flat Showing nodes accounting for 27.66GB, 99.90% of 27.69GB total Dropped 10 nodes (cum &lt;= 0.14GB) Showing top 10 nodes out of 17 flat flat% sum% cum cum% 25.59GB 92.43% 92.43% 25.59GB 92.43% math/big.nat.make (inline) 2.02GB 7.30% 99.73% 2.02GB 7.30% main.allocMem 0.03GB 0.1% 99.83% 0.21GB 0.76% math/big.nat.itoa 0.02GB 0.073% 99.90% 0.25GB 0.91% fmt.Sprintln 0 0% 99.90% 0.23GB 0.84% fmt.(*pp).doPrintln 0 0% 99.90% 0.23GB 0.84% fmt.(*pp).handleMethods 0 0% 99.90% 0.23GB 0.84% fmt.(*pp).printArg 0 0% 99.90% 25.41GB 91.78% github.com/polarsignals/pprof-example-app-go/fib.Fibonacci 0 0% 99.90% 0.25GB 0.91% log.Println 0 0% 99.90% 25.66GB 92.69% main.calculateFib Copy By looking at the output we can see that it's math/big.nat.make that takes the most resources. Just for fun I also generated a flame graph using pprof by port-forwarding to the application and running go tool pprof -http=: http://localhost:8080/debug/pprof/allocs Copy  "},{"title":"Cleanup​","type":1,"pageTitle":"Profiling Go in Kubernetes","url":"blog/2022/11/21/golang-memory-dump#cleanup","content":"To cleanup the resources we created run: kubectl delete -f https://raw.githubusercontent.com/polarsignals/pprof-example-app-go/main/manifests/deployment.yaml kubectl delete svc pprof-example-app-go kubectl delete pod curl Copy "},{"title":"Conclusion​","type":1,"pageTitle":"Profiling Go in Kubernetes","url":"blog/2022/11/21/golang-memory-dump#conclusion","content":"The kubectl debug command is extremely useful when you want to debug your application and you don't have access to a shell or the tools that you need in your normal container. It saves us from having to install unneeded applications in our container which lowers the amount of potential CVE:s and the time it takes to start your container by lower container size. Kubectl debug isn't perfect and it won't work for all your uses cases especially since you can't use it to interact with existing volumes but it's a great start. When it comes to continues profiling it's probably better to look at tool specifically written for it like Grafana Phlare or Parca. But using a tool like pprof locally can be a good start. Hopefully we will get time to write a blog about continues profiling in the future. "},{"title":"Introduction","type":0,"sectionRef":"#","url":"docs/","content":"Introduction Welcome to Xenit's open source projects! We at Xenit are strong believers in open source, we feel that we have gotten so much from open source and we try to give as much back as possible as part of our daily work. This page hosts our general documentation but we also store documentation close to our source code which can give you extra needed context. Here are a few of our projects. terraform-modules stores all our Terraform code and the base for XKF.azure-devops-templates contains both GitHub and Azure DevOps templates for our CI/CD solution.azad-kube-proxy is an auth proxy for kubernetes used in XKF.gitops-promotion is a small Go application that we have built to auto update our GitOps repositories, instead of having to write complex shell scripts.git-auth-proxy is a proxy to allow multi-tenant sharing of GitHub and Azure DevOps credentials in Kubernetes.github-actions is a Go application that automatically prepares Terraform state management and a container image to run in Terraform pipelines. We have many more open source projects so feel free to look around in https://github.com/XenitAB.","keywords":""},{"title":"Javascript","type":0,"sectionRef":"#","url":"docs/xenit-style-guide/javascript","content":"","keywords":""},{"title":"Tracing​","type":1,"pageTitle":"Javascript","url":"docs/xenit-style-guide/javascript#tracing","content":""},{"title":"Datadog​","type":1,"pageTitle":"Javascript","url":"docs/xenit-style-guide/javascript#datadog","content":"To trace a frontend application using Datadog as tracing provider, see Connect RUM (Real User Monitoring) and Traces. This will inject the trace headers into the HTTP requests which will be propagated to the backend services. Enabling RUM will also insert a user session id into the request that can be used to group multiple traces to one user session. "},{"title":"Overview","type":0,"sectionRef":"#","url":"docs/xks/","content":"Overview Xenit Kubernetes Service is an opinionated Kubernetes deployment on top of a cloud provider's managed Kubernetes service. XKS currently supports Azure Kubernetes Service (AKS) and AWS Elastic Kubernetes service (EKS). Xenit Kubernetes Service: is secure by defaultis DevOps-orientedhas batteries includedhas a team-oriented permissions model This documentation consists of two main sections: Developer Guide: This documentation is targeted towards developers using XKS. It covers the basics of Kubernetes and the custom features that are offered by XKS. Operator Guide: This section is meant primarily for Xenit's operations staff. It collects Xenit's internal documentation for operating XKS clusters. It is public and part of this documentation because we believe in transparency. It serves as a reference to how the various services included in XKS are set up. It also describes various recurring procedures, such as replacing an existing Kubernetes cluster. XKS is assembled from Open Source services, some of which are provided to XKS customers. This assembly is itself Open Source and the important components are documented under the Projects section in the menu bar. For more information about the services, please refer to their respective documentation. Some of the more prominent projects: KubernetesFluxNginx Ingress Controller","keywords":""},{"title":"Containers","type":0,"sectionRef":"#","url":"docs/xenit-style-guide/containers","content":"","keywords":""},{"title":"Docker​","type":1,"pageTitle":"Containers","url":"docs/xenit-style-guide/containers#docker","content":"TBD "},{"title":"Helm​","type":1,"pageTitle":"Containers","url":"docs/xenit-style-guide/containers#helm","content":"Helm has since version 3.8.0 supported storing Helm charts in OCI registires. This solution has a lot of benefits over the old Helm registry and for this reason should be the only way Helm charts are published in Xenit open source projects. The Helm chart OCI artifact should have the same name as the Docker image which the application produces and share the same version number. To avoid name conflicts however the Helm chart should be prefixed with helm-charts/. Refer to the project node-ttl for an example. It has both the Docker image node-ttl and the Helm chart helm-charts/node-ttl stored in Github container registries. Below is an GitHub action which builds and pushes a chart to GitHub container registry. name: release on: release: types: [published] jobs: helm: runs-on: ubuntu-latest steps: - name: Clone repo uses: actions/checkout@v2 - name: Install Helm uses: azure/setup-helm@v1 - name: Get GitHub Tag id: get_tag run: | echo &quot;::set-output name=tag::${GITHUB_REF#refs/tags/}&quot; - name: Publish Helm charts run: | cd charts helm registry login -u ${{ github.repository_owner }} -p ${{ secrets.GITHUB_TOKEN }} ghcr.io helm package --app-version ${{ steps.get_tag.outputs.tag }} --version ${{ steps.get_tag.outputs.tag }} node-ttl helm push node-ttl-${{ steps.get_tag.outputs.tag }}.tgz oci://ghcr.io/xenitab/helm-charts Copy "},{"title":"Best Practices","type":0,"sectionRef":"#","url":"docs/xks/developer-guide/best-practices","content":"","keywords":""},{"title":"Container Resources​","type":1,"pageTitle":"Best Practices","url":"docs/xks/developer-guide/best-practices#container-resources","content":""},{"title":"Probes​","type":1,"pageTitle":"Best Practices","url":"docs/xks/developer-guide/best-practices#probes","content":""},{"title":"Pod Scaling​","type":1,"pageTitle":"Best Practices","url":"docs/xks/developer-guide/best-practices#pod-scaling","content":""},{"title":"Disruption Budgets​","type":1,"pageTitle":"Best Practices","url":"docs/xks/developer-guide/best-practices#disruption-budgets","content":""},{"title":"Resources​","type":1,"pageTitle":"Best Practices","url":"docs/xks/developer-guide/best-practices#resources","content":"Here are some good resources to also read on top of this page. https://srcco.de/posts/web-service-on-kubernetes-production-checklist-2019.html "},{"title":"Architecture and design","type":0,"sectionRef":"#","url":"docs/xks/architecture-and-design","content":"","keywords":""},{"title":"Overview​","type":1,"pageTitle":"Architecture and design","url":"docs/xks/architecture-and-design#overview","content":"Xenit Kubernetes Framework (XKF) are the open source building blocks for a service Xenit AB provides to customers: Xenit Kubernetes Service (XKS) In the terminology of Microsoft Cloud Adoption Framework (CAF), Xenit Kubernetes Service is an enterprise-scale landing zone. Additionally, the workload supports multiple cloud providers and AWS is also supported at the moment (but still requires the governance part in Azure).  "},{"title":"Glossary​","type":1,"pageTitle":"Architecture and design","url":"docs/xks/architecture-and-design#glossary","content":"Platform team: the team managing the platform (XKF)Tenant: A group of people (team/project/product) at the company using XKS "},{"title":"Role-based access management​","type":1,"pageTitle":"Architecture and design","url":"docs/xks/architecture-and-design#role-based-access-management","content":"All role-based access control (RBAC) and identity &amp; access management (IAM) is handled with Azure AD. Azure AD groups are created and nested using the framework and framework admins as well as customer end users are granted access through these different groups. Where possible two different permissions are exposed through Azure AD groups: Reader and Contributor. These permissions are scoped in many different ways and start at the management group level, subscription level, resource group level and at last namespaces in Kubernetes. These are also split over the different environments (development, quality assurance and production) meaning you can have read/write in one environment but only read in the others. An owner role and group is also created for most resources, but the recommendation is not to use it as owners will be able to actually change the IAM which in most cases is undesirable. Usually the tenant is granted read/write to their resource groups and namespaces, meaning they will be able to add/remove whatever they want in their limited scope. This usually means creating deployments in Kubernetes as well as databases and other stateful resources in their Azure Resource Groups. When using AWS Elastic Kubernetes Service (EKS) the delegation is not as rigorous as in Azure and the default setup creates three accounts where all the customer tenants share resources. Each tenant namespace has the ability to use the cloud provider metadata service to access services in the cloud provider. This is enabled through tools like Azure AD POD Identity (aad-pod-identity) and IAM Roles for Service Accounts (IRSA). These tools enable the tenants to access resources in their respective resource groups or accounts without having to create manually shared secrets (that would also have to be rotated). "},{"title":"Security and access​","type":1,"pageTitle":"Architecture and design","url":"docs/xks/architecture-and-design#security-and-access","content":""},{"title":"Security​","type":1,"pageTitle":"Architecture and design","url":"docs/xks/architecture-and-design#security","content":"The platform is based on the principle of least privilege and in every layer we try to only delegate what is needed and no more. This is both true for the tenant and platform resources and their respective access. The second part of the work we do regarding security is focused around trying to keep the platform as lean and easy to understand as possible, making sure to remove complexity where needed. Adding security often means selecting a much more complex solution and it is much harder to understand and maintain something complex over something simple. Keeping the cognitive load of maintaining a platform like this as low as possible is always a priority. Additionally, we try to add products and services into the mix to make it easier for both the platform team and tenant teams to keep the environment secure. This is an ever-changing environment where new things are added as needed and the list below is not an exhaustive one: Tenant security​ GitOps for application delivery to clustersCI/CD templates for building containers and promoting applications with GitOpsTools for security scanning and linting inside the pipelinesAdmission controllers to make sure unsafe configuration is prohibitedMutating admission controllers to make it easier to have sane defaultsContinuous education from the platform teamOne account to protect instead of having multiple accounts to access different resourcesInfrastructure as Code with corresponding CI/CD templates for secure deployment of resourcesSSO from the tenant namespaces to the tenant resource groups to make sure no secrets have to be sharedAbility to use mTLS together with a service mesh (linkerd)Automated certificate and DNS management from the platform Platform​ Observability of the platform handled by the platform teamRuntime security in the platform using FalcoAutomated update management of Infrastructure as Code (with code review) "},{"title":"Access​","type":1,"pageTitle":"Architecture and design","url":"docs/xks/architecture-and-design#access","content":"The primary authentication method to access any resource is based on Azure AD. Most of the actions taken by a tenant engineer will require authentication to Azure AD either using the Azure Management Console or the Azure CLI. A tenant will be granted access to the clusters using a proxy built by Xenit and provided in the framework, making sure they do not have to reconfigure their computers when a blue/green deployment of the clusters are made and the Kubernetes API endpoint change. The proxy will move with the clusters and it will be seamless for the tenant engineers. The proxy also provides a CLI (kubectl plugin through krew) that makes it easier to both discover and configure access to the clusters. A valid session with Azure CLI is required to use it. Other than that, most of the access and work with the tenant resources are done through a GitOps repository for changes in regards to applications in Kubernetes and a Terraform repository in regards to resources in the cloud provider. "},{"title":"Network design​","type":1,"pageTitle":"Architecture and design","url":"docs/xks/architecture-and-design#network-design","content":"By default, the network setup is expected to be quite autonomous and usually considered to be an external service compared to everything else in the organization using it. It is possible to setup peering with internal networks, but usually it begins with a much simpler setup and then grows organically when required.  The cluster environments are completely separated from each other, but a hub in the production subscription has a peering with them to provide static IP-addresses for CI/CD like terraform to access resources. Inside an environment the cluster is using kubenet and Calico to keep the amount of IP-addresses to a minimum. Kubernetes Services can either be exposed internally or externally using either a service resource or an ingress resource, where most tenants exclusively use ingress (provided by NGINX Ingress Controller). Inside the clusters Calico is used to restrict traffic between namespaces. "},{"title":"Backup​","type":1,"pageTitle":"Architecture and design","url":"docs/xks/architecture-and-design#backup","content":"Xenit Kubernetes Framework is built to be ephemeral wich means any cluster can at any time be wiped and a new setup without any loss of data. This means that tenants are not allowed to store state inside of the clusters and are required to store it in the cloud provider (blob storage, databases, message queues etc.). Since both the platform team and the tenant teams are deploying resources using GitOps, the current state of the environment is stored in git. The content of stateful resources (including backups) are handled by the cloud provider and it is up to the tenants to configure and manage. "},{"title":"Cost optimization​","type":1,"pageTitle":"Architecture and design","url":"docs/xks/architecture-and-design#cost-optimization","content":"The platform team limits how much the clusters can auto scale and a service delivery manager together with the platform team helps the tenants understand their utilization and provides feedback to keep the cost at bay. "},{"title":"Container management​","type":1,"pageTitle":"Architecture and design","url":"docs/xks/architecture-and-design#container-management","content":"When a new tenant is being setup, the platform team provides onboarding for them and they then continously work together to assist in any questions. Monthly health checks are done to make sure that no obvious mistakes have been made by the tenants and monitoring is setup to warn the platform team if something is wrong with the platform. Most of the management of the workloads that the tenants deploy are handled through GitOps but they are also able to work with the clusters directly, with the knowledge that any cluster may at any time be rolled over (blue/green) and anything not in git will not be persisted. "},{"title":"Xenit Kubernetes Framework​","type":1,"pageTitle":"Architecture and design","url":"docs/xks/architecture-and-design#xenit-kubernetes-framework","content":"XKF is set up from a set of Terraform modules that when combined creates the full XKS service. There are multiple individual states that all fulfill their own purpose and build upon each other in a hierarchical manner. The first setup requires applying the Terraform in the correct order, but after that ordering should not matter. Separate states are used as it allows for a more flexible architecture that could be changed in parallel.  The AKS Terraform contains three modules that are used to setup a Kubernetes cluster. To allow for blue/green deployments of AKS clusters resources have to be split up into global resources that can be shared between the clusters, and cluster-specific resources. The aks-global module contains the global resources like ACR, DNS and Azure AD configuration. The aks and aks-core module creates an AKS cluster and configures it. This cluster will have a suffix, normally a number to allow for temporarily creating multiple clusters when performing a blue/green deployment of the clusters. Namespaces will be created in the cluster for each of the configured tenants. Each namespace is linked to a resource group in Azure where namespace resources are expected to be created.  "},{"title":"API migrations","type":0,"sectionRef":"#","url":"docs/xks/developer-guide/api-migrations","content":"","keywords":""},{"title":"Upgrading to Kubernetes 1.25​","type":1,"pageTitle":"API migrations","url":"docs/xks/developer-guide/api-migrations#upgrading-to-kubernetes-125","content":""},{"title":"CronJob​","type":1,"pageTitle":"API migrations","url":"docs/xks/developer-guide/api-migrations#cronjob","content":"CronJob apiVersion is moved from batch/v1beta1 to batch/v1. The only thing you need to do is to change the API version. From: apiVersion: batch/v1beta1 kind: CronJob metadata: name: foo Copy To: apiVersion: batch/v1 kind: CronJob metadata: name: foo Copy "},{"title":"HorizontalPodAutoscaler​","type":1,"pageTitle":"API migrations","url":"docs/xks/developer-guide/api-migrations#horizontalpodautoscaler","content":"HorizontalPodAutoscaler apiVersion is moved from autoscaling/v2beta1 to autoscaling/v2. Changes include the apiVersion but there's also changes to how targetAverageUtilization is used. See example below. From: apiVersion: autoscaling/v2beta2 kind: HorizontalPodAutoscaler metadata: name: foo spec: metrics: - type: Resource resource: name: cpu targetAverageUtilization: 50 - type: Resource resource: name: memory targetAverageUtilization: 50 Copy To: apiVersion: autoscaling/v2 kind: HorizontalPodAutoscaler metadata: name: foo spec: metrics: - type: Resource resource: name: cpu target: type: Utilization averageUtilization: 50 - type: Resource resource: name: memory target: type: Utilization averageUtilization: 50 Copy "},{"title":"PodDisruptionBudget​","type":1,"pageTitle":"API migrations","url":"docs/xks/developer-guide/api-migrations#poddisruptionbudget","content":"PodDisruptionBudget apiVersion is moved from policy/v1beta1 to policy/v1. The main thing you need to do is to change apiVersion. From: apiVersion: policy/v1beta1 kind: PodDisruptionBudget metadata: name: foo Copy To: apiVersion: policy/v1 kind: PodDisruptionBudget metadata: name: foo Copy Another important change is that in policy/v1 an empty spec.selector {} selects all pods in the namespace, when the previous policy/v1beta1 did not select any pods. "},{"title":"PodSecurityPolicy​","type":1,"pageTitle":"API migrations","url":"docs/xks/developer-guide/api-migrations#podsecuritypolicy","content":"PodSecurityPolicy in the policy/v1beta1 API version will no longer be served in v1.25, and the PodSecurityPolicy admission controller will be removed. More information can be found here. This is not something that we use in XKF, but it is good to know. "},{"title":"Patching​","type":1,"pageTitle":"API migrations","url":"docs/xks/developer-guide/api-migrations#patching","content":"If you have any kustomizations patching in different values between environments, or similar, these will also have to be changed to match the new apiVersion. From: - path: pdb-patch.yaml target: group: policy version: v1beta1 kind: PodDisruptionBudget name: app Copy To: - path: pdb-patch.yaml target: group: policy version: v1 kind: PodDisruptionBudget name: app Copy "},{"title":"General API changes​","type":1,"pageTitle":"API migrations","url":"docs/xks/developer-guide/api-migrations#general-api-changes","content":""},{"title":"SecretProviderClass v1alpha1 to v1​","type":1,"pageTitle":"API migrations","url":"docs/xks/developer-guide/api-migrations#secretproviderclass-v1alpha1-to-v1","content":"SecretProviderClass apiVersion is moved from secrets-store.csi.x-k8s.io/v1alpha1 to secrets-store.csi.x-k8s.io/v1. The only thing you need to do is to change is the API version. From: apiVersion: secrets-store.csi.x-k8s.io/v1alpha1 kind: SecretProviderClass metadata: name: foo spec: provider: &lt;provider&gt; parameters: objects: | - objectName: &quot;bar&quot; objectType: &quot;&lt;type&gt;&quot; - objectName: &quot;baz&quot; objectType: &quot;&lt;type&gt;&quot; Copy To: apiVersion: secrets-store.csi.x-k8s.io/v1 kind: SecretProviderClass metadata: name: foo spec: provider: &lt;provider&gt; parameters: objects: | - objectName: &quot;bar&quot; objectType: &quot;&lt;type&gt;&quot; - objectName: &quot;baz&quot; objectType: &quot;&lt;type&gt;&quot; Copy "},{"title":"Flux sourcecontroller v1beta1 to v1beta2​","type":1,"pageTitle":"API migrations","url":"docs/xks/developer-guide/api-migrations#flux-sourcecontroller-v1beta1-to-v1beta2","content":"The only thing that you need to change when moving source.toolkit.fluxcd.io from source.toolkit.fluxcd.io/v1beta1to source.toolkit.fluxcd.io/v1beta2 is the API version. In XKF we handle the upgrade of your default GitRepository, but if you have created some other source object you will need to update it on your own. Note that other Flux controllers (e.g. kustomization.toolkit.fluxcd.io and notification.toolkit.fluxcd.io) remain at v1beta1. "},{"title":"Continuous Delivery","type":0,"sectionRef":"#","url":"docs/xks/developer-guide/ci-cd/cd","content":"","keywords":""},{"title":"GitOps​","type":1,"pageTitle":"Continuous Delivery","url":"docs/xks/developer-guide/ci-cd/cd#gitops","content":""},{"title":"Azure AD Identity​","type":1,"pageTitle":"Continuous Delivery","url":"docs/xks/developer-guide/ci-cd/cd#azure-ad-identity","content":""},{"title":"GitOps Setup​","type":1,"pageTitle":"Continuous Delivery","url":"docs/xks/developer-guide/ci-cd/cd#gitops-setup","content":""},{"title":"Validation for GitOps Status​","type":1,"pageTitle":"Continuous Delivery","url":"docs/xks/developer-guide/ci-cd/cd#validation-for-gitops-status","content":"Important to remember is to setup the Build Validation for the Status pipeline to trigger correctly. That is done: In Azure devopsProject Settings -&gt; Repositories -&gt; 'your-gitops-repo' -&gt; On the 'Policies' tabSelect your default branch, usually 'main'. And on the 'Build Validation' section, press the '+' button.Select your Status pipelineEnter the following settings:Trigger: AutomaticPolicy requirement: RequiredBuild expiration: 12h Also see images below:    "},{"title":"Setup CI/CD pipeline​","type":1,"pageTitle":"Continuous Delivery","url":"docs/xks/developer-guide/ci-cd/cd#setup-cicd-pipeline","content":"At Xenit we have created a CI/CD template to make it easier to get started with GitOps in our case using FluxV2 and the GitOps toolkit. You can find the base for all our Azure DevOps pipelines in our Azure Devops Templates repo. Follow the example documentation on how to setup your base repo. Below we will explain how to do the manual steps that is needed to get Azure DevOps to enable some of the flows that we are creating. "},{"title":"Enable CI user to push to gitops repo​","type":1,"pageTitle":"Continuous Delivery","url":"docs/xks/developer-guide/ci-cd/cd#enable-ci-user-to-push-to-gitops-repo","content":"The core feature of the gitops repo is that one of the pipelines automatically updates the image tag in your repository so Flux will automatically update your deployment in Kubernetes. We have to grant it permissions to do this, sadly manually... In cases where there are multiple &quot;build services&quot; make sure that you are setting correct permissons the correct one. This appears to be a trial-and-error based mission. Use the Project Collection Build Service ({your organization}), not the group Project Collection Build Service Accounts ({your organization}).  "},{"title":"Service connections​","type":1,"pageTitle":"Continuous Delivery","url":"docs/xks/developer-guide/ci-cd/cd#service-connections","content":"To be able to talk from Azure DevOps to AKS using our gitops pipelines we also need to configure service connections to tenant namespace. Sadly setting up the Service Connections is a manual step. Get the needed config. az keyvault secret show --vault-name &lt;vault-name&gt; --name &lt;secret-name&gt; -o tsv --query value # Example az keyvault secret show --vault-name kv-prod-we-core-1337 --name sp-rg-xks-prod-backend-contributor -o tsv --query value # The output will look something like this {&quot;clientId&quot;:&quot;12345&quot;,&quot;clientSecret&quot;:&quot;SoMuchSecret&quot;,&quot;subscriptionId&quot;:&quot;sub-id&quot;,&quot;tenantId&quot;:&quot;tenant-id&quot;} Copy In Azure DevOps: Project settings -&gt; Service connections -&gt; New service connection -&gt; Azure Resource Manager -&gt; Service principal (manual) Subscription Id = subscriptionIdService Principal Id = clientIdService principal key = clientSecretTenant ID = tenantIdService connection name = random-name "},{"title":"Continuous Integration","type":0,"sectionRef":"#","url":"docs/xks/developer-guide/ci-cd/ci","content":"","keywords":""},{"title":"Hadolint​","type":1,"pageTitle":"Continuous Integration","url":"docs/xks/developer-guide/ci-cd/ci#hadolint","content":"Hadolint is a linter for container files and gives you suggestions on how to follow best practices. This check is enabled by default and you can disable it by adding: dockerLint: enabled: false ignoreRuleViolations: false Copy You can add a config file for hadolint where you canignore specific errors. "},{"title":"Trivy​","type":1,"pageTitle":"Continuous Integration","url":"docs/xks/developer-guide/ci-cd/ci#trivy","content":"Trivy is a container image scanner in a cli tool used to scan for CVE:s in both your code and base image. To scan the image: trivy &lt;image-name&gt; Copy After we have built the image we scan it and send you a report. If you want to ignore specific Trivy errors you can create a .trivyignore file. For example it can look like this: CVE-2020-29652 Copy If you want to disable Trivy you can do so by appending the following to your CI file. imageScan enable: true ignoreRuleViolations: true Copy "},{"title":"Horusec​","type":1,"pageTitle":"Continuous Integration","url":"docs/xks/developer-guide/ci-cd/ci#horusec","content":"A CLI tool to scan your application code for security issues that is disabled by default. "},{"title":"To enable Horusec in your CI pipeline​","type":1,"pageTitle":"Continuous Integration","url":"docs/xks/developer-guide/ci-cd/ci#to-enable-horusec-in-your-ci-pipeline","content":"Before enabling Horusec in your CI pipeline you should configure what issues your CI pipeline should catch: horusec generate Copy This will generate a file called horusec-config.json, in this file you can configure everything Horusec should scan and report. You can find more Horusec config flags here. Key config values: horusecCliSeveritiesToIgnore: What severities do you want to show?horusecCliFilesOrPathsToIgnore: What paths do you want to ignore? To save time it is easy to run Horusec locally: horusec start -p . Copy "},{"title":"GitOps a la XKS","type":0,"sectionRef":"#","url":"docs/xks/developer-guide/ci-cd/gitops","content":"","keywords":""},{"title":"What is GitOps?​","type":1,"pageTitle":"GitOps a la XKS","url":"docs/xks/developer-guide/ci-cd/gitops#what-is-gitops","content":"GitOps works by using Git as a single source of truth for declarative infrastructure and applications. With GitOps, the use of software agents can alert on any divergence between Git and what is running in [an environment]. If there is a difference, Kubernetes reconcilers automatically update or rollback the cluster depending on what is appropriate. ‐ Weave Works - Guide To GitOps XKS supports GitHub and Azure DevOps with almost identical workflows. XKF refers to these as Git providers. For simplicity, we refer to their CI/CD automation as &quot;pipelines&quot;. If you are using GitHub, whenever this text refers to &quot;pipeline&quot;, think &quot;GitHub Actions workflow&quot;. As you saw in the previous section, XKS comes with a set of pipelines that automatically detects app releases and promotes them through a series of environments. The allows both rapid iteration and strong validation of apps. XKS is built around trunk-based development. "},{"title":"User story: Emilia updates an app​","type":1,"pageTitle":"GitOps a la XKS","url":"docs/xks/developer-guide/ci-cd/gitops#user-story-emilia-updates-an-app","content":"In the previous section, we looked at deploying our first app to Kubernetes using a fully automatic flow. But what actually happened in that flow? This section tells the story about a developer called Emilia. She has updated an app packaged as a container image. A pipeline in the app's repository has just tagged the container image as a8b91c33 and uploaded it to a container registry. The GitOps repository for Emilia's app has a gitops-promotion.yaml that looks like this (for much more details, see the gitops-promotion readme): prflow: per-app environments: - env: dev auto: true - env: qa auto: true - env: prod auto: false Copy The dev and qa environments have auto: true which means that new releases will be automatically applied, while the prod environment is configured with auto: false. This means that pull requests must be merged by a human, presumably one that has verified that the update worked as expected in previous environments. The GitOps repository is configured to require checks on pull requests to pass in order to allow merge. "},{"title":"Applying to dev​","type":1,"pageTitle":"GitOps a la XKS","url":"docs/xks/developer-guide/ci-cd/gitops#applying-to-dev","content":"The flow is fully automatic and is triggered by the container image upload.  The / container image upload triggers a pipeline in the GitOps repository that runs the / gitops-promotion new command. It pushes a new branch and updates the dev environment manifest for the app with the new tag. It then opens an &quot;auto-merging&quot; pull request to integrate the new tag into the main branch.The pull request triggers another pipeline that runs / gitops-promotion status command. Since dev is the first environment in the list, it does nothing and reports success.The pull request check turns green and the pull request is automatically merged by the Git provider.The Flux Kustomization controller detects that there has been an update to the app's tag in the Git repository and applies this update to the Kubernetes resource for the app (typically a Deployment).The pods running the new container image came up healthy and so Flux sets a commit status on the main branch in the GitOps repository, reporting that the update was successfully applied. This will be significant in the next section. "},{"title":"Applying to qa​","type":1,"pageTitle":"GitOps a la XKS","url":"docs/xks/developer-guide/ci-cd/gitops#applying-to-qa","content":" Merging a promotion to the main branch triggers a pipeline in the GitOps repository that runs the gitops-promotion promote command. Like new, it creates a branch and updates the qa environment manifest for the app with the new tag. Because the configuration for this environment says auto: true it creates an auto-merging pull request.As before, this new pull request triggers another pipeline that runs the status command. This time there is a previous environment and the status command reads the Flux commit status for that environment. Since Flux managed to apply the change in dev the status command reports success.The pull request check turns green and the pull request is automatically merged by the Git provider.The Flux Kustomization controller detects that there has been an update to the app's tag in the Git repository and applies this update to the Kubernetes resource for the app (typically a Deployment).In this case, someone had applied manual changes to the app's database in the dev environment during development. These updates are not present in the qa environment, and when the pods running the new container image come up, they cannot read state from the database and so fail their health check. Flux consequently sets a commit status on the main branch of the GitHub repository, reporting that the update failed. Emilia's team has configured Flux to notify them when updates fail and so Emilia's chat client informs her that the update did not go through. "},{"title":"Application to prod is blocked​","type":1,"pageTitle":"GitOps a la XKS","url":"docs/xks/developer-guide/ci-cd/gitops#application-to-prod-is-blocked","content":" The workflow for applying to prod is similar to that of qa above, but since Flux reported failure when applying the update to qa, the pipeline running the status command will fail and the Git provider will block merging of the pull request. Seeing that the rollout failed, Emilia investigates and realizes that the release is missing a database migration script. She pushes an updated release tagged cc2b7e0a and so triggers the pipline running the new command. Because the configuration says prflow: per-app, the command &quot;resets&quot; the blocked pull request to apply to the updated release to the dev environment. "},{"title":"Second attempt applying to prod​","type":1,"pageTitle":"GitOps a la XKS","url":"docs/xks/developer-guide/ci-cd/gitops#second-attempt-applying-to-prod","content":" Emilia's updated app with database migration is successfully applied, first to the dev environment and then to the qa environment. The status check for the pull request against prod turns green and the pull request can be merged. Since the configuration says auto: false, the pull request is not automatically merged. Emilia can now verify the update in the qa environment and then merge the pull request through the Git provider's user interface. "},{"title":"Flux","type":0,"sectionRef":"#","url":"docs/xks/developer-guide/ci-cd/flux","content":"","keywords":""},{"title":"Flux CLI​","type":1,"pageTitle":"Flux","url":"docs/xks/developer-guide/ci-cd/flux#flux-cli","content":"You do not have to use the Flux CLI but it can be very helpful especially if want to force a reconciliation of a Flux resource. In some of the commands and debugging we assume that you have the CLI installed. Here you can find more information on how to setup the Flux CLI. "},{"title":"XKF and Flux​","type":1,"pageTitle":"Flux","url":"docs/xks/developer-guide/ci-cd/flux#xkf-and-flux","content":"In the XKF framework we talk a lot about tenants, from a Kubernetes point of view a tenant is a namespace that has been generated using Terraform. If you want more information on how that works you can look at the AZDO moduleand the GitHub module. The module populates the tenant namespaces by creating a basic config with a Flux GitRepository and Kustomization pointing to a pre-defined repository and path. It stores this in a central repository that normally your platform team manages and it should only be updated using Terraform. At the time of writing these docs the files generated could look something like this if you are using Azure DevOps (AZDO) and AZDO-proxy. As a member of tenant1 you will be able to see these resources in your namespace, in this case tenant1. You should never modify these resources manually, Flux will overwrite any manual changes back to the config defined in the git repository. --- apiVersion: source.toolkit.fluxcd.io/v1beta1 kind: GitRepository metadata: name: tenant1 namespace: tenant1 spec: # If you are using github libgit2 will not be defined gitImplementation: libgit2 interval: 1m # This example url assumes that you are using AZDO-proxy https://github.com/XenitAB/azdo-proxy url: http://azdo-proxy.flux-system.svc.cluster.local/Org1/project1/_git/gitops secretRef: name: flux ref: branch: main --- apiVersion: kustomize.toolkit.fluxcd.io/v1beta1 kind: Kustomization metadata: name: tenant1 namespace: tenant1 spec: serviceAccountName: flux interval: 5m path: ./tenant/dev sourceRef: kind: GitRepository name: tenant1 prune: true validation: client Copy "},{"title":"Debugging​","type":1,"pageTitle":"Flux","url":"docs/xks/developer-guide/ci-cd/flux#debugging","content":"Below, you will find a few good base commands to debug why Flux has not applied your changes. "},{"title":"Normal error​","type":1,"pageTitle":"Flux","url":"docs/xks/developer-guide/ci-cd/flux#normal-error","content":"When adding a new file to your GitOps repository do not forget to update the kustomization.yaml file. It can easily happen that you create a file in your repository and you commit it and when you look in the cluster it has not been synced. This is most likely due to that you have missed to update the kustomization.yaml file. apiVersion: kustomize.config.k8s.io/v1beta1 kind: Kustomization resources: - deployment.yaml - ingress.yaml - networkpolicy.yaml Copy "},{"title":"git repositories​","type":1,"pageTitle":"Flux","url":"docs/xks/developer-guide/ci-cd/flux#git-repositories","content":"Shows you the status if your changes have been synced to the cluster. $ kubectl get gitrepositories NAME URL READY STATUS AGE wt http://azdo-proxy.flux-system.svc.cluster.local/Org1/project1/_git/gitops True Fetched revision: main/9baa401630894b78ecc5fa5ebdf72c978583dea8 2d2h Copy Flux should automatically pull the changes to the cluster but if you think they sync takes to long time or you want to sync it for some other reason you can. Remember to provide the --namespace flag, else Flux will assume the source is in the flux-system namespace. flux reconcile source git tenant1 --namespace tenant1 Copy "},{"title":"Kustomization​","type":1,"pageTitle":"Flux","url":"docs/xks/developer-guide/ci-cd/flux#kustomization","content":"It is always good to check if Flux has applied your changes and if your health checks have passed. Overall the checksum of your source and the kustomization resource should be the same. $ kubectl get kustomizations NAME READY STATUS AGE apps-dev True Applied revision: main/9baa401630894b78ecc5fa5ebdf72c978583dea8 47h tenant1 True Applied revision: main/9baa401630894b78ecc5fa5ebdf72c978583dea8 2d2h Copy "},{"title":"Helm​","type":1,"pageTitle":"Flux","url":"docs/xks/developer-guide/ci-cd/flux#helm","content":"You can of course use flux to manage your helm charts. "},{"title":"Upgrade retries exhausted​","type":1,"pageTitle":"Flux","url":"docs/xks/developer-guide/ci-cd/flux#upgrade-retries-exhausted","content":"For different reasons, the helm release can come into a state of no more retries to apply your helm release. One of the errors that can be shown is Upgrade retries exhausted. $ kubectl get helmreleases.helm.toolkit.fluxcd.io NAME READY STATUS AGE app1 True Release reconciliation succeeded 14d app2 False upgrade retries exhausted 14d Copy After you have debugged and solved the issue you probably want to retrigger the reconciliation of the helmrelease. flux reconcile helmrelease app2 -n tenant1 Copy But due to a bug it's currently not possible. An easy workaround for this is to suspend and resume the helm release. flux suspend hr app2 -n tenant1 flux resume hr app2 -n tenant1 Copy "},{"title":"Golang","type":0,"sectionRef":"#","url":"docs/xenit-style-guide/golang","content":"","keywords":""},{"title":"Shared Library​","type":1,"pageTitle":"Golang","url":"docs/xenit-style-guide/golang#shared-library","content":"Some code may be best to share between multiple repositories. These shared packages should be stored in pkg. "},{"title":"Startup and Shutdown​","type":1,"pageTitle":"Golang","url":"docs/xenit-style-guide/golang#startup-and-shutdown","content":"It may seem a bit nit picky to document how a program should startup and shutdown, but it is necissary as there are a lot of resources like blogs which offer a multitude of solutions how it could be implemented. Doing this properly is important to make sure that all incoming messages such as HTTP requests and processed before shutting down, the alternative would be to cancel HTTP request without responding to them. There are generally two event soures which cause a shutdown, either an internal error which requires the program shutdown or an external signal notifying the program to shut down. In a lot of cases a program may run multiple go routines simultaneously, and example of this would be running both the buisness logic HTTP server and the HTTP server that is serving metrics. All of these would need to be gracefully stopped and also cause a graceful shutdown in case of an error. The errgroup package offers a solution to this problem by wrapping the go routines with an error handler which cancels a context when one of the go routines returns an error. package main import ( &quot;context&quot; &quot;fmt&quot; &quot;os&quot; &quot;os/signal&quot; &quot;time&quot; &quot;golang.org/x/sync/errgroup&quot; ) func main() { ctx, cancel := signal.NotifyContext(context.Background(), os.Interrupt) defer cancel() g, ctx := errgroup.WithContext(ctx) g.Go(func() error { &lt;-ctx.Done() fmt.Println(&quot;completed 1&quot;) return nil }) g.Go(func() error { var err error select { case &lt;-ctx.Done(): case &lt;-time.After(5 * time.Second): err = fmt.Errorf(&quot;example timeout error&quot;) } fmt.Println(&quot;completed 2&quot;) return err }) g.Go(func() error { &lt;-ctx.Done() fmt.Println(&quot;completed 3&quot;) return nil }) fmt.Println(&quot;running&quot;) if err := g.Wait(); err != nil { fmt.Printf(&quot;stopped with error: %v\\n&quot;, err) os.Exit(1) return } fmt.Println(&quot;stopped without error&quot;) } Copy All of the go routines will run at the same time. The program can be stopped with two methods. Either it runs for 5 seconds and the second go routine will return an error, or a signal will stop the program before the error is returned. Either way the context will be cancelled causing all of the go routines to run to completion. When this occurs the Wait() call returns wither with the error returned from one of the go routines or nil. This logic may seem complicated but is with the help of error groups simple to implement. A general good practice when building programs with multiple go routines is to allow the error group to manage the go routine. This means that all functions when possible should be blocking and should not implement there own threading and error handling. Instead if possible the functions should accept a context as a parameter and listen to the returned done channel. This increases testability of individual components while simplifying logic and decreasing risks of improper shutdowns and orphande go routines. "},{"title":"Program Input​","type":1,"pageTitle":"Golang","url":"docs/xenit-style-guide/golang#program-input","content":"Go has a lot of options when parsing program input flags and commands. On top of the standard flag library there are a bunch of other options out there that do the one or the other, or both. For this reason it is good to standardize on a library to use at Xenit. The most popular libraries at this time are cobra and by extension pflag. While these libraries may be popular they also introduce a lot of opinion and complexity to the project structure. A lightweight alternative is go-arg which offers the same feature set in a less opinionated manner. It offers most features that can be needed such as struct mapping, environment variables, default values, descriptions, short and long form, and argument lists. On top of these features it offers support for subcommands which allows for specific arguments which are only expected for specific subcommands. All flags are defined in a struct which is then populated with the input args. All configuration of the parsing is done through annotations. package main import ( &quot;fmt&quot; &quot;github.com/alexflint/go-arg&quot; ) type args struct { Id bool `arg:&quot;-i,--id,env:ID&quot; help:&quot;id input&quot;` Verbose bool `arg:&quot;-v,--verbose,env:VERBOSE&quot; default:&quot;true&quot; help:&quot;verbosity level&quot;` } func main() { a := &amp;args{} arg.MustParse(a) fmt.Printf(&quot;%#v\\n&quot;, a) } Copy Subcommands are useful when there are multiple functions that can be called in the same program. type CheckoutCmd struct { Branch string `arg:&quot;positional&quot;` Track bool `arg:&quot;-t&quot;` } type CommitCmd struct { All bool `arg:&quot;-a&quot;` Message string `arg:&quot;-m&quot;` } type PushCmd struct { Remote string `arg:&quot;positional&quot;` Branch string `arg:&quot;positional&quot;` SetUpstream bool `arg:&quot;-u&quot;` } var args struct { Checkout *CheckoutCmd `arg:&quot;subcommand:checkout&quot;` Commit *CommitCmd `arg:&quot;subcommand:commit&quot;` Push *PushCmd `arg:&quot;subcommand:push&quot;` Quiet bool `arg:&quot;-q&quot;` // this flag is global to all subcommands } arg.MustParse(&amp;args) switch { case args.Checkout != nil: fmt.Printf(&quot;checkout requested for branch %s\\n&quot;, args.Checkout.Branch) case args.Commit != nil: fmt.Printf(&quot;commit requested with message \\&quot;%s\\&quot;\\n&quot;, args.Commit.Message) case args.Push != nil: fmt.Printf(&quot;push requested from %s to %s\\n&quot;, args.Push.Branch, args.Push.Remote) } Copy Refer to the API Documentation for more detailed information. "},{"title":"Logging​","type":1,"pageTitle":"Golang","url":"docs/xenit-style-guide/golang#logging","content":"The options when logging with Go is usual many and opinionated, they depend on the required output format or how variables are passed. Currently the best compromise out there is logr which is a logging interface compatible with a lot of logging libraries. This means that the logging library can easily be replaced in the future without having to refactor the whole code base. A main feature of logr is that it supports structured logging, which means that parameters can easily be passed with the log message in a structured manner. caution Using the status field when logging can interfere with how Datadog interprets the severity of the log. If you have a log with fields status = 400 and level = error, then Datadog reports this as an info log. If you use another field for the status code, for example http_response.status_code, together with level = error, Datadog will report it as an error log. You can read more about the status field and other reserved log fields in Datadog here. Unless there are any other requirements it is good to use the zapr logger. Zapr is fast and will output logs as JSON. package main import ( &quot;context&quot; &quot;fmt&quot; &quot;github.com/go-logr/logr&quot; &quot;github.com/go-logr/zapr&quot; &quot;go.uber.org/zap&quot; ) func main() { zapLog, err := zap.NewProduction() if err != nil { panic(fmt.Sprintf(&quot;who watches the watchmen (%v)?&quot;, err)) } log := zapr.NewLogger(zapLog) log.Info(&quot;Logr in action!&quot;, &quot;the answer&quot;, 42) } Copy The logging object should be passed through the context when calling a function that needs to log. This removes the need to add an additional parameter that needs to be tracked. ctx := logr.NewContext(context.Background(), log) run(ctx) func run(ctx context.Context) { log := logr.FromContextOrDiscard(ctx) log.Info(&quot;running function&quot;) } Copy "},{"title":"Metrics​","type":1,"pageTitle":"Golang","url":"docs/xenit-style-guide/golang#metrics","content":"The Prometheus Go client contains packages which help instrumenting a project with metrics. The promhttp package help expose these metrics through an HTTP handler. Programs should expose their metrics through the path /metrics. It is important to NOT add the metrics handler to an existing HTTP server if one already exists. Metrics should be served on its own address that is not used by the buisness logic. This is to avoid exposing metrics, which should not but could, contain sensitive information to the public. The promauto package allows registering of new metrics in a Go native method. package main import ( &quot;context&quot; &quot;errors&quot; &quot;fmt&quot; &quot;net/http&quot; &quot;os&quot; &quot;os/signal&quot; &quot;time&quot; &quot;golang.org/x/sync/errgroup&quot; &quot;github.com/alexflint/go-arg&quot; &quot;github.com/prometheus/client_golang/prometheus&quot; &quot;github.com/prometheus/client_golang/prometheus/promauto&quot; &quot;github.com/prometheus/client_golang/prometheus/promhttp&quot; ) var ( opsProcessed = promauto.NewCounter(prometheus.CounterOpts{ Name: &quot;processed_ops_total&quot;, Help: &quot;The total number of processed events&quot;, }) ) type arguments struct { MetricsAddr string `arg:&quot;--metrics-addr&quot; default:&quot;:9090&quot;` } func main() { args := &amp;arguments{} arg.MustParse(args) ctx, cancel := signal.NotifyContext(context.Background(), os.Interrupt) defer cancel() g, ctx := errgroup.WithContext(ctx) mux := http.NewServeMux() mux.Handle(&quot;/metrics&quot;, promhttp.Handler()) srv := &amp;http.Server{ Addr: args.MetricsAddr, Handler: mux, } g.Go(func() error { if err := srv.ListenAndServe(); err != nil &amp;&amp; !errors.Is(err, http.ErrServerClosed) { return err } return nil }) g.Go(func() error { &lt;-ctx.Done() shutdownCtx, cancel := context.WithTimeout(context.Background(), 5*time.Second) defer cancel() return srv.Shutdown(shutdownCtx) }) g.Go(func() error { for { select { case &lt;-ctx.Done(): return nil case &lt;-time.After(2 * time.Second): opsProcessed.Inc() } } }) if err := g.Wait(); err != nil { fmt.Printf(&quot;stopped with error: %v\\n&quot;, err) os.Exit(1) return } } Copy "},{"title":"Tracing​","type":1,"pageTitle":"Golang","url":"docs/xenit-style-guide/golang#tracing","content":"To trace a request, HTTP headers carrying trace information are extracted from and injected into the request as it propagates through the microservices. A trace is made up of one or more spans. At the start of a request, a trace id is generated, which is the same for all spans, and a root span. For each service the request passes through, a new span is started as a child of the calling span.  "},{"title":"Datadog​","type":1,"pageTitle":"Golang","url":"docs/xenit-style-guide/golang#datadog","content":"The middleware to use depends on the tracing provider. The main thing that differs between the providers is the HTTP headers to use. The examples below uses Datadog. Datadog's default HTTP headers for trace and span id's are X-Datadog-Trace-Id and X-Datadog-Parent-Id. First, start the tracer. import &quot;gopkg.in/DataDog/dd-trace-go.v1/ddtrace/tracer&quot; func main() { tracer.Start() defer tracer.Stop() } Copy Then, add the middleware that extracts and injects the tracing headers. The example below uses middleware for gin. import ( gintrace &quot;gopkg.in/DataDog/dd-trace-go.v1/contrib/gin-gonic/gin&quot; &quot;github.com/gin-gonic/gin&quot; ) func main() { router := gin.New() router.Use(gintrace.Middleware(&quot;my-service-name&quot;)) } Copy To add tracing for other http frameworks and libraries, see Go Compatibility Requirements. It is recommended to use Datadog's Unified Service Tagging, to tag the traces with the environment where the service is run and its name and version. These variables can be set when calling tracer.Start() or as environment variables. "},{"title":"Connect logs and traces​","type":1,"pageTitle":"Golang","url":"docs/xenit-style-guide/golang#connect-logs-and-traces","content":"To connect logs and traces, log the span id of the current span. An example of a custom gin middleware logger that uses Datadog as tracing provider is shown below. It adds a dd field that holds the trace and span id. Read more on how to connect logs and traces in Datadog here. type span struct { SpanID uint64 `json:&quot;span_id&quot;` TraceID uint64 `json:&quot;trace_id&quot;` } func getSpan(c *gin.Context) span { span, _ := tracer.SpanFromContext(c.Request.Context()) return span{ SpanID: span.Context().SpanID(), TraceID: span.Context().TraceID(), } } func ginlogrWithSpan(logger logr.Logger) gin.HandlerFunc { return func(c *gin.Context) { path := c.Request.URL.Path span := getSpan() // Extract other values to log // ... logger.Info(path, // Other fields // ... &quot;dd&quot;, span, ) } } Copy "},{"title":"HTTP​","type":1,"pageTitle":"Golang","url":"docs/xenit-style-guide/golang#http","content":"The Go standard library includes a http package that works well for simple applications, but requires a lot of custom code when building larger projects. While it is fine to just use the standard library for simple applications it is preferable to switch to Gin as project feature requirements develop. Gin provides extra functionality and extensions to simplify things like parsing parameters in the URL path and endpoint authorization. The HTTP server should be started and stopped in accordance to the Startup and Shutdown documentation. The HTTP server should be gracefully stopped before the program exits to make sure that all in flight requests are processed. It is highly recommended to use the default Gin router from Xenit Gin PKG. It sets up default midleware such as logging and metrics which all applications should configure. The option to extend additional functionality is still possible but it removes the need to think about the best practices. package main import ( &quot;context&quot; &quot;errors&quot; &quot;fmt&quot; &quot;net/http&quot; &quot;os&quot; &quot;os/signal&quot; &quot;time&quot; &quot;golang.org/x/sync/errgroup&quot; &quot;github.com/alexflint/go-arg&quot; &quot;github.com/gin-gonic/gin&quot; &quot;github.com/go-logr/zapr&quot; &quot;go.uber.org/zap&quot; pkggin &quot;github.com/xenitab/pkg/gin&quot; ) type arguments struct { Addr string `arg:&quot;--addr&quot; default:&quot;:8080&quot;` } func main() { args := &amp;arguments{} arg.MustParse(args) zapLog, err := zap.NewProduction() if err != nil { panic(fmt.Sprintf(&quot;who watches the watchmen (%v)?&quot;, err)) } log := zapr.NewLogger(zapLog) ctx, cancel := signal.NotifyContext(context.Background(), os.Interrupt) defer cancel() g, ctx := errgroup.WithContext(ctx) router := pkggin.Default(logger) router.GET(&quot;/ping&quot;, func(c *gin.Context) { c.JSON(http.StatusOK, gin.H{ &quot;message&quot;: &quot;pong&quot;, }) }) srv := &amp;http.Server{ Addr: args.Addr, Handler: router, } g.Go(func() error { if err := srv.ListenAndServe(); err != nil &amp;&amp; !errors.Is(err, http.ErrServerClosed) { return err } return nil }) g.Go(func() error { &lt;-ctx.Done() shutdownCtx, cancel := context.WithTimeout(context.Background(), 5*time.Second) defer cancel() return srv.Shutdown(shutdownCtx) }) fmt.Println(&quot;running&quot;) if err := g.Wait(); err != nil { fmt.Printf(&quot;stopped with error: %v\\n&quot;, err) os.Exit(1) return } fmt.Println(&quot;stopped without error&quot;) } Copy "},{"title":"GitOps Repository Structure","type":0,"sectionRef":"#","url":"docs/xks/developer-guide/ci-cd/repo-structure","content":"","keywords":""},{"title":"Repository Structure​","type":1,"pageTitle":"GitOps Repository Structure","url":"docs/xks/developer-guide/ci-cd/repo-structure#repository-structure","content":"There are 2 ways one can structure the GitOps repository, based upon how you want the kustomizations in flux to work. Per ENV kustomization Somewhat easier to maintain and configure, every app relies on one reconciliation of the kustomization.One kustomization per ENV in the Flux-configuration, looking something like this: apiVersion: kustomize.toolkit.fluxcd.io/v1beta1 kind: Kustomization metadata: name: apps-dev spec: serviceAccountName: flux interval: &quot;1m&quot; timeout: &quot;30s&quot; prune: true sourceRef: kind: GitRepository name: giops path: &quot;./apps/dev&quot; Copy Per APP kustomization More granular control and every app has its own reconciliation and kustomization.One kustomization per app in the Flux-configuration, looking something like this:  apiVersion: kustomize.toolkit.fluxcd.io/v1beta1 kind: Kustomization metadata: name: app1-dev spec: serviceAccountName: flux interval: &quot;1m&quot; timeout: &quot;30s&quot; prune: true sourceRef: kind: GitRepository name: gitops path: &quot;./App1/dev&quot; Copy Below we will show examples of the repository structure to achieve both cases. "},{"title":"Per ENV - Repository Structure​","type":1,"pageTitle":"GitOps Repository Structure","url":"docs/xks/developer-guide/ci-cd/repo-structure#per-env---repository-structure","content":" GitOps ├── Apps/ │ ├── dev/ │ │ ├── kustomization.yaml │ │ ├── App1/ │ │ │ ├── kustomization.yaml │ │ │ ├── deployment.yaml │ │ │ ├── service.yaml │ │ │ └── ingress.yaml │ │ ├── App2/ │ │ │ ├── kustomization.yaml │ │ │ ├── deployment.yaml │ │ │ ├── service.yaml │ │ │ └── ingress.yaml │ │ ├── secret-provider-class-patch.yaml │ │ ├── certificate.yaml │ ├── qa/ │ │ ├── kustomization.yaml │ │ ├── App1/ │ │ │ ├── kustomization.yaml │ │ │ ├── deployment.yaml │ │ │ ├── service.yaml │ │ │ └── ingress.yaml │ │ ├── App2/ │ │ │ ├── kustomization.yaml │ │ │ ├── deployment.yaml │ │ │ ├── service.yaml │ │ │ └── ingress.yaml │ │ ├── secret-provider-class-patch.yaml │ │ ├── certificate.yaml │ ├── prod/ │ │ ├── kustomization.yaml │ │ ├── App1/ │ │ │ ├── kustomization.yaml │ │ │ ├── deployment-patch.yaml │ │ │ └── ingress-patch.yaml │ │ ├── App2/ │ │ │ ├── kustomization.yaml │ │ │ ├── deployment-patch.yaml │ │ │ └── ingress-patch.yaml │ │ ├── secret-provider-class-patch.yaml │ │ ├── certificate.yaml ├── Tenant/ │ └── Platform / Flux configuration. Copy "},{"title":"Per APP - Repository Structure​","type":1,"pageTitle":"GitOps Repository Structure","url":"docs/xks/developer-guide/ci-cd/repo-structure#per-app---repository-structure","content":" GitOps ├── App1/ │ ├── base/ │ │ ├── kustomization.yaml │ │ ├── deployment.yaml │ │ ├── service.yaml │ │ └── ingress.yaml │ ├── dev/ │ │ ├── kustomization.yaml │ │ ├── deployment.yaml │ │ ├── service.yaml │ │ └── ingress.yaml │ ├── qa/ │ │ ├── kustomization.yaml │ │ ├── deployment.yaml │ │ ├── service.yaml │ │ └── ingress.yaml │ ├── prod/ │ │ ├── kustomization.yaml │ │ ├── deployment.yaml │ │ ├── service.yaml │ │ └── ingress.yaml ├── App2/ │ ├── dev/ │ │ ├── kustomization.yaml │ │ ├── deployment.yaml │ │ ├── service.yaml │ │ └── ingress.yaml │ ├── qa/ │ │ ├── kustomization.yaml │ │ ├── deployment.yaml │ │ ├── service.yaml │ │ └── ingress.yaml │ ├── prod/ │ │ ├── kustomization.yaml │ │ ├── deployment.yaml │ │ ├── service.yaml │ │ └── ingress.yaml ├── Apps/ │ ├── dev/ │ │ ├── kustomization.yaml │ │ ├── secret-provider-class.yaml │ │ ├── certificate.yaml │ ├── qa/ │ │ ├── kustomization.yaml │ │ ├── secret-provider-class.yaml │ │ ├── certificate.yaml │ ├── prod/ │ │ ├── kustomization.yaml │ │ ├── secret-provider-class.yaml │ │ ├── certificate.yaml ├── Tenant/ │ └── Platform / Flux configuration. Copy Note that we in this example also utilize the &quot;Per ENV&quot; method for things that are used by all applications, such as the certificates we create, we only create one certificate resource containing multiple SANs for each application. Each app also uses the same Key Vault, therefor this can be the same also. Another repository structure worth mentioning is using a /base folder for each application, in both the &quot;Per ENV&quot; and &quot;Per App&quot; structures. This will enable you to have a &quot;Common&quot; folder containing all the configuration that is the same between your environments and then patch the correct values in each ENV folder. "},{"title":"Container Security","type":0,"sectionRef":"#","url":"docs/xks/developer-guide/container-security","content":"","keywords":""},{"title":"Default Security Context​","type":1,"pageTitle":"Container Security","url":"docs/xks/developer-guide/container-security#default-security-context","content":"All Pods in the cluster will have a default security context enforced. Any default security context setting which has not been configured properly when applying the Pod to the cluster will be automatically set before the Pod is created. No action has to be taken from the end user but it is important to be aware that the Pod applied may not be identical to the Pod created and what these settings mean. The Pod below is the most basic yet valid Pod which can be applied to a Kubernetes. apiVersion: v1 kind: Pod metadata: name: app spec: containers: - name: app image: busybox:1.35.0 Copy When applied to an XKF Kubernetes cluster the following mutations occur. apiVersion: v1 kind: Pod metadata: name: app spec: automountServiceAccountToken: false containers: - name: app image: busybox:1.35.0 securityContext: readOnlyRootFilesystem: true allowPrivilegeEscalation: false capabilities: drop: - NET_RAW - CAP_SYS securityContext: seccompProfile: type: RuntimeDefault Copy "},{"title":"Read Only Root Filesystem​","type":1,"pageTitle":"Container Security","url":"docs/xks/developer-guide/container-security#read-only-root-filesystem","content":"This setting is the one that may have the largest impact on a deployed application if it is expected to write to the file system. Containers may be ephemeral but that does not mean that certain applications may still want to write temporary files. Enabling read only root filesystem blocks the container from writing to its filesystem by default. The reasoning for this is that if an attacker is able to gain access to a container the attacker will not be able to modify any files or add any additional binaries to the container. Of course there is a solution for applications that still do want to write to the filesystem. The solution is to create an emptyDir volume and mount it to the directory to which files should be written. In the example below the emptyDir volume tmp is mounted to the path /tmp making it possible to write files to the directory. apiVersion: v1 kind: Pod metadata: name: app spec: containers: - name: app image: busybox:1.35.0 securityContext: readOnlyRootFilesystem: true allowPrivilegeEscalation: false capabilities: drop: - NET_RAW - CAP_SYS_ADMIN volumeMounts: - mountPath: /tmp name: tmp volumes: - emptyDir: {} name: tmp Copy "},{"title":"Allow Privilege Escalation​","type":1,"pageTitle":"Container Security","url":"docs/xks/developer-guide/container-security#allow-privilege-escalation","content":"Disabling privilege escalation for all Pods is critical for applications to not gain greater access to the Node than they need to. This setting will probably never affect most applications. "},{"title":"Dropped Capabilities​","type":1,"pageTitle":"Container Security","url":"docs/xks/developer-guide/container-security#dropped-capabilities","content":"Linux capabilities are certain root permissions that can be given to containers without giving root access. Certain capabilities should never be given to a container as they would give too much access, for this reason they are always dropped. CAP_SYS_ADMIN - Is the same as making the container root, as it would allow a container to escalate privileges.NET_RAW - Allows a container to craft raw network packets which can be exploited for malicious actions. "},{"title":"Pod Constraints​","type":1,"pageTitle":"Container Security","url":"docs/xks/developer-guide/container-security#pod-constraints","content":"In addition to the default security context there are certain configuration settings which are not allowed in XKF. They have a lower impact on end users as they are opted into, but they are good to be aware of and can explain why a Pods creation may be blocked by the cluster. The enforcement is done before the Pod is created, this means that a Deployment may be allowed to be created but the Pods that result will not, causing a Deployment to always have zero replicas. "},{"title":"Host Namespaces​","type":1,"pageTitle":"Container Security","url":"docs/xks/developer-guide/container-security#host-namespaces","content":"Being able to share process IDs or memory between a container and a host means that the container would in theory have access to the host or any containers also running on the host. Setting hostPID or hostIPC to true would allow for this to happen. For this reason that is not allowed. apiVersion: v1 kind: Pod metadata: name: app spec: hostPID: true hostIPC: true Copy "},{"title":"Host Networking​","type":1,"pageTitle":"Container Security","url":"docs/xks/developer-guide/container-security#host-networking","content":"Allowing a Pod access to the host network namespace would mean giving it access to the hosts loopback device and enable it to listen to the network traffic of all Pods on the host. apiVersion: v1 kind: Pod metadata: name: app spec: hostNetwork: true Copy Using a host port for a Pod's container will result in binding that hosts port to the container port. This would expose the container to the network outside of the cluster. This can cause issues in cases where the ports have to be unique, and is generally a feature that is not needed in XKF, which is why it is not allowed. apiVersion: v1 kind: Pod metadata: name: app spec: containers: - ports: - hostPort: 10902 Copy "},{"title":"Privileged Containers​","type":1,"pageTitle":"Container Security","url":"docs/xks/developer-guide/container-security#privileged-containers","content":"Privileged containers have root capabilities on the host machine and are therefore not allowed in XKF. There would be no separation between the container and the host if this setting were to be allowed. apiVersion: v1 kind: Pod metadata: name: app spec: containers: - securityContext: privileged: true Copy "},{"title":"Capabilities​","type":1,"pageTitle":"Container Security","url":"docs/xks/developer-guide/container-security#capabilities","content":"As stated in the default security context section certain capabilities will always be dropped. It is however also possible to grant capabilities to a Pod. Therefore any Pods that attempt to add extra capabilities are not allowed, as it could lead to privilege escalation. apiVersion: v1 kind: Pod metadata: name: app spec: containers: - securityContext: capabilities: add: - NEW_RAW Copy "},{"title":"Proc Mount​","type":1,"pageTitle":"Container Security","url":"docs/xks/developer-guide/container-security#proc-mount","content":"There are certain files in the /proc directory that may be sensitive if exposed to a container. By default the container runtime will apply a filter to the files mounted to the container from the directory, so opting out of it is not allowed. apiVersion: v1 kind: Pod metadata: name: app spec: containers: - securityContext: procMount: UnmaskedProcMount Copy "},{"title":"Volume Type​","type":1,"pageTitle":"Container Security","url":"docs/xks/developer-guide/container-security#volume-type","content":"The types of volumes are limited to a known set to mitigate host path mounting and the use of unknown volume types. The allowed volume types cover the majority of use cases. The following volume types are currently allowed in XKF, configMap, downwardAPI, emptyDir, persistentVolumeClaim, secret, projected, csi. As stated before blocking any hostPath volumes is critical as mounting certain directories or files on the host would give a container insight to or control over the host. apiVersion: v1 kind: Pod metadata: name: app spec: volumes: - hostPath: path: /proc Copy "},{"title":"Flex Volumes​","type":1,"pageTitle":"Container Security","url":"docs/xks/developer-guide/container-security#flex-volumes","content":"Flex Volumes are a deprecated type of volume that are no longer maintained in the Kubernetes project. Flex volume drives are not installed in XKF clusters but are still not allowed to be used. "},{"title":"Non Root User​","type":1,"pageTitle":"Container Security","url":"docs/xks/developer-guide/container-security#non-root-user","content":"A common practice for a lot of images is to, by default, run the containers as the user root. It is widespread because it is the deafault in a lot of the base images, but it does not mean that it is the best choice. Running the container as root is generally not a problem as it is not the same as root on the node. It does however become a problem when new container runtime vulnerabilities are found which allow escaping the sandbox. An example of such a vulnerability is CVE-2019-5736 which allowed containers running as root users to escalate its privileges to becoming root on the node. A mitigation for this vulnerability would have been to make sure that the container is running as user other than root. With the knowledge that the mitigation for future container vulnerabilities is so simple it would be lazy not to take the precautionary steps. Changing the user has to be solved both at the source of the image and in Kubernetes to ensure no issues appear. A common problem is that the new non root user does not have the correct permissions for required files and directories. A general recommendation is also to use a user ID (UID) and group ID (GID) greater than 10000 to avoid overlapping with other system users. Create a Dockerfile which changes the UID and GID from the default of the base image. FROM alpine:latest RUN addgroup -g 10000 app &amp;&amp; adduser -u 10000 -G app -D app USER 10000 Copy Configure the Pod that that is runs with a non root user. The field runAsUser has to be configured when runAsNonRoot is set to true. Set both these values to be the UID and GID configured in the Dockerfile. apiVersion: v1 kind: Pod metadata: name: app spec: securityContext: runAsNonRoot: true runAsUser: 10000 runAsGroup: 10000 Copy In the future enforcement of running as a non root users will be handled by XKF. When this is enabled a containers attempting to run with the UID 0 will not be permitted. The minimum UID and GID will also be enforced to 10000. "},{"title":"Seccomp​","type":1,"pageTitle":"Container Security","url":"docs/xks/developer-guide/container-security#seccomp","content":"Secure computing mode (seccomp) is a pod wide securityContext setting and is a way to restrict which system calls a application can make inside a container. XKF is configured to mutate all Pods, which do not specify a seccomp profile, with the profiler RuntimeDefault. This is a security measure to give you as a developer a good base to stand-on while minimizing the risk of getting issues in your application. It is possible to use a different profile if the application for some reason does not work as intended with the RuntimeDefaultprofile. To allow all system calls the application can use the profile Unconfined. The Pod will not be mutated as long as a seccomp profile has been set in the security context. apiVersion: v1 kind: Pod metadata: name: test spec: securityContext: seccompProfile: type: Unconfined containers: - command: - /bin/sh resources: requests: memory: &quot;16Mi&quot; cpu: &quot;10m&quot; limits: memory: &quot;64Mi&quot; cpu: &quot;100m&quot; image: alpine:latest name: container-00 tty: true Copy To see which system calls is disabled in RuntimeDefault the most human readable option we have found is dockers profile page. Another option is to read the containerd code, they might not be identical but it's close enough. RuntimeDefault isn't a ideal solution and in the long run we hope to add support to something like security-profile-operator to XKF. "},{"title":"Automount ServiceAccount Token​","type":1,"pageTitle":"Container Security","url":"docs/xks/developer-guide/container-security#automount-serviceaccount-token","content":"By default all pods uses the default serviceAccount. The pod will also automatically get the default serviceAccounts Kubernetes token. Most application workloads don't have a need for this token. XKF is configured to mutate all Pods, which do not specify explicitly spec.automountServiceAccountToken and will set the value to false by default. apiVersion: v1 kind: Pod metadata: name: app spec: automountServiceAccountToken: false containers: - name: app image: busybox:1.35.0 Copy "},{"title":"Vulnerability Reports​","type":1,"pageTitle":"Container Security","url":"docs/xks/developer-guide/container-security#vulnerability-reports","content":"Containers can use the same image for a very long time, either because the application has not been updated for a long time, or because an external image is used and not updated as new versions are released. These old image versions may collect security vulnerabilities as new ones are discovered and patched in later releases, but it does not mean that an image is insecure just because an image version is old. XKF helps developers detect security vulnerabilities in container images with the help of Starboard. One thing Starboard does is to continuously scan images used in the cluster with the help of Trivy. Any vulnerability found will be stored as a resource in the Namespace in which it was found in. View all the vulnerability reports that exists in the current namespace. kubectl get vulnerabilityreports Copy Describe a specific vulnerability report to view specific details. kubectl describe vulnerabilityreports.aquasecurity.github.io &lt;report-name&gt; Copy Below is an example of how the output may look. It includes information of the scanner used, when the scan was run, and the vulnerabilities found. Each vulnerability has a CVE ID and a link for more information about the vulnerability. Report: Scanner: Name: Trivy Vendor: Aqua Security Version: 0.24.3 Summary: Critical Count: 0 High Count: 3 Low Count: 0 Medium Count: 1 Unknown Count: 0 Update Timestamp: 2022-04-04T08:49:01Z Vulnerabilities: Fixed Version: 1.1.1n-r0 Installed Version: 1.1.1l-r0 Links: Primary Link: https://avd.aquasec.com/nvd/cve-2022-0778 Resource: libcrypto1.1 Score: 7.5 Severity: HIGH Title: openssl: Infinite loop in BN_mod_sqrt() reachable when parsing certificates Vulnerability ID: CVE-2022-0778 Fixed Version: 3.3.3p1-r3 Installed Version: 3.3.3p1-r2 Links: Primary Link: https://avd.aquasec.com/nvd/cve-2022-0778 Resource: libretls Score: 7.5 Severity: HIGH Title: openssl: Infinite loop in BN_mod_sqrt() reachable when parsing certificates Vulnerability ID: CVE-2022-0778 Fixed Version: 1.1.1n-r0 Installed Version: 1.1.1l-r0 Links: Primary Link: https://avd.aquasec.com/nvd/cve-2022-0778 Resource: libssl1.1 Score: 7.5 Severity: HIGH Title: openssl: Infinite loop in BN_mod_sqrt() reachable when parsing certificates Vulnerability ID: CVE-2022-0778 Fixed Version: 1.2.12-r0 Installed Version: 1.2.11-r3 Links: Primary Link: https://avd.aquasec.com/nvd/cve-2018-25032 Resource: zlib Score: 4.4 Severity: MEDIUM Title: zlib: A flaw in zlib-1.2.11 when compressing (not decompressing!) certain inputs. Vulnerability ID: CVE-2018-25032 Copy The presence of a vulnerability does not mean that a patched version exists yet, or even that the image is inherently insecure. The vulnerability may affect components that are not used or have low risk to being exposed. It is however important to be conscious of the vulnerabilities that exist and consider future work to take responsibility for fixing them. Refer to the official Starboard documentation for more specific documentation. "},{"title":"Pod max PID​","type":1,"pageTitle":"Container Security","url":"docs/xks/developer-guide/container-security#pod-max-pid","content":"XKF has set a maximum limit on how many PIDs a single Pod can utilize. This is to make sure that a Pod can't cause PID starvation for other Pods by consuming all available PIDs on the host OS. The max value today is 1000. Configuration changes on cluster level have to be made in the case where a Pod requires more than 1000 PIDs. "},{"title":"Cloud IAM","type":0,"sectionRef":"#","url":"docs/xks/developer-guide/cloud-iam","content":"","keywords":""},{"title":"Cloud Providers​","type":1,"pageTitle":"Cloud IAM","url":"docs/xks/developer-guide/cloud-iam#cloud-providers","content":""},{"title":"Azure​","type":1,"pageTitle":"Cloud IAM","url":"docs/xks/developer-guide/cloud-iam#azure","content":"The recommended way to authenticate towards Azure in XKS is to make use of AAD Pod Identity which runs inside the cluster. AAD Pod Identity allows Pods within the cluster to use managed identities to authenticate towards Azure. This removes the need for static credentials that have to be passed to the Pods. It works by intercepting API requests before they leave the cluster and will attach the correct credential based on the source Pod of the request. Each tenant namespace comes preconfigured with an AzureIdentity andAzureIdentityBinding. These have been setup so that the identity has access to the tenant's resource group. All that has to be done to enable the managed identity is to add the label foo to the Pod. The preconfigured AzureIdentity has a labelselector which expects the label to have the same value as the namespace name. This example will deploy a Pod with the Azure CLI so that you can test access to the Azure API. apiVersion: v1 kind: Pod metadata: name: msi-test namespace: tenant labels: aadpodidbinding: ${NAMESPACE_NAME} spec: containers: - name: msi-test image: mcr.microsoft.com/azure-cli tty: true volumeMounts: - name: az-cli mountPath: /root/.azure volumes: - name: az-cli emptyDir: {} Copy After the Pod has started you can execute a shell in the Pod and verify that the managed identity is working. kubectl -n tenant exec -it msi-test az login --identity az account show Copy Make sure your application supports retries when retrieving tokens. It should at least be able to retry for 60 seconds. Read more about it here. More good practices can be found in the aad-pod-identity docs. SDK​ A common scenario is that an application may need API access to Azure resources through the API. In these cases the best solution is to use the language specific SDKs which will most of the time support MSI credentials. Below are examples for how to create a client using MSI credentials which can interact with Azure storage account blobs.  Golang  package main import ( &quot;time&quot; &quot;github.com/Azure/go-autorest/autorest/azure/auth&quot; blob &quot;github.com/Azure/azure-storage-blob-go/azblob&quot; ) func main() { msiConfig := auth.NewMSIConfig() spt, err := msiConfig.ServicePrincipalToken() if err != nil { return nil, err } if err := spt.Refresh(); err != nil { return nil, err } tc, err := blob.NewTokenCredential(spt.Token().AccessToken, func(tc blob.TokenCredential) time.Duration { err := spt.Refresh() if err != nil { return 30 * time.Second } tc.SetToken(spt.Token().AccessToken) return spt.Token().Expires().Sub(time.Now().Add(2 * time.Minute)) }), nil } Copy  C# with ASP.NET  using Azure; using Azure.Identity; using Azure.Storage.Blobs; async static Task CreateBlockBlobAsync(string accountName, string containerName, string blobName) { string containerEndpoint = string.Format(&quot;https://{0}.blob.core.windows.net/{1}&quot;, accountName, containerName); BlobContainerClient containerClient = new BlobContainerClient(new Uri(containerEndpoint), new DefaultAzureCredential()); } Copy Limiting Permissions​ TBD "},{"title":"AWS​","type":1,"pageTitle":"Cloud IAM","url":"docs/xks/developer-guide/cloud-iam#aws","content":"When authenticating towards AWS in XKS we recommend using IAM Roles for Service Accounts (IRSA). IRSA works by intercepting AWS API calls before leaving the cluster and appending the correct authentication token to the request. This removes the need for static security credentials as it is handled outside the app. IRSA works by annotating a Service Account with a reference to a specfic AWS IAM role. When that Service Account is attached to a Pod, the Pod will be able to assume the IAM role. The reason IRSA works in a multi-tenant cluster is because the reference is multi-directional. The Service Account has to specify the full role ARN it wants to assume and the IAM role has to specify the name and namespace of the Service Account which is allowed to assume the role. So it is not enough to know the ARN of the role unless you have access to the correct namespace and Service Account. Start by defining a variable for the OIDC URLs that should be trusted. Currently this is a static definition that needs to be specified but work is planned to make this value discoverable in the future. variable &quot;oidc_urls&quot; { description = &quot;List of EKS OIDC URLs to trust.&quot; type = list(string) } Copy A new OIDC provider has to be created for each trusted URL. The simplest way to do this is to iterate the URL list. This should only be done once per tenant account, so try to define all roles in the same Terraform state. data &quot;tls_certificate&quot; &quot;this&quot; { for_each = { for v in var.oidc_urls : v =&gt; v } url = each.value } resource &quot;aws_iam_openid_connect_provider&quot; &quot;this&quot; { for_each = { for v in var.oidc_urls : v =&gt; v } client_id_list = [&quot;sts.amazonaws.com&quot;] thumbprint_list = [data.tls_certificate.this[each.value].certificates[0].sha1_fingerprint] url = each.value } Copy Define an AWS IAM policy document and an instance of the IRSA Terraform module. The policy document describes which permissions should be granted to a Pod and the IRSA module creates the IAM policy and role for a Service Account in a specific namespace. The example below will for example only work with a Service Account called irsa-test in the namespace tenant. Keep in mind that a policy document and module instance is required for each unique permission set. data &quot;aws_iam_policy_document&quot; &quot;get_login_profile&quot; { statement { effect = &quot;Allow&quot; actions = [ &quot;iam:GetLoginProfile&quot;, ] resources = [&quot;*&quot;] } } module &quot;irsa_test&quot; { source = &quot;github.com/xenitab/terraform-modules//modules/aws/irsa?ref=2021.08.9&quot; name = &quot;irsa-test&quot; oidc_providers = [ for v in var.oidc_urls : { url = v arn = aws_iam_openid_connect_provider.this[v].arn } ] kubernetes_namespace = &quot;tenant&quot; kubernetes_service_account = &quot;irsa-test&quot; policy_json = data.aws_iam_policy_document.get_login_profile.json } Copy It is a good idea to output the ARN of the created role, as it will be needed in the next step. output &quot;irsa_test_arn&quot; { value = module.irsa_test.role_arn } Copy The correct IAM roles and policies should be created after the Terraform has been applied. The next step is to create a Service Account with the same name as specified in the IRSA module and annotate it with the key eks.amazonaws.com/role-arn. The value should be the full ARN of the created IAM role. Note that the account id is part of the ARN as the IAM role is created in a different account than the one the cluster is located in. apiVersion: v1 kind: ServiceAccount metadata: name: irsa-test namespace: tenant annotations: eks.amazonaws.com/role-arn: arn:aws:iam::111111111111:role/irsa-test Copy Create a Pod using the newly created Service Account to test using the IAM role. apiVersion: v1 kind: Pod metadata: name: irsa-test namespace: tenant spec: serviceAccountName: irsa-test containers: - name: irsa-test image: amazon/aws-cli command: [&quot;sh&quot;] stdin: true tty: true Copy After the Pod has started you can execute a shell in the Pod and verify that the managed identity is working. kubectl -n tenant exec -it irsa-test aws sts get-caller-identity Copy SDK​ TBD "},{"title":"Introduction","type":0,"sectionRef":"#","url":"docs/xks/developer-guide/introduction","content":"","keywords":""},{"title":"Getting Started​","type":1,"pageTitle":"Introduction","url":"docs/xks/developer-guide/introduction#getting-started","content":"A good starting point is to get CLI access to the clusters. It is useful to have manual access to the clusters for debugging purposes. Try to avoid doing any changes to cluster resources with the CLI as the changes will be difficult to track and update in the future. There are a couple of prerequisites that have to be met before getting access however. Start off by installing the Azure CLI. az login Copy "},{"title":"Kubectl Configuration​","type":1,"pageTitle":"Introduction","url":"docs/xks/developer-guide/introduction#kubectl-configuration","content":"You can run the following commands to add the AKS cluster to your kubeconfig, assuming that you have installed the Azure CLIand authenticated with the Azure portal. "},{"title":"Using AZAD Proxy​","type":1,"pageTitle":"Introduction","url":"docs/xks/developer-guide/introduction#using-azad-proxy","content":"If you use the AZAD proxy you can find documentation to help you set it up here: AZAD Documentation "},{"title":"Otherwise​","type":1,"pageTitle":"Introduction","url":"docs/xks/developer-guide/introduction#otherwise","content":"Once you have logged in you can list your subscriptions az account list -o table Copy In the case where you have more than one subscription, you might want to change the default subscription in order to target the correct environment. az account set -s &lt;SubscriptionId&gt; Copy To get information about cluster name and resource group for your current default subscription you can use: az aks list -o table Copy Once you know the resource group and name of the cluster, you can run the following to add the credentials to your kubekonfig: az aks get-credentials --resource-group &lt;ResourceGroup&gt; --name &lt;Name&gt; Copy "},{"title":"My First Application​","type":1,"pageTitle":"Introduction","url":"docs/xks/developer-guide/introduction#my-first-application","content":"TBD "},{"title":"Next Steps​","type":1,"pageTitle":"Introduction","url":"docs/xks/developer-guide/introduction#next-steps","content":"TBD "},{"title":"Troubleshoot applications in Kubernetes​","type":1,"pageTitle":"Introduction","url":"docs/xks/developer-guide/introduction#troubleshoot-applications-in-kubernetes","content":"There is a great guide how to debug Kubernetes deployment over at learnk8s.io. To debug flux issues have a look at our Flux docs. "},{"title":"Observability","type":0,"sectionRef":"#","url":"docs/xks/developer-guide/observability","content":"","keywords":""},{"title":"What is observability?​","type":1,"pageTitle":"Observability","url":"docs/xks/developer-guide/observability#what-is-observability","content":"Within observability we normally talk about three pillars. metricsloggingtracing Monitoring applications is an especially important feature when developing microservices and something that all developers of microservices needs to focus on. We currently support two solutions to gather observability data in XKF, Datadog and the Opentelemetry stack which is an open-source solution. "},{"title":"Datadog​","type":1,"pageTitle":"Observability","url":"docs/xks/developer-guide/observability#datadog","content":"When Datadog monitoring is used the Datadog Operator will be added to your cluster. On top of deploying a Datadog agent to every node to collect metrics, logs, and traces it also adds the ability to create Datadog monitors from the cluster. The Datadog agent handles all communication with the Datadog API meaning that individual applications do not have to deal with things such as authentication. "},{"title":"Logging​","type":1,"pageTitle":"Observability","url":"docs/xks/developer-guide/observability#logging","content":"All logs written to stdout and stderr from applications in the tenant namespace will be collected by the Datadog agents. This means that no additional configuration has to be done to the application, other than making sure the logs are written to the correct destination. This means that kubectl logs and Datadog will display the same information. Check the official Datadog Logging Documentation for more detailed information. "},{"title":"Metrics​","type":1,"pageTitle":"Observability","url":"docs/xks/developer-guide/observability#metrics","content":"Datadog can collect Prometheus or OpenMetrics metrics exposed by your application. In simple terms this means that the application needs to expose an endpoint which the Datadog agent can scrape to get the metrics. All that is required is that the Pod contains annotations which tells Datadog where to find the metrics HTTP endpoint. Given that your application is exposing metrics on port 8080 your pod should contain the following annotations. annotations: ad.datadoghq.com/prometheus-example.instances: | [ { &quot;prometheus_url&quot;: &quot;http://%%host%%:8080/metrics&quot; } ] Copy Check the official Datadog Metrics Documentation for more detailed information. "},{"title":"Tracing​","type":1,"pageTitle":"Observability","url":"docs/xks/developer-guide/observability#tracing","content":"Datadog tracing is done with Application Performance Monitoring (APM), which sends traces from an application to Datadog. For traces to work the application needs to be configured with the language specific libraries. Check the Language Documentationfor language specific instructions. Some of the languages that are supported are. GolangC#JavaPython Configure your Deployment with the DD_AGENT_HOST environment for the APM agent to know where to send the traces. apiVersion: apps/v1 kind: Deployment spec: containers: - env: - name: DD_AGENT_HOST valueFrom: fieldRef: fieldPath: status.hostIP Copy Check the official Datadog Tracing Documentation for more detailed information. To add tracing to Datadog's Browser Test results, add the URLs that the browser tests visits under UX Monitoring/Synthetic Settings/Integration Settings. See Synthetic APM for more information. You can see a example on how to set this up below. By using a wildcard, multiple endpoints can be traced.  "},{"title":"Networkpolicy datadog​","type":1,"pageTitle":"Observability","url":"docs/xks/developer-guide/observability#networkpolicy-datadog","content":"When using XKF and your cluster has Datadog enabled the tenant namespace will automatically get a networkpolicy that allows egress for tracing and ingress for metrics. You can view these rules by typing: kubectl get networkpolicies -n &lt;tenant-namespace&gt; Copy "},{"title":"Opentelemetry​","type":1,"pageTitle":"Observability","url":"docs/xks/developer-guide/observability#opentelemetry","content":"To gather opentelemetry data we rely on the grafana agent operator. The grafana agent operator deploys a grafana-agent in a central namespace configured as a part of XKF. The grafana agent gathers both metrics and logs and is able to receive traces. "},{"title":"Metrics​","type":1,"pageTitle":"Observability","url":"docs/xks/developer-guide/observability#metrics-1","content":"To gather metrics data we use servicemonitors or podmonitors that is managed in XKF using the prometheus-operator. The prometheus-operator has a great getting started guidebut if you want a quick example you can look below. In order for the grafana agent to find the pod you have to put this exact label on the pod/service monitor yaml: xkf.xenit.io/monitoring: tenant, or else the grafana agent will not find the rule to gather the metric. The selectors is used to find ether the pod or the service that you want to monitor. Use a podmonitor when you do not have a service in front of your pod. For example this might be the case when your application does not use an HTTP endpoint to get requests. apiVersion: monitoring.coreos.com/v1 kind: PodMonitor metadata: name: podmonitor-example labels: xkf.xenit.io/monitoring: tenant spec: selector: matchLabels: app.kubernetes.io/name: app1 podMetricsEndpoints: - port: http-metrics Copy In general use a servicemonitor when you have a service in front of your pod. apiVersion: monitoring.coreos.com/v1 kind: ServiceMonitor metadata: labels: xkf.xenit.io/monitoring: tenant name: servicemonitor-example spec: endpoints: - interval: 60s port: metrics selector: matchLabels: app.kubernetes.io/instance: app1 Copy You can do a lot of configuration when it comes to metrics gathering but the above config will get you started. "},{"title":"Logging​","type":1,"pageTitle":"Observability","url":"docs/xks/developer-guide/observability#logging-1","content":"To gather logs from your application you need to define a PodLogs object. Just like metrics you have to define a label like xkf.xenit.io/monitoring: tenant in your PodLogs. The PodLogs CRD is created by the grafana agent operator and functions very similarly to how the prometheus operator works, especially when it comes to selectors. Below you will find a very basic example that will scrape a single pod in the namespace where it is created. apiVersion: monitoring.grafana.com/v1alpha1 kind: PodLogs metadata: name: app1 labels: xkf.xenit.io/monitoring: tenant spec: selector: matchLabels: app.kubernetes.io/name: app1 pipelineStages: - cri: {} Copy You can do a lot of configuration when it comes to log filtering using PodLogs. For example you can drop specific log types that you do not want to send to your long time storage. Sadly the grafana agent operator does not supply great documentation around how to define this configuration in the operator. However, together with running kubectl explain podlogs.monitoring.grafana.com.spec.pipelineStages on the cluster and reading the official documentation on how to create pipelines you can get a good understanding of how to create the configuration that you need. If you do not have any needs to filter or do any custom config per application you can create a namespace-wide PodLogs gatherer. apiVersion: monitoring.grafana.com/v1alpha1 kind: PodLogs metadata: name: tenant-namespace-log labels: xkf.xenit.io/monitoring: tenant spec: selector: {} pipelineStages: - cri: {} Copy "},{"title":"Tracing​","type":1,"pageTitle":"Observability","url":"docs/xks/developer-guide/observability#tracing-1","content":"The tracing setup is a bit different compared to logging and metrics, instead of having some yaml file where you define how to gather metrics and logs from your application, you instead push data to a central collector. Opentelemetry supports both HTTP and gRPC communication to gather traces from your application. To send HTTP data we use 4318 and for gRPC we use 4317. Point your OpenTelemetry SDK to http://grafana-agent-traces.opentelemetry.svc.cluster.local:4318/v1/tracesor http://grafana-agent-traces.opentelemetry.svc.cluster.local:4317/v1/traces. Tail-based sampling​ By default the grafana agent that is deployed by XKF forwards all traces without any special config to your service provider. This can cause high costs thanks to the amount of data that is sent. You can configure the agent to use probabilistic sampling which grafana agent delivers their own solution for called tail-based sampling, which can help you solve this issue. To setup a custom agent with tail-based sampling you can setup your own trace agent with the custom config that you want and then have it forward all the traffic to our central trace agent in the opentelemetry namespace. Below you can find a simple example configmap that you can use together with your trace agent to send data to the central agent. kind: ConfigMap apiVersion: v1 metadata: name: grafana-agent-traces data: agent.yaml: | tempo: configs: - name: default remote_write: - endpoint: &quot;grafana-agent-traces.grafana-agent.svc.cluster.local:4317&quot; insecure: true receivers: otlp: protocols: http: {} grpc: {} tail_sampling: # policies define the rules by which traces will be sampled. Multiple policies # can be added to the same pipeline. # For more information: https://grafana.com/docs/agent/latest/configuration/traces-config/ # https://github.com/open-telemetry/opentelemetry-collector-contrib/tree/b2327211df976e0a57ef0425493448988772a16b/processor/tailsamplingprocessor policies: - probabilistic: {sampling_percentage: 10} - status_code: {status_codes: [ERROR, UNSET]} Copy "},{"title":"Networkpolicy grafana agent​","type":1,"pageTitle":"Observability","url":"docs/xks/developer-guide/observability#networkpolicy-grafana-agent","content":"When using XKF and your cluster have enabled the grafana agent your tenant namespace will automatically get a networkpolicy that allows incoming metrics gathering and egress for tracing. You can view these rules by typing: kubectl get networkpolicies -n &lt;tenant-namespace&gt; Copy "},{"title":"Reports","type":0,"sectionRef":"#","url":"docs/xks/developer-guide/reports","content":"","keywords":""},{"title":"Security​","type":1,"pageTitle":"Reports","url":"docs/xks/developer-guide/reports#security","content":"Xenit is not responsible for the security of your application but as a part of the report we state the number of CVE:s in your applications found by our automated scanning tools that we have in XKF. This is a extra way for us at Xenit to help you as a developer, but you cannot rely on Xenit to point out specific security flaws in your application. Security is something that you and your company is in charge of, we are just trying to visualize some of the data that we have access to. As a part of XKF we continuously scan all images running in our clusters. To do this we depend onStarboard together with Trivy. If you are using the Xenit CI solution you are by default scanning your container images with Trivy at creation time. However, new CVE:s may have been released since you created your image and that is why continuous scanning of images is important. "},{"title":"Scheduling and Scaling","type":0,"sectionRef":"#","url":"docs/xks/developer-guide/scheduling-scaling","content":"","keywords":""},{"title":"Pod Resources​","type":1,"pageTitle":"Scheduling and Scaling","url":"docs/xks/developer-guide/scheduling-scaling#pod-resources","content":"Setting good resource requests and limits for a container is an important component in helping the scheduler select the correct Node to schedule a Pod to. There are two types of resource configuration fields, and these are configured for each container in a Pod. The resource request tells the scheduler how much of each resource each container in a Pod is expected to consume, though a Pod is allowed to consume more. The limit sets the maximum amount of resources each container in a Pod can consume. There are two main resource types that one should be aware of, CPU and memory. The CPU resource is defined in CPU units where one CPU unit is the equivalent of one CPU core. Fractional units can also be requested, in either decimals like 0.1 but also in terms of millicores where 100m would be the same amount. The memory resource is defined in whole integers with the quantity suffixesEi, Pi, Ti, Gi, Mi, Ki. If you do not specify any resources for a container the default resource request and limit will be applied as shown below. These resources are low on purpose, both to minimize the effects of overprovisioning but also to make it obvious for XKF users that resources have not been specified. The keen-eyed may notice that a CPU limit is not set by default, this is on purpose and the reasons for it will be discussed later. resources: request: cpu: 50m memory: 32Mi limits: memory: 256Mi Copy The scheduler will look at the cumulative resource requests across all containers in a Pod during the scheduling phase. The scheduler will exclude any Node which does not have capacity for the Pods resource request. Capacity is determined based on the total resources available in a Node minus the sum of all the resource requests of all Pods currently scheduled to the Node. A Pod may at times request more resource than any Node has capacity for, there are two possible outcomes for this situation. If the Pods resource request is less than a Nodes total available resources, a new Node will be added to the cluster. The Pod will however be considered unschedulable if the resource request exceeds the total resources available on a single node. In these cases either the resource request has to change or a new Node type has to be added to the cluster to cater to these needs.  It is possible to overprovision Node resources in cases where the resource request for each container is much larger that the actual resource consumption. Efficient resource allocation is a constant battle between requesting enough resources to avoid under allocation while not requesting too much which would result in overallocation. The easiest way to think about resources consumption and availability is to imaging the capacity as a glass, as more resources are consumed water is added to the glass. If the consumption increase does not stop the glass will eventually overfill.  The resource limit defined for a Pod has no affect on the scheduling of a Pod. Limits instead comes into play for a Pod during runtime. Exceeding the resource limit for CPU and memory will have different affects. A Pod which exceeds the memory limit will be terminated with an out of memory error (OOM Error). The Pod will after termination be started again, it may start to exceed the limit again which will result in another OOM error. These types of errors can either be resolved by having the application consume less memory alternatively increasing the memory limit. Without a memory limit a Pod would be able to continue consuming memory until the Node runs out. This would not only affect critical system processes that runs in the node but other Pods which may not even be able to consume the resources it requested. CPU limits should be treated slightly differently from memory limits. When memory is overconsumed applications will crash when there is no place to store new data. However when CPU is overconsumed throttling will occur. Applications will not immediately crash even if performance may be severely deprecated. CPU differs from memory as a resource in the sense that CPU time will be wasted if not consumed and applications required CPU time can vary a lot. Reserving CPU time that is not used is a waste if another application would benefit from using it. Setting a CPU limit for a Pod will result in artificial CPU throttling even if it would not necessarily be required. It is for that reason that a CPU limit is not enforced for Pods by default, instead it is something that should be opted into when the effects of CPU throttling is understood. It is still important to set a reasonable CPU request for Kubernetes to determine the minimal resource requirements, but CPU limits should be avoided in most cases where it is not fully understood. "},{"title":"Vertical Pod Autoscaler​","type":1,"pageTitle":"Scheduling and Scaling","url":"docs/xks/developer-guide/scheduling-scaling#vertical-pod-autoscaler","content":"Defining good resource definitions is one of the hardest things that you can do in Kubernetes. To help out with this there is something called Vertical Pod Autoscaler (VPA). On XKF VPA is running in &quot;Off&quot; mode which only provide resource recommendations instead of change the resource definitions for you. VPA uses cpu and memory usage metrics from 8:days back to recommend the resource settings. The VPA object is automatically on all your deployments in XKF with the help of Fairwinds Goldilocks. To view the VPA recommendations: $ kubectl get vpa NAME MODE CPU MEM PROVIDED AGE goldilocks-debug Off 15m 36253748 True 1d goldilocks-lab Off 11m 12582912 True 21d Copy To get a detailed overview of add -o yaml. $ kubectl get vpa goldilocks-debug -o yaml apiVersion: autoscaling.k8s.io/v1 kind: VerticalPodAutoscaler metadata: labels: creator: Fairwinds source: goldilocks name: goldilocks-debug spec: targetRef: apiVersion: apps/v1 kind: Deployment name: debug updatePolicy: updateMode: &quot;Off&quot; status: conditions: - lastTransitionTime: &quot;2022-07-11T14:14:08Z&quot; status: &quot;True&quot; type: RecommendationProvided recommendation: containerRecommendations: - containerName: debug lowerBound: cpu: 15m memory: &quot;36253326&quot; target: cpu: 15m memory: &quot;36253748&quot; uncappedTarget: cpu: 15m memory: &quot;36253748&quot; upperBound: cpu: 15m memory: &quot;36464648&quot; Copy Overall target is the value that is recommended to follow. VPA isn't perfect and should be seen as a guide in helping you setting resource definitions. "},{"title":"Scheduling on Specific Nodes​","type":1,"pageTitle":"Scheduling and Scaling","url":"docs/xks/developer-guide/scheduling-scaling#scheduling-on-specific-nodes","content":"Node Selectors are the easiest may to make sure that a Pod runs on a specific Node. It is a label selector which filters out which Nodes the Pod can be scheduled to. Any Node which does not match the label selector will not be considered for scheduling. A Pod with a Node Selector which does not match with any Node will never be scheduled. Given the two nodes with different values for the agentpool label. apiVersion: v1 kind: Node metadata: name: aks-standard-node labels: agentpool: standard --- apiVersion: v1 kind: Node metadata: name: aks-memory-node labels: agentpool: memory Copy The Node Selector would make sure that the Pod would only be scheduled on the second Node aks-memory-node and never on the first Node. apiVersion: v1 kind: Pod metadata: name: app spec: nodeSelector: agentpool: memory Copy "},{"title":"Affinity and Anti Affinity​","type":1,"pageTitle":"Scheduling and Scaling","url":"docs/xks/developer-guide/scheduling-scaling#affinity-and-anti-affinity","content":"This is similar to nodeSelector bur greatly enhances the types of constraints you can express. The affinity/anti-affinity language is more expressive. The language offers more matching rules.Rules can be &quot;preferred&quot; rather than hard requirements, so if the scheduler can't satisfy them, the pod will still be scheduledYou can constrain against labels on other pods running on the node (or other topological domain), rather than against labels on the node itself. "},{"title":"Nodes​","type":1,"pageTitle":"Scheduling and Scaling","url":"docs/xks/developer-guide/scheduling-scaling#nodes","content":"There are currently two types of node affinity, called requiredDuringSchedulingIgnoredDuringExecution and preferredDuringSchedulingIgnoredDuringExecution. You can think of them as &quot;hard&quot; and &quot;soft&quot; requirements to schedule a pod. The IgnoredDuringExecution part of the names means that if labels on a node change at runtime such that the affinity rules on a pod are no longer met, the pod continues to run on the node. spec: affinity: nodeAffinity: requiredDuringSchedulingIgnoredDuringExecution: nodeSelectorTerms: - matchExpressions: - key: kubernetes.io/name operator: In values: - ABC - XYZ preferredDuringSchedulingIgnoredDuringExecution: - weight: 100 preference: matchExpressions: - key: label-key operator: In values: - label-value Copy This example only allows pods to be scheduled on nodes with a key kubernetes.io/name with value ABC or XYZ Among the nodes matching this criteria, nodes with the key label-key and the value label-value will be preferred. The weight field is ranged 1-100 and for each node matching all scheduling requirements, the kube-scheduler computes a score, as mentioned earlier. It then adds this number to that sum to calculate the best matching node. "},{"title":"Pods​","type":1,"pageTitle":"Scheduling and Scaling","url":"docs/xks/developer-guide/scheduling-scaling#pods","content":"podAffinity and podAntiAffinity lets you constrain which nodes pods are eligible to be scheduled on based of label of the pods running on the node rather than the labels on the node. podAnitAffinity spec: affinity: podAffinity: requiredDuringSchedulingIgnoredDuringExecution: - labelSelector: matchExpressions: - key: label1 operator: In values: - label-value topologyKey: topology.kubernetes.io/zone podAntiAffinity: preferredDuringSchedulingIgnoredDuringExecution: - weight: 100 podAffinityTerm: labelSelector: matchExpressions: - key: label2 operator: In values: - label-value-anti topologyKey: topology.kubernetes.io/zone Copy This shows an example where we use both affinity rules. Affinity rule: the pod can only be scheduled onto a node if that node is in the same zone as at least one already-running pod that has a label with key label1 and value label-value. antiAffinity rule: the pod should not be scheduled onto a node if that node is in the same zone as a pod with a label having key label2 and value label-value-anti  affinity: podAntiAffinity: preferredDuringSchedulingIgnoredDuringExecution: - podAffinityTerm: labelSelector: matchExpressions: - key: prometheus operator: In values: - xks topologyKey: kubernetes.io/hostname weight: 100 - podAffinityTerm: labelSelector: matchExpressions: - key: prometheus operator: In values: - xks topologyKey: topology.kubernetes.io/zone weight: 100 Copy This is an example configuration of podAntiAffinity for Prometheus. Spreading the pod deployment based on topology.kubernetes.io/zone and topology.kubernetes.io/hostname to only allow 1 pod on each node and to mitigate downtime in case an entire zone goes down, e.g: if a pod runs in zone A with key prometheus and value xks do not schedule it in zone A - choose zone B or C. Note that these settings are &quot;preferred&quot; and not required. We recommend using this configuration, as critical services should be distributed to multiple zones to minimize downtime. You can read more about this in the official documentation. "},{"title":"Pod Disruption Budget​","type":1,"pageTitle":"Scheduling and Scaling","url":"docs/xks/developer-guide/scheduling-scaling#pod-disruption-budget","content":"Pod Disruption Budgets are critical for any production deployment of an application. It enforces so that there are always a set amount of replicas of an application running. There is a risk that an application will during a short period of time have zero replicas running without if a Pod Disruption Budget has not been defined. XKS depends heavily on the existence of Pod Disruption Budgets to make sure that a cluster node pool can be scaled safely and upgrades can be applied to node pools without involving individual developers. During these types of event multiple Nodes will be drained. The Node will block any new Pods from being scheduled to it and start evicting all existing Pods running the Node during a drain. Without a Pod Disruption Budget all Pods belonging to the same Deployment may be stopped at the same time, before any new Pods have had the time to start. With a Pod Disruption Budget a limited amount of Pods will be stopped, and then started on a new Node. Eviction will continue with the remaining Pods after the new Pods are running and passed their readiness probe. This documentation is only relevant for applications that are deployed with multiple replicas. It is not possible to create a Pod Disruption Budget for a single replica application, one has to assume that downtime will most likely happen and an application is deployed as a single replica. Creating a Pod Disruption Budget can be very simple. Assume a Deployment named podinfo with the label app: podinfo and a replica count of three exists in a cluster. The Pod Disruption Budget below would make sure that at least two of the replicas will be running at all times. It is important that minAvailible is always smaller than replicas. It would be impossible to evict a Pod if the two values were equal. Removing a Pod would go against the Pod Disruption Budget, and creating an extra Pod would go against the replica count. This result is that a Node will never be able to evict Pods safely. apiVersion: policy/v1 kind: PodDisruptionBudget metadata: name: podinfo spec: minAvailable: 2 selector: matchLabels: app: podinfo Copy "},{"title":"Horizontal Pod Autoscaler​","type":1,"pageTitle":"Scheduling and Scaling","url":"docs/xks/developer-guide/scheduling-scaling#horizontal-pod-autoscaler","content":"A static replica count may work for a lot of applications but may not be optimal for production workloads. The goal should be to achieve good stability and latency while avoiding overprovisioning. As discussed earlier one way to scale an application is through increasing its resource requests and limits, this type of scaling is known as vertical scaling. Another option is to increase the amount of replicas instead, this type of scaling is known as horizontal scaling. Increasing the replica count will result in more Pods which can share the workload and increase the through put. Changing the replica count manually during the day would just be time consuming and impossible to achieve at scale. The Horizontal Pod Autoscaler can do this automatically. The version autoscaling/v1 of the Horizontal Pod Autoscaler only supports scaling based on CPU utilization. Future versions will support scaling based on more metrics types. apiVersion: autoscaling/v1 kind: HorizontalPodAutoscaler metadata: name: app spec: minReplicas: 1 maxReplicas: 5 targetCPUUtilizationPercentage: 60 scaleTargetRef: apiVersion: apps/v1 kind: Deployment name: app Copy The Pod Disruption Budget discusses the problems related to setting a minimum availability which is the same or higher than the static replica count. The same problem can occur with Horizontal Pod Autoscalers if the value is smaller than the minimum replica value. For more details refer to the Horizontal Pod Autoscaler walkthrough. "},{"title":"Priority Class​","type":1,"pageTitle":"Scheduling and Scaling","url":"docs/xks/developer-guide/scheduling-scaling#priority-class","content":"The Kubernetes scheduler will out of the box treat each Pod with the same priority. The scheduler will assign Pods to Nodes in the order in which they were created, when multiple Pods without a set priority are waiting to be scheduled. This is usually not an issue as Pods tend to be assigned to Nodes quickly. However them scheduling duration may increase if multiple Horizontal Pod Autoscalers were to increase the replica count at the same time, as new Nodes would have to be provisioned first. In this case the queue would grow while waiting for more capacity in the cluster. Some applications may be more critical than others for the survival of a product. Waiting for the applications turn may not be the optimal solution if other applications have no problem waiting a bit longer to start running. Setting Priority Class to a Pod can help the scheduler decide which Pods are more important and should be assigned to a Node first. In XKS there are three Priority Classes available. tenant-lowtenant-mediumtenant-high The Priority Class is set with the priorityClassName field. Always start off with using the Priority Class tenant-low, and decide later on if it should be changed. This is for the simple reason that if every Pod has a high priority no Pod will be considered of high priority as it would be the same as setting no priority at all. Setting tenant-medium could be a good choice for applications that are user facing and may scale up and down frequently. apiVersion: v1 kind: Pod metadata: name: nginx spec: priorityClassName: tenant-low containers: - name: nginx image: nginx Copy "},{"title":"Eviction​","type":1,"pageTitle":"Scheduling and Scaling","url":"docs/xks/developer-guide/scheduling-scaling#eviction","content":"A pod can be evicted due to many reasons. All from priority classes to memory starvation on the nodes, you can find an excellent blog about it at https://sysdig.com/blog/kubernetes-pod-evicted. "},{"title":"Specific workload on node pool types example​","type":1,"pageTitle":"Scheduling and Scaling","url":"docs/xks/developer-guide/scheduling-scaling#specific-workload-on-node-pool-types-example","content":"As mentioned earlier there are a number of ways scheduling workloads on different node pools. To make it a easier to understand lets provide an example. If you want to have a batch job running on a specific node type and only your pod running on that specific node you need to set a node taint, this is done by your Kubernetes Admin. In this example the node taint is key=batch, value=&quot;true&quot;. To schedule your pod on this node you will need to add a toleration to your pod. apiVersion: v1 kind: Pod metadata: name: example spec: containers: - name: busybox image: busybox tty: true imagePullPolicy: IfNotPresent resources: requests: memory: 12Gi limits: memory: 22Gi tolerations: - key: &quot;batch&quot; value: &quot;true&quot; effect: &quot;NoSchedule&quot; Copy If you also want to make sure that this pod only can run on that specific node type you also have to provide a nodeSelector apiVersion: v1 kind: Pod metadata: name: example spec: containers: - name: busybox image: busybox tty: true imagePullPolicy: IfNotPresent resources: requests: memory: 12Gi limits: memory: 22Gi nodeSelector: batch: &quot;true&quot; tolerations: - key: &quot;batch&quot; value: &quot;true&quot; effect: &quot;NoSchedule&quot; Copy If it's okay that your pod run on other nodes as well as the batch node you can use pod affinity. This way your pod will be able to run even if the specific node type isn't available. apiVersion: v1 kind: Pod metadata: name: example spec: containers: - name: busybox image: busybox tty: true imagePullPolicy: IfNotPresent resources: requests: memory: 12Gi limits: memory: 22Gi tolerations: - key: &quot;batch&quot; value: &quot;true&quot; effect: &quot;NoSchedule&quot; affinity: nodeAffinity: preferredDuringSchedulingIgnoredDuringExecution: - weight: 80 preference: matchExpressions: - key: batch operator: In values: - &quot;true&quot; Copy "},{"title":"Working with XKF from Windows","type":0,"sectionRef":"#","url":"docs/xks/developer-guide/wsl2","content":"","keywords":""},{"title":"Installation of WSL2 - Windows Subsystem for Linux​","type":1,"pageTitle":"Working with XKF from Windows","url":"docs/xks/developer-guide/wsl2#installation-of-wsl2---windows-subsystem-for-linux","content":"Install via Powershell Installation is also possible via Microsoft Store. Search for “linux”. Make sure that Windows Subsystem for Linux is enabled as a feature in Windows. In Windows: Go to Control Panel → Programs and Features. In the left-hand menu, select “Turn Windows features on or off”  "},{"title":"Install Docker-Desktop​","type":1,"pageTitle":"Working with XKF from Windows","url":"docs/xks/developer-guide/wsl2#install-docker-desktop","content":"Download and install Docker. Once installation is complete, verify that the application works. We experienced issues when trying to start Docker-desktop within a managed organization using AD accounts, this caused an error with us not being members of a group called “docker-users“. To solve this, open up “Computer Management” in Windows as an administrator. Navigate to “local users and groups” → Groups and locate the &quot;docker-users&quot; group and double-click.Press “Add” and search for “Authenticated Users” and add to the group. Now sign out from Windows and sign back in, and the Docker application should work. Verify in settings that the WSL2-based engine is used.  Also under settings, go to Resources → WSL Integration and verify that you have access to the WSL integration with your installed WSL (in this case Ubuntu), and make sure it is checked.  To verify functionality: In your Ubuntu WSL2 instance - Run the command: docker run hello-world Copy Wait for the image to be pulled and if everything works properly the output should be: Hello from Docker! This message shows that your installation appears to be working correctly. "},{"title":"Utilising Make with WSL2, Terraform and Docker​","type":1,"pageTitle":"Working with XKF from Windows","url":"docs/xks/developer-guide/wsl2#utilising-make-with-wsl2-terraform-and-docker","content":"We have noticed when running Terraform from within our Ubuntu instance, that there appears to be some network issues. We saw quite slow network connections, probably caused by the TCP connection, which resulted in the following error: │ Error: Failed to install provider │ │ Error while installing hashicorp/azurerm v2.64.0: local error: tls: bad record MAC We ran the Terraform command again - and it worked perfectly. "},{"title":"Issues​","type":1,"pageTitle":"Working with XKF from Windows","url":"docs/xks/developer-guide/wsl2#issues","content":""},{"title":"File lock issues​","type":1,"pageTitle":"Working with XKF from Windows","url":"docs/xks/developer-guide/wsl2#file-lock-issues","content":"If your .azure folder is mounted towards Windows from your WSL2 environment we have seen potential lock issues when running terraform and azure-cli 2.32.0, this might apply to other versions as well. We think this have something to do with how WSL2 and Windows manages locking of files, to workaround you can make sure that your .azure folder is only in your Linux environment. Assuming that you have not defined a custom AZURE_CONFIG_DIR you can perform the following to verify you are mounting your .azure folder to Windows: $ cd $ ls -la drwx------ 61 user1 user1 4096 Jan 14 09:21 . drwxr-xr-x 4 root root 4096 Feb 12 2021 .. drwxr-xr-x 7 user1 user1 4096 Dec 12 13:04 .azure -&gt; /mnt/c/Users/user1/.azure Copy Running the following commands will create a new .azure folder in your current working directory and tell azure-cli to use that folder to store its login data. Remember that the export command only is per terminal. You can make the config persistent by adding the export command to your .bashrc file located in your home folder. export AZURE_CONFIG_DIR=$(pwd)/.azure # Store Azure CLI configuration here [ -d $(pwd)/.azure ] || mkdir $(pwd)/.azure # login to azure agaiin az login Copy This is a example how the traceback look liked when we encountered this error: │ Error: getting authenticated object ID: Error parsing json result from the Azure CLI: Error waiting for the Azure CLI: exit status 1: ERROR: The command failed with an unexpected error. Here is the traceback: │ ERROR: [Errno 2] No such file or directory │ Traceback (most recent call last): │ File &quot;/opt/az/lib/python3.6/site-packages/azure/cli/core/commands/command_operation.py&quot;, line 352, in handler │ client = self.client_factory(self.cli_ctx) if self.client_factory else None │ TypeError: get_graph_client_signed_in_users() missing 1 required positional argument: '_' │ │ During handling of the above exception, another exception occurred: │ │ Traceback (most recent call last): │ File &quot;/opt/az/lib/python3.6/site-packages/knack/cli.py&quot;, line 231, in invoke │ cmd_result = self.invocation.execute(args) │ File &quot;/opt/az/lib/python3.6/site-packages/azure/cli/core/commands/__init__.py&quot;, line 657, in execute │ raise ex │ File &quot;/opt/az/lib/python3.6/site-packages/azure/cli/core/commands/__init__.py&quot;, line 720, in _run_jobs_serially │ results.append(self._run_job(expanded_arg, cmd_copy)) │ File &quot;/opt/az/lib/python3.6/site-packages/azure/cli/core/commands/__init__.py&quot;, line 712, in _run_job │ return cmd_copy.exception_handler(ex) │ File &quot;/opt/az/lib/python3.6/site-packages/azure/cli/command_modules/role/commands.py&quot;, line 69, in graph_err_handler │ raise ex │ File &quot;/opt/az/lib/python3.6/site-packages/azure/cli/core/commands/__init__.py&quot;, line 691, in _run_job │ result = cmd_copy(params) │ File &quot;/opt/az/lib/python3.6/site-packages/azure/cli/core/commands/__init__.py&quot;, line 328, in __call__ │ return self.handler(*args, **kwargs) │ File &quot;/opt/az/lib/python3.6/site-packages/azure/cli/core/commands/command_operation.py&quot;, line 354, in handler │ client = self.client_factory(self.cli_ctx, command_args) if self.client_factory else None │ File &quot;/opt/az/lib/python3.6/site-packages/azure/cli/command_modules/role/commands.py&quot;, line 89, in get_graph_client_signed_in_users │ return _graph_client_factory(cli_ctx).signed_in_user │ File &quot;/opt/az/lib/python3.6/site-packages/azure/cli/command_modules/role/_client_factory.py&quot;, line 25, in _graph_client_factory │ resource=cli_ctx.cloud.endpoints.active_directory_graph_resource_id) │ File &quot;/opt/az/lib/python3.6/site-packages/azure/cli/core/_profile.py&quot;, line 335, in get_login_credentials │ credential = self._create_credential(account, client_id=client_id) │ File &quot;/opt/az/lib/python3.6/site-packages/azure/cli/core/_profile.py&quot;, line 588, in _create_credential │ return identity.get_user_credential(username_or_sp_id) │ File &quot;/opt/az/lib/python3.6/site-packages/azure/cli/core/auth/identity.py&quot;, line 182, in get_user_credential │ return UserCredential(self.client_id, username, **self._msal_app_kwargs) │ File &quot;/opt/az/lib/python3.6/site-packages/azure/cli/core/auth/msal_authentication.py&quot;, line 41, in __init__ │ accounts = self.get_accounts(username) │ File &quot;/opt/az/lib/python3.6/site-packages/msal/application.py&quot;, line 872, in get_accounts │ accounts = self._find_msal_accounts(environment=self.authority.instance) │ File &quot;/opt/az/lib/python3.6/site-packages/msal/application.py&quot;, line 912, in _find_msal_accounts │ query={​​&quot;environment&quot;: environment}​​) │ File &quot;/opt/az/lib/python3.6/site-packages/msal_extensions/token_cache.py&quot;, line 53, in find │ with CrossPlatLock(self._lock_location): │ File &quot;/opt/az/lib/python3.6/site-packages/msal_extensions/cache_lock.py&quot;, line 29, in __enter__ │ file_handle = self._lock.__enter__() │ File &quot;/opt/az/lib/python3.6/site-packages/portalocker/utils.py&quot;, line 199, in __enter__ │ return self.acquire() │ File &quot;/opt/az/lib/python3.6/site-packages/portalocker/utils.py&quot;, line 161, in acquire │ fh = self._prepare_fh(fh) │ File &quot;/opt/az/lib/python3.6/site-packages/portalocker/utils.py&quot;, line 194, in _prepare_fh │ fh.truncate(0) │ FileNotFoundError: [Errno 2] No such file or directory │ To open an issue, please run: 'az feedback' │ │ with provider[&quot;registry.terraform.io/hashicorp/azuread&quot;], │ on main.tf line 19, in provider &quot;azuread&quot;: │ 19: provider &quot;azuread&quot; {​​}​​ Copy "},{"title":"Overview","type":0,"sectionRef":"#","url":"docs/xks/operator-guide/","content":"","keywords":""},{"title":"Architecture​","type":1,"pageTitle":"Overview","url":"docs/xks/operator-guide/#architecture","content":"XKS is set up from a set of Terraform modules that when combined creates the full XKS service. There are multiple individual states that all fulfill their own purpose and build upon each other in a hierarchical manner. The first setup requires applying the Terraform in the correct order, but after that ordering should not matter. Separate states are used as it allows for a more flexible architecture that could be changed in parallel.  "},{"title":"Network diagram​","type":1,"pageTitle":"Overview","url":"docs/xks/operator-guide/#network-diagram","content":"Looking at a cluster, the simple network diagram looks like this:  "},{"title":"Terraform modules​","type":1,"pageTitle":"Overview","url":"docs/xks/operator-guide/#terraform-modules","content":"The following Terraform modules are used in XKS. "},{"title":"Governance​","type":1,"pageTitle":"Overview","url":"docs/xks/operator-guide/#governance","content":"Governance is split into global and regional, it handles the creation and delegation of Azure Resource Groups, Azure KeyVaults, Azure AD groups, Service Principals and resources like that. "},{"title":"Core​","type":1,"pageTitle":"Overview","url":"docs/xks/operator-guide/#core","content":"Core sets up the main network for an environment. "},{"title":"Hub​","type":1,"pageTitle":"Overview","url":"docs/xks/operator-guide/#hub","content":"Hub is setup in the production subscription and is used for things like Azure Pipelines agents. "},{"title":"AKS​","type":1,"pageTitle":"Overview","url":"docs/xks/operator-guide/#aks","content":"The AKS Terraform contains three modules that are used to setup a Kubernetes cluster. To allow for blue/green deployments of AKS clusters resources have to be split up into global resources that can be shared between the clusters, and cluster specific resources. The aks-global module contains the global resources like ACR, DNS and Azure AD configuration. The aks and aks-core modules create a AKS cluster and configures it. This cluster will have a suffix, normally a number to allow for temporarily creating multiple clusters when performing a blue/green deployment of the clusters. Namespaces will be created in the cluster for each of the configured tenants. Each namespaces is linked to a resource group in Azure where namespace resources are expected to be created.  "},{"title":"AWS azure-devops","type":0,"sectionRef":"#","url":"docs/xks/operator-guide/aws-azdo","content":"","keywords":""},{"title":"Terraform​","type":1,"pageTitle":"AWS azure-devops","url":"docs/xks/operator-guide/aws-azdo#terraform","content":"All this is possible to solve using Terraform but we currently do not have any terraform module to handle it so for now you can read about the manual steps. Create an IAM user Tip: put both these users under a specific path, for example CISave the access key in a safe place Attach IAM policy AmazonEKSClusterPolicyAdministratorAccess "},{"title":"GitOps user​","type":1,"pageTitle":"AWS azure-devops","url":"docs/xks/operator-guide/aws-azdo#gitops-user","content":"In our GitOps pipelines using Flux we need to be able to push and delete images in ECR. So very similar to how we created the Terraform user: Create an IAM user Tip: put both these users under a specific path, for example CISave the credentials in a safe place Attach IAM policy AmazonEC2ContainerRegistryFullAccess "},{"title":"Networking","type":0,"sectionRef":"#","url":"docs/xks/developer-guide/networking","content":"","keywords":""},{"title":"Network Policies​","type":1,"pageTitle":"Networking","url":"docs/xks/developer-guide/networking#network-policies","content":"Network Policies in Kubernetes add the ability to allow and deny network traffic from specific pods and namespaces. Both egress traffic from a Pod and ingress traffic to a Pod can be controlled. In a vanilla Kubernetes cluster all traffic between all namespaces is allowed by default. This is not the case in XKS. Out of the box in XKS all tenant namespaces have a default deny rule added to them. This default deny rule will block any traffic going between namespaces. It will deny both ingress traffic from other namespaces and egress traffic to other namespaces. All traffic within the namespace between Pods is allowed. The reasoning behind this setup is that Pods should not have more network access than they require to function, as it reduces the blast radius in case of an exploit.  The default deny Network Policy contains an exception for traffic destined to the cluster's DNS service. Without this exception DNS resolution would not work. The Pod selector in the Network Policy is empty, this means that the Network Policy will apply for all Pods in the namespace. apiVersion: networking.k8s.io/v1 kind: NetworkPolicy metadata: name: default-deny namespace: tenant spec: egress: - ports: - port: 53 protocol: UDP to: - namespaceSelector: {} podSelector: matchLabels: k8s-app: kube-dns - to: - podSelector: {} ingress: - from: - podSelector: {} podSelector: {} policyTypes: - Ingress - Egress Copy There may come a time when you have to create new Network Policies to allow specific Pods traffic, as the default can be pretty restrictive. A common situation when this is required is when a Pod needs to communicate with the public Internet, or communicate with other Pods in other tenant namespaces. When creating new Network Policies make sure that you do not open up more than is actually required. A good source of example Network Policies is the Github repository kubernetes-network-policy-recipes. It contains a lot of good examples with diagrams and descriptions. The examples found there contain the most common use cases to make things simpler for you. A helpful tool when create new Network Policies is the Cilium Network Policy Editor. "},{"title":"Examples​","type":1,"pageTitle":"Networking","url":"docs/xks/developer-guide/networking#examples","content":"Allow Internet Egress​ A common scenario is opening up traffic to the public Internet. A current limitation with Network Policies today is that it is not possible to create egress rules based on DNS names. This means that the simplest solution is to allow traffic to all public IPs, as trying to resolve the DNS to an IP would only work short term. apiVersion: networking.k8s.io/v1 kind: NetworkPolicy metadata: name: allow-internet-egress spec: egress: - to: - ipBlock: cidr: 0.0.0.0/0 podSelector: matchLabels: app: foo policyTypes: - Egress Copy Allow Ingress Nginx​ Traffic from the ingress controller has to be explicitly allowed as no traffic is allowed from outside the namespace by default. This can be considered a fail-safe to protect against accidental Ingress creation, where an application is exposed to the Internet when that was not the intent. It is enough to allow ingress from the ingress controller even if the traffic actually originates from outside the cluster. apiVersion: networking.k8s.io/v1 kind: NetworkPolicy metadata: name: allow-ingress-controller spec: ingress: - from: - namespaceSelector: matchLabels: name: ingress-nginx podSelector: matchLabels: app: foo policyTypes: - Ingress Copy Allow Cross Namespace​ When allowing network traffic across tenant namespaces considerations have to be made for the default deny Network Policy in both namespaces. An allow rule has to be created to allow the source namespace (the side initating the connection) to send traffic to the destination namespace. The destination namespace has to allow traffic from the source namespace. The first Network Policy should be used in the source namespace and the second should be used in the destination namespace. apiVersion: networking.k8s.io/v1 kind: NetworkPolicy metadata: name: allow-egress-to-destination namespace: source spec: egress: - to: - namespaceSelector: matchLabels: name: destination podSelector: matchLabels: app: bar podSelector: matchLabels: app: foo policyTypes: - Egress Copy apiVersion: networking.k8s.io/v1 kind: NetworkPolicy metadata: name: allow-ingress-from-source namespace: destination spec: ingress: - from: - namespaceSelector: matchLabels: name: source podSelector: matchLabels: app: foo podSelector: matchLabels: app: bar policyTypes: - Ingress Copy "},{"title":"Debugging​","type":1,"pageTitle":"Networking","url":"docs/xks/developer-guide/networking#debugging","content":"TBD "},{"title":"Ingress​","type":1,"pageTitle":"Networking","url":"docs/xks/developer-guide/networking#ingress","content":"Ingress in Kubernetes is used to allow network traffic from the outside the cluster to reach Pods inside the cluster. Ingress works as a layer on top of Kubernetes Services by exposing the Service with a hostname. All Ingress traffic is Layer 7 routed, meaning that traffic is routed based on the host header in the HTTP request. This also means that Ingress only works with HTTP traffic. Doing it this way means that only a single load balancer is required reducing cost compared to running multiple load balancers, one per Ingress.  XKS comes with everything pre-configured for Ingress to work. The cluster will either have a single Nginx Ingress Controller which is exposed to the public Internet or two controllers where one is public and one is private. On top of that the cluster is configured with External DNS(which creates DNS records) and Cert Manager (which deals with certificate creation and renewal). Together these three tools offer an automated solution where the complexity of DNS and certificates are not handled by the application. The recommendation is to always enable TLS for all Ingress resources no matter how small the service is. Updating a certificate is quick and easy so there is no reason not to do this. Every XKS cluster comes with a preconfigured Cluster Issuer which will provision certificates from Let's Encrypt. Start off by creating a Certificate resource for your Ingress. It is possible to have Cert Manager automatically create a Certificate when an Ingress resource is created. This however has the downside that every Ingress resource will receive its own Certificate. Lets Encrypt has rate limits for the same domain, if one were to create a Certificate per ingress that rate limit would be hit pretty quickly. For this reason it is better to create a shared Certificate per tenant namespace with multiple DNS names instead. Each DNS name will be present in the Certificate so that it can be used for multiple Ingress resources. When the Certificate is provisioned it will be written to a Secret. apiVersion: cert-manager.io/v1 kind: Certificate metadata: name: shared namespace: tenant spec: issuerRef: group: cert-manager.io kind: ClusterIssuer name: letsencrypt dnsNames: - app-one.example.com - app-two.example.com secretName: shared-cert Copy To complete the ingress configuration an Ingress resource has to be created. The Ingress resource defines the Service where the traffic should be routed to and the DNS name which should resolve to that Service. An additional configuration is the TLS configuration which configures the certificate to use. Cert Manager writes the certificate data to a Secret which is configured in the CertificatesecretName. That same Secret should be referenced in the TLS configuration. A DNS record will be automatically created when the Ingress is applied to the cluster. apiVersion: networking.k8s.io/v1 kind: Ingress metadata: name: app-one namespace: tenant spec: rules: - host: app-one.example.com http: paths: - path: / backend: service: name: app-one port: name: http tls: - hosts: - app-one.example.com secretName: shared-cert Copy "},{"title":"Public and Private Ingress​","type":1,"pageTitle":"Networking","url":"docs/xks/developer-guide/networking#public-and-private-ingress","content":"By default an XKS cluster will deploy a single public Ingress controller. All Ingress resources will be routed with a public IP and therefore exposed to the public Internet. It is however also possible to create private Ingress resources which are only exposed through an IP that is private to the virtual network in which the Kubernetes cluster is deployed. This is an opt in feature as two load balancing services are needed. Making an Ingress private is simple when the private Ingress feature is enabled. All that is required is that the Ingress class has to be set to nginx-private, this makes sure that the resource is only served through the private IP. apiVersion: networking.k8s.io/v1 kind: Ingress metadata: name: app-one namespace: tenant spec: ingressClassName: nginx-private rules: - host: app-one.example.com http: paths: - path: / backend: service: name: app-one port: name: http tls: - hosts: - app-one.example.com secretName: shared-cert Copy "},{"title":"External Routing​","type":1,"pageTitle":"Networking","url":"docs/xks/developer-guide/networking#external-routing","content":"There is no requirement that the destination for an Ingress resource has to be served from within the cluster. It is possible to route Ingress traffic either to endpoints outside of the cloud provider or to another service that is only accessible from within the private network. Using the XKS Ingress instead of a separate solution has it's benefits in these situations, as DNS record creation and certificate management is already setup to work. A typical use case may be during a migration period when XKS and another solution may exist in parallel. All traffic can enter through XKS but then be forwarded to the external destination. The service endpoints can be updated as applications are migrated to run inside XKS instead of outside. A Service resource is required to configure the destination of the traffic. There are two options available in Kubernetes when directing traffic outside of the cluster. The first option is to create a Service of type ExternalName which specifies a host name which the Service should write to. When a request is made to the Service the given external name IP will be resolved and the request will be sent to that destination. apiVersion: v1 kind: Service metadata: name: to-external spec: type: ExternalName externalName: example.com Copy The other option is to create a Service which routes to a static IP. This is implemented with a Serivce without an external name or label selector, then also creating an Endpoint for the Service. This way the Service will only resolve to the static IP given, in this case the static IP is 1.2.3.4 and the port is 443. apiVersion: v1 kind: Service metadata: name: to-external spec: ports: - protocol: TCP port: 443 --- apiVersion: v1 kind: Endpoints metadata: name: to-external subsets: - addresses: - ip: 1.2.3.4 ports: - port: 443 Copy The Serivce resources only solve half the problem as they are only accessible within the cluster. They have to be exposed with an Ingress resource to solve the other half, so that on host name can be translated to another. The example assume that all traffic is expected to be HTTPS on both ends. The Ingress below exposes the Service named to-external on the port 443 with the host name forward.xenit.io. It also assumes that a TLS Secret exists which is valid for the Ingress host name. apiVersion: networking.k8s.io/v1 kind: Ingress metadata: name: forward-traffic annotations: nginx.ingress.kubernetes.io/backend-protocol: &quot;HTTPS&quot; nginx.ingress.kubernetes.io/upstream-vhost: &quot;forward.xenit.io&quot; spec: rules: - host: forward.xenit.io http: paths: - pathType: Prefix path: / backend: service: name: forward-traffic port: number: 443 tls: - hosts: - forward.xenit.io secretName: to-external Copy The only major changes with the Ingress compared to a &quot;normal&quot; Ingress resource are the annotations. The annotations nginx.ingress.kubernetes.io/backend-protocol: &quot;HTTPS&quot; makes sure that the traffic on the backend is sent as HTTPS traffic. Without this annotation there is a risk that the backend traffic could be transported as HTTP. The second annotationsnginx.ingress.kubernetes.io/upstream-vhost: &quot;forward.xenit.io&quot; specifies the host header set in the upstream request. This annotation is not necessarily required for all external endpoints, but a lot of endpoints resolve their routing through layer 7 which means that the host header has to be set properly. A good practice is to set the annotation value to be the same as the external name. Another use case is to rewrite the request paths. This is possible through the nginx.ingress.kubernetes.io/rewrite-target which can allow for complex path modification logic. For details how to use the annotation refer to the documentation. "},{"title":"Rate Limiting​","type":1,"pageTitle":"Networking","url":"docs/xks/developer-guide/networking#rate-limiting","content":"At times it is beneficial to rate limit the amount of requests that reach an internal application from the Internet. Rate limiting can be configured to act based on different parameters. Rate limiting is configured for an Ingress through the use of annotations. The annotations nginx.ingress.kubernetes.io/limit-connections limits the amount of concurrent connections allowed from a single source IP. The other annotation nginx.ingress.kubernetes.io/limit-rps sets a limit for the amount of requests per second that can be sent from a single source IP. These two strategies do not work together, you have to decide on one or the other. Below is an example of a Ingress which limits the amount of concurrent connections. apiVersion: networking.k8s.io/v1 kind: Ingress metadata: name: example annotations: nginx.ingress.kubernetes.io/limit-connections: 10 spec: rules: - host: example.com http: paths: - path: / backend: service: name: example port: name: http tls: - hosts: - example.com secretName: cert Copy For more information refer to the official documentation. "},{"title":"Nginx Configuration​","type":1,"pageTitle":"Networking","url":"docs/xks/developer-guide/networking#nginx-configuration","content":"It is useful to be aware of annotation configuration in the Nginx ingress controller. Sometimes a specific Ingress requires custom behavior that is not default in the ingress controller, this behavior can be customized with the help of annotations for a specific Ingress resource. For example, changing the client body buffer size may be useful if the header size in the expected requests is larger than the buffer. apiVersion: extensions/v1beta1 kind: Ingress metadata: name: foo namespace: bar annotations: nginx.ingress.kubernetes.io/client-body-buffer-size: 1M spec: rules: - host: foo.dev.example.com http: paths: - backend: serviceName: foo servicePort: http tls: - hosts: - foo.dev.example.com Copy "},{"title":"Debugging​","type":1,"pageTitle":"Networking","url":"docs/xks/developer-guide/networking#debugging-1","content":"Common networking problems include forgetting to set up egress or ingress rules that apply for your pods - or setting them up and then having the requirements change, which then causes connection errors. Remember that you can inspect your network policies with kubectl get networkpolicies. If you cannot see your policy there, verify if it is actually present in your kustomization.yaml file. "},{"title":"Linkerd​","type":1,"pageTitle":"Networking","url":"docs/xks/developer-guide/networking#linkerd","content":"Linkerd is an optional service mesh that can be added to XKS. The component is opt-in as it adds a certain amount of overhead, so unless it has been requested Linkerd will not be present in XKS. A service mesh extends the networking functionality in a Kubernetes cluster. It is useful when features such as end-to-end encryption or GRPC load balancing is required. Linkerd will automatically handle TCP loadbalancing so when GRPC is used Linkerd will detect this and loadbalance between instances of GRPC servers. Refer to the official documentation for documentation that may be missing from this page. Linkerd works by injecting a sidecar into every Pod which uses Linkerd. All network requests have to be sent through the sidecar which will then be responsible for forwarding it. The sidecar will handle things like traffic encryption before sending the packets outside of the node.  "},{"title":"Get Started​","type":1,"pageTitle":"Networking","url":"docs/xks/developer-guide/networking#get-started","content":"To enable sidecar injection the Pod has to have the annotation linkerd.io/inject: enabled added to it. A common mistake when enabling Linkerd is that the annotation is added to Deployment and not the Pod template, make sure that you do not do this as the sidecar will not be injected if you do. apiVersion: apps/v1 kind: Deployment metadata: name: linkerd-test spec: replicas: 1 selector: matchLabels: app: linkerd-test template: metadata: annotations: linkerd.io/inject: enabled labels: app: linkerd-test spec: containers: - name: linkerd-test image: alpine:latest ports: - containerPort: 8080 name: http protocol: TCP Copy Eventually a Pod should be created. An important detail is that there should be two containers in the Pod. One container should be the one defined in the Deployment and the other one the Linkerd sidecar. This can be verified by getting the Pod's containers: $ kubectl get pods &lt;POD_NAME&gt; -o jsonpath=&quot;{.spec.containers[*].name}&quot; linkerd-test linkerd-proxy Copy With the sidecar added all traffic going out of the container will automatically be proxied through the sidecar. "},{"title":"FAQ​","type":1,"pageTitle":"Networking","url":"docs/xks/developer-guide/networking#faq","content":"Is all network traffic encrypted?​ No, it depends on the traffic type and is something that should be verified rather than assumed. More information can be found in the Linkerd documentation. What overhead can I expect?​ Each Pod will at a minimum consume an additional 10 MB due to the extra sidecar, and the number can grow as traffic increases. "},{"title":"Blast Radius","type":0,"sectionRef":"#","url":"docs/xks/operator-guide/blast-radius","content":"","keywords":""},{"title":"Workflow​","type":1,"pageTitle":"Blast Radius","url":"docs/xks/operator-guide/blast-radius#workflow","content":"Our normal way of working uses a Makefile to make managing commands easier. When running Terraform locally and in our CI pipelines we use our Markdown files. "},{"title":"OPA Blast Radius calculation​","type":1,"pageTitle":"Blast Radius","url":"docs/xks/operator-guide/blast-radius#opa-blast-radius-calculation","content":"The calculation of the blast radius value is done in the Dockerfile that is started through the Makefile. We use OPA to calculate the blast radius itself. To see exactly how we do it you can look in this bash script The default value is currently set to 50. "},{"title":"Overwrite OPA blast radius locally​","type":1,"pageTitle":"Blast Radius","url":"docs/xks/operator-guide/blast-radius#overwrite-opa-blast-radius-locally","content":"Run your make command normally, but when running plan add OPA_BLAST_RADIUS and the value you want. make plan ENV=dev DIR=governance OPA_BLAST_RADIUS=51 Copy "},{"title":"Overwrite OPA blast radius in CI​","type":1,"pageTitle":"Blast Radius","url":"docs/xks/operator-guide/blast-radius#overwrite-opa-blast-radius-in-ci","content":"We are using Just Enough Administration (JEA) at Xenit and in many cases our admins do not have enough access to run Terraform plan/apply locally. Instead we are forced to use our CI/CD systems to manage this for us. If you look at the Makefile you will see that we do not use any environment variables to overwrite the OPA_BLAST_RADIUS value. So how should we change the OPA_BLAST_RADIUS without having to update the pipeline file every time we want to overwrite the default value? Sadly here comes some magic, if you look in https://github.com/XenitAB/azure-devops-templates/you will see that we are listening for the environment variable opaBlastRadius. So to overwrite the OPA_BLAST_RADIUS value during a single run we can utilize the opaBlastRadius environment variable. To add a custom value to your pipeline in Azure DevOps do the following: Pipelines -&gt; &quot;pipeline-you-want-to-run&quot; -&gt; Run pipeline -&gt; Variables -&gt; Add variable Add opaBlastRadius=51, it should look something like this:  To start the job you have to push &quot;&lt;-&quot; and Run. Remember, this variable is only set for one run. "},{"title":"Secrets Management","type":0,"sectionRef":"#","url":"docs/xks/developer-guide/secrets-management","content":"","keywords":""},{"title":"Auto updating secrets​","type":1,"pageTitle":"Secrets Management","url":"docs/xks/developer-guide/secrets-management#auto-updating-secrets","content":"The CSI drivers sits and pools the cloud provider for changes in the secrets defined in your secretProviderClass. Depending on which cloud provider they have different default values. But the CSI driver only pools the secrets if you have a pod that is actively using the secret defined in the cloud. This can cause issues if you are using CSI driver in a cronjob and that secret don't have any other long running pods that mounts it. For example a cronjob that runs for under 2 minutes is not guaranteed to get the latest updated secret from the cloud provider. And even if you have a longer running cronjob that would get a update you would then have to restart that job to make sure that you got the latest secret if someone have updated the secret. To workaround this issue you have to create a pod that uses the same secret to make sure that the secret is up to date. All this long running pod needs to do is to mount the secret and sleep. Below you can find a suggestion on how to that: apiVersion: apps/v1 kind: Deployment metadata: name: foo spec: selector: matchLabels: app: test template: metadata: labels: app: test spec: containers: - name: busybox image: busybox:latest command: [&quot;/bin/sh&quot;, &quot;-c&quot;, &quot;--&quot;] args: [&quot;while true; do sleep 30; done;&quot;] tty: true volumeMounts: - name: secret-store mountPath: &quot;/mnt/secrets-store&quot; readOnly: true env: - name: BAR valueFrom: secretKeyRef: name: bar key: bar volumes: - name: secret-store csi: driver: secrets-store.csi.k8s.io readOnly: true volumeAttributes: secretProviderClass: foo Copy "},{"title":"Cloud Providers​","type":1,"pageTitle":"Secrets Management","url":"docs/xks/developer-guide/secrets-management#cloud-providers","content":""},{"title":"Azure​","type":1,"pageTitle":"Secrets Management","url":"docs/xks/developer-guide/secrets-management#azure","content":"The Azure provider for the CSI driver requires some configuration to work, as it is possible to have multiple Azure Key Vaults. For that reason the Secret Provider Class has to specify the name of the Key Vault and the tenant id where the CSI Driver can find the secret. Additionally it is important to set usePodIdentity: &quot;true&quot; as authentication to the Azure API will be done with the help of AAD Pod Identity. apiVersion: secrets-store.csi.x-k8s.io/v1 kind: SecretProviderClass metadata: name: connection-string-test spec: provider: azure parameters: usePodIdentity: &quot;true&quot; keyvaultName: &quot;kvname&quot; objects: | array: - | objectName: connectionstring objectType: secret tenantId: &quot;11111111-1111-1111-1111-111111111111&quot; Copy To use the Secret Provider Class simply mount it as a volume in the Pod where you want to read the secret. The only extra configuration that is required is setting the label aadpodidbinding to the name of the Azure Identity. This is required as the CSI Driver will assume the Pods identity when authenticating with the Azure API. Without this label the fetching of the secret will fail. apiVersion: apps/v1 kind: Deployment metadata: name: connection-string-test namespace: tenant spec: selector: matchLabels: app: connection-string-test template: metadata: labels: app: connection-string-test aadpodidbinding: tenant spec: containers: - name: connection-string-test image: alpine:latest volumeMounts: - name: secret-store mountPath: &quot;/mnt/secrets-store&quot; readOnly: true volumes: - name: secret-store csi: driver: secrets-store.csi.k8s.io readOnly: true volumeAttributes: secretProviderClass: connection-string-test Copy "},{"title":"AWS​","type":1,"pageTitle":"Secrets Management","url":"docs/xks/developer-guide/secrets-management#aws","content":"There are two secret store services in AWS that is supported by the CSI Driver, AWS Secret Manager and AWS System Manager Parameter Store. Both services have their own pros and cons in regards to features and pricing, but in the end both services deliver the same features in the cluster. The example below shows how to read the secret application/connection-string-test/connectionstring with examples for both Secret Manager and System Manager Parameter Store. Create an IAM role which gives permission to read the specific secret, note that the full ARN path including the secret name is included in the resource field. This is to limit secret acccess for the application as there is only a single service instance per account and region. The CSI Driver also requires the secretsmanager:ListSecrets permission or ssm:DescribeParameters to function properly. It will not be able to read any secret values with this permission, just list them. data &quot;aws_iam_policy_document&quot; &quot;db_connection_string&quot; { statement { effect = &quot;Allow&quot; actions = [ &quot;secretsmanager:ListSecrets&quot;, ] resources = [&quot;*&quot;] } statement { effect = &quot;Allow&quot; actions = [ &quot;secretsmanager:GetSecretValue&quot;, &quot;secretsmanager:DescribeSecret&quot;, &quot;secretsmanager:GetResourcePolicy&quot;, &quot;secretsmanager:ListSecretVersionIds&quot; ] resources = [&quot;arn:aws:secretsmanager:${data.aws_region.current.name}:${data.aws_caller_identity.current.account_id}:secret:application/connection-string-test/connectionstring&quot;] } } Copy or data &quot;aws_iam_policy_document&quot; &quot;db_connection_string&quot; { statement { effect = &quot;Allow&quot; actions = [ &quot;ssm:DescribeParameters&quot;, ] resources = [&quot;*&quot;] } statement { effect = &quot;Allow&quot; actions = [ &quot;ssm:GetParameter&quot;, &quot;ssm:GetParameters&quot;, ] resources = [&quot;arn:aws:ssm:${data.aws_region.current.name}:${data.aws_caller_identity.current.account_id}:parameter/db-*&quot;] } } Copy Complete the configuration by passing the policy document to the IRSA module which will create the IAM policy and role, this should be the same for both Secret Manager and System Manager Parameter Store. module &quot;irsa_test&quot; { source = &quot;github.com/xenitab/terraform-modules//modules/aws/irsa?ref=2021.08.9&quot; name = &quot;irsa-test&quot; oidc_providers = [ for v in var.oidc_urls : { url = v arn = aws_iam_openid_connect_provider.this[v].arn } ] kubernetes_namespace = &quot;tenant&quot; kubernetes_service_account = &quot;connection-string-test&quot; policy_json = data.aws_iam_policy_document.get_login_profile.json } Copy After the IAM role and policy have been created a Secret Provider Class has to be created specifying the secrets that should be read. Make sure to specify the correct object type, it should either besecretsmanager or ssmparameter. Note the configuration of objectAlias for the object. This is required as the secret name contains the character / in the name. By default the CSI Driver uses the name as the file name, which would cause issues as this is not permitted in Linux. The solution is to give the secret an alias instead. apiVersion: secrets-store.csi.x-k8s.io/v1 kind: SecretProviderClass metadata: name: connection-string-test namespace: tenant spec: provider: aws parameters: objects: | - objectName: &quot;application/connection-string-test/connectionstring&quot; objectType: &quot;secretsmanager&quot; | &quot;ssmparameter&quot; objectAlias: &quot;connectionstring&quot; secretObjects: - data: - key: password objectName: &quot;connectionstring&quot; secretName: connectionstring type: Opaque Copy Create a deployment which mounts the secret from the remote service. The secret is mounted as a volume in the Pod and will be populated with the value stored in the remote service. It is important that the Service Account is configured properly as the CSI Driver will assume the Pod's role when fetching the secret value. apiVersion: v1 kind: ServiceAccount metadata: name: connection-string-test namespace: tenant annotations: eks.amazonaws.com/role-arn: arn:aws:iam::111111111111:role/connection-string-test --- apiVersion: apps/v1 kind: Deployment metadata: name: connection-string-test namespace: tenant spec: selector: matchLabels: app: connection-string-test template: metadata: labels: app: connection-string-test spec: serviceAccountName: connection-string-test containers: - name: connection-string-test image: alpine:latest volumeMounts: - name: secret-store mountPath: &quot;/mnt/secrets-store&quot; readOnly: true volumes: - name: secret-store csi: driver: secrets-store.csi.k8s.io readOnly: true volumeAttributes: secretProviderClass: connection-string-test Copy "},{"title":"Automatic Reloading​","type":1,"pageTitle":"Secrets Management","url":"docs/xks/developer-guide/secrets-management#automatic-reloading","content":"A Pod will get the latest version of the Secret Provider Class when started. The CSI Driver will poll the secret and update when the secret value is updated. However the Pod will not be updated as this would require the application to be able to restart the process and read the file instead. The Pod will not receive the new value until a new instance of the Pod is created. This could become annoying for situations where the secret value may change often or there are a lot of secrets being read. The solution in XKS is to configure the Secret Provider Class to annotate the Pod to be recreated when the Secret value is updated. The Pod recreation is done with theReloader project which is present in all XKS clusters. Reloader works by adding an annotation with the key secret.reloader.stakater.com/reload, where the value is the name of the secret. If you need to recreate your Pod when any of multiple secrets are changed, use comma-separated values: secret.reloader.stakater.com/reload: &quot;foo,bar&quot; Copy When using an object alias the object name in the secrets objects refers to the alias and not to the original object name. Below is an example of creating a Service Provider Class which also creates a Kubernetes Secret, there is no need to actually use the created secret but in the example below it is mounted as an environment variable. apiVersion: secrets-store.csi.x-k8s.io/v1 kind: SecretProviderClass metadata: name: application namespace: tenant spec: provider: &lt;provider&gt; parameters: objects: | - objectName: &quot;foo&quot; objectType: &quot;&lt;type&gt;&quot; secretObjects: - data: - key: bar objectName: foo secretName: foo type: Opaque --- apiVersion: apps/v1 kind: Deployment metadata: name: application namespace: tenant spec: selector: matchLabels: app: application template: metadata: annotations: secret.reloader.stakater.com/reload: &quot;foo&quot; labels: app: application spec: serviceAccountName: application containers: - name: application image: alpine:latest env: - name: BAR valueFrom: secretKeyRef: name: foo key: bar volumeMounts: - name: secret-store mountPath: &quot;/mnt/secrets-store&quot; readOnly: true volumes: - name: secret-store csi: driver: secrets-store.csi.k8s.io readOnly: true volumeAttributes: secretProviderClass: foo Copy "},{"title":"Troubleshooting​","type":1,"pageTitle":"Secrets Management","url":"docs/xks/developer-guide/secrets-management#troubleshooting","content":"There are a lot of things that can go wrong when configuring Secrets. Here are some pointers for things to check: "},{"title":"Did you forget to declare the SecretProviderClass?​","type":1,"pageTitle":"Secrets Management","url":"docs/xks/developer-guide/secrets-management#did-you-forget-to-declare-the-secretproviderclass","content":"Remember, getting access to your secrets consists of 2 separate parts: a SecretProviderClass, which tells Kubernetes where it can get the stored secreta mount of the provided secrets-store for your deployment "},{"title":"Verifying your loaded YAML​","type":1,"pageTitle":"Secrets Management","url":"docs/xks/developer-guide/secrets-management#verifying-your-loaded-yaml","content":"By running kubectl get secretproviderclasspodstatuses -o yaml you can get a lot of information about if and how your secrets got correctly loaded. Check here first! For example, look out to see that all the secrets you expect to see are available, and that they are mounted: status: mounted: true Copy "},{"title":"Is your key vault correctly configured, and does the pod have access to it?​","type":1,"pageTitle":"Secrets Management","url":"docs/xks/developer-guide/secrets-management#is-your-key-vault-correctly-configured-and-does-the-pod-have-access-to-it","content":"Speaking from experience, it is all too easy to setup access to the wrong key vault. If you are accessing the right key vault and you are using Azure, double check thatusePodIdentity: &quot;true&quot; is set on the SecretProviderClass. You also need to make sure that the metadata of your deployment's template section contains a declaration of which aadpodidbinding to use (which is always your tenant's name): apiVersion: apps/v1 kind: Deployment metadata: name: connection-string-test namespace: tenant spec: selector: matchLabels: app: connection-string-test template: metadata: labels: app: connection-string-test aadpodidbinding: tenant Copy Please also verify that aadpodidbinding is set on the metadata section under template and not on the root metadata section. The latter will not work. "},{"title":"Are your environment variables correctly set?​","type":1,"pageTitle":"Secrets Management","url":"docs/xks/developer-guide/secrets-management#are-your-environment-variables-correctly-set","content":"If you want to load your secret as an environment variable, remember that it still needs to be mounted as a volume. Also, don't forget that it doesn't automatically become available as a corresponding environment variable, you still need to load it explicitly, like this: env: - name: BAR valueFrom: secretKeyRef: name: foo key: bar Copy "},{"title":"Are your secret names matching the names in the reloader statement?​","type":1,"pageTitle":"Secrets Management","url":"docs/xks/developer-guide/secrets-management#are-your-secret-names-matching-the-names-in-the-reloader-statement","content":"If this is not the case, you will see errors when running kubectl describe &lt;podname&gt;. A well-behaved reloader will emit events that look like this: Normal SecretRotationComplete 4m22s (x889 over 29h) csi-secrets-store-rotation successfully rotated K8s secret &lt;secret-name&gt; Copy "},{"title":"Agents","type":0,"sectionRef":"#","url":"docs/xks/operator-guide/agents","content":"","keywords":""},{"title":"Governance​","type":1,"pageTitle":"Agents","url":"docs/xks/operator-guide/agents#governance","content":"The first step is to create the resource groups for the hub and agents. In the governance Terraform two resource groups have to be added to the common.tfvars file in the variables directory. The hub resource group has to be created for both Azure DevOps and GitHub, the only difference is the name of the resource group where the agents are located.  { common_name = &quot;hub&quot;, delegate_aks = false, delegate_key_vault = true, delegate_service_endpoint = false, delegate_service_principal = false, lock_resource_group = false, tags = { &quot;description&quot; = &quot;Hub Network&quot; } }, # Azure DevOps { common_name = &quot;azpagent&quot;, delegate_aks = false, delegate_key_vault = true, delegate_service_endpoint = false, delegate_service_principal = true, lock_resource_group = false, tags = { &quot;description&quot; = &quot;Azure Pipelines Agent&quot; } }, # GitHub { common_name = &quot;ghrunner&quot;, delegate_aks = false, delegate_key_vault = true, delegate_service_endpoint = false, delegate_service_principal = true, lock_resource_group = false, tags = { &quot;description&quot; = &quot;GitHub Runner&quot; } }, Copy The Service Principal credentials need to be stored as a secret when running Packer from GitHub. This step does not have to be followed when setting up Azure DevOps. The Service Principal id and credentials can be retrieved after the Terraform has been applied. Read the getting started guide for information about how to get the credential information, the difference being that the application will be named sp-rg-xks-prod-ghrunner-contributor instead of az-mg-lz-xks-owner. The secret should be added to the repository packer, as the VM image only has to be built for production it is enough to create the secret AZURE_CREDENTIALS_PROD. The format of the secret content should be as in the example below. {&quot;clientId&quot;: &quot;00000000-0000-0000-0000-000000000000&quot;, &quot;clientSecret&quot;: &quot;super-duper-secret-value&quot;, &quot;subscriptionId&quot;: &quot;00000000-0000-0000-0000-000000000000&quot;, &quot;tenantId&quot;: &quot;00000000-0000-0000-0000-000000000000&quot;} Copy "},{"title":"VM Image​","type":1,"pageTitle":"Agents","url":"docs/xks/operator-guide/agents#vm-image","content":"We make use of Packer to create the VM images. Packer allows for the automation of the process, creating the VM snapshot for us. The VM image has to be created before any VM image can be created. Create a repository called packer that is going to contain the CI jobs that will build the VM images. Doing this will allow for tracking of versions and automate the complicated build process. There are templates for Azure DevOps that can be used to build the VM images for the agents. The following pipeline definition should be commited to the file .ci/azure-pipelines-agent.yaml in the new packer repository. After that is done create a Azure DevOps pipeline for the given pipeline definition. name: $(Build.BuildId) trigger: none resources: repositories: - repository: templates type: git name: XKS/azure-devops-templates ref: refs/tags/2020.12.5 stages: - template: packer-docker/main.yaml@templates parameters: poolNameTemplate: &quot;&quot; azureSubscriptionTemplate: &quot;xks-{0}-owner&quot; resourceGroupTemplate: &quot;rg-{0}-we-azpagent&quot; packerTemplateRepo: &quot;https://github.com/XenitAB/packer-templates.git&quot; packerTemplateRepoBranch: &quot;2021.06.1&quot; packerTemplateFile: &quot;templates/azure/azure-pipelines-agent/azure-pipelines-agent.json&quot; Copy There is also a template for GitHub that can be used for building with Packer. The following pipeline definition should be committed to the file .github/workflows/github-runner.yaml in the packerrepository. name: packer_github_runner on: workflow_dispatch: {} jobs: packer: uses: xenitab/azure-devops-templates/.github/workflows/packer-docker.yaml@2021.11.1 with: ENVIRONMENTS: | { &quot;environments&quot;:[ {&quot;name&quot;:&quot;prod&quot;} ] } RESOURCE_GROUP_NAME_SUFFIX: &quot;ghrunner&quot; PACKER_TEMPLATE_REPO: &quot;https://github.com/XenitAB/packer-templates.git&quot; PACKER_TEMPLATE_REPO_BRANCH: &quot;2021.06.1&quot; PACKER_TEMPLATE_FILE: &quot;templates/azure/azure-pipelines-agent/azure-pipelines-agent.json&quot; secrets: AZURE_CREDENTIALS_PROD: ${{ secrets.AZURE_CREDENTIALS_PROD }} Copy Start the Packer build pipeline and allow it to run until completion. This may take up to 40 minutes to run so give it time. Afte the build is completed a new VM image should be created and stored in the agent's resource group in Azure. The name of the image is dynamic and includes a timestamp to allow versioning of the images. The following Azure CLI command gets the name of the image: # Assuming that you do not have any other image this RG. az image list -o json --query '[0].name' Copy The name should be similar to azp-agent-2021-04-09T08-18-30Z. "},{"title":"Pre Setup​","type":1,"pageTitle":"Agents","url":"docs/xks/operator-guide/agents#pre-setup","content":""},{"title":"GitHub​","type":1,"pageTitle":"Agents","url":"docs/xks/operator-guide/agents#github","content":"When using GitHub Runners a GitHub application has to be created that will allow the agent to communicate back to GitHub. Follow the steps in the GitHub Runner Documentation for instructions in how to create the GitHub Application with the correct permissions. In the end you should have created and installed a GitHub Application and have an application id, installation id, and private key. These parameters should all be stored in the already created Azure Key Vault in the ghrunner resource group. The secrets should be named github-app-id, github-private-key,github-installation-id, and github-organization. "},{"title":"Terraform​","type":1,"pageTitle":"Agents","url":"docs/xks/operator-guide/agents#terraform","content":"When setting up the Terraform make sure to set the correct value for azure_pipelines_agent_image_name or github_runner_image_name. If everything has been configured properly the hub VNET and VMs should be created without any issues. module &quot;hub&quot; { source = &quot;github.com/xenitab/terraform-modules//modules/azure/hub?ref=2021.05.12&quot; environment = var.environment location_short = var.location_short subscription_name = var.subscription_name azure_ad_group_prefix = var.azure_ad_group_prefix name = var.name vnet_config = var.vnet_config peering_config = var.peering_config } # Azure DevOps module &quot;azpagent&quot; { source = &quot;github.com/xenitab/terraform-modules//modules/azure/azure-pipelines-agent-vmss?ref=2021.05.12&quot; environment = var.environment location_short = var.location_short unique_suffix = var.unique_suffix name = &quot;azpagent&quot; azure_pipelines_agent_image_name = &quot;azp-agent-2021-06-11T06-44-34Z&quot; vmss_sku = &quot;Standard_F4s_v2&quot; vmss_disk_size_gb = 64 vmss_subnet_config = { name = module.hub.subnets[&quot;sn-${var.environment}-${var.location_short}-${var.name}-servers&quot;].name virtual_network_name = module.hub.virtual_networks.name resource_group_name = module.hub.resource_groups.name } } # GitHub module &quot;ghrunner&quot; { source = &quot;github.com/xenitab/terraform-modules//modules/azure/github-runner?ref=2021.05.12&quot; environment = var.environment location_short = var.location_short name = &quot;ghrunner&quot; github_runner_image_name = &quot;github-runner-2020-12-07T22-06-18Z&quot; vmss_sku = &quot;Standard_D2s_v3&quot; vmss_instances = 2 vmss_disk_size_gb = 50 unique_suffix = var.unique_suffix vmss_subnet_config = { name = module.hub.subnets[&quot;sn-${var.environment}-${var.location_short}-${var.name}-servers&quot;].name virtual_network_name = module.hub.virtual_networks.name resource_group_name = module.hub.resource_groups.name } } Copy "},{"title":"Post Setup​","type":1,"pageTitle":"Agents","url":"docs/xks/operator-guide/agents#post-setup","content":"After the cloud resources have been created their respective git providers have to be configured to be aware of the agent pools. Follow the instructions below to complete the post setup. "},{"title":"Azure DevOps​","type":1,"pageTitle":"Agents","url":"docs/xks/operator-guide/agents#azure-devops","content":"This step only has to be followed when setting up Azure DevOps Agents. To be able to communicate with the VMSS we need to configure a Service Connection. You will find service connection under a random project within Azure DevOps. To setup the Service Connection you need to get a secret generated by Terraform. # Assuming that you are connected to the correct subscription az keyvault secret show --vault-name &lt;vault-name&gt; --name &lt;secret-name&gt; -o tsv --query value # Example az keyvault secret show --vault-name kv-prod-we-core-1337 --name sp-rg-xks-prod-azpagent-contributor -o tsv --query value Copy Service Connections​ To create a new Service connection from Azure DevOps: Project settings -&gt; Service connections -&gt; New service connection -&gt; Azure Resource Manager -&gt; Service principal (manual) {&quot;clientId&quot;:&quot;12345&quot;,&quot;clientSecret&quot;:&quot;SoMuchSecret&quot;,&quot;subscriptionId&quot;:&quot;sub-id&quot;,&quot;tenantId&quot;:&quot;tenant-id&quot;} Subscription Id = subscriptionIdService Principal Id = clientIdService principal key = clientSecretTenant ID = tenantIdService connection name = random-name Agent Pool​ In Azure DevOps under project settings. Agent pools -&gt; Add Pool -&gt; Pick VMSS from dropdown  Billing​ Configure billing. This will increase your azure cost. Read up on how much on your own. Organization Settings -&gt; Billing Under &quot;Self-Hosted CI/CD&quot; set &quot;Paid parallel jobs&quot; = 3 "},{"title":"Peering Configuration​","type":1,"pageTitle":"Agents","url":"docs/xks/operator-guide/agents#peering-configuration","content":"To complete the setup we need to configure the VNET peering between the new hub VNET and the environments VNETs. This enables the agents to communicate with private resources without having to egress into the public Internet first. In the hubs prod.tfvars you want to add configuration to all VNETs that the VNET should have access to. If you have multiple environments there should be multiple entries in the list. peering_config = [ { name = &quot;core-dev&quot; remote_virtual_network_id = &quot;/subscriptions/your-sub-id/resourceGroups/rg-dev-we-core/providers/Microsoft.Network/virtualNetworks/vnet-dev-we-core&quot; allow_forwarded_traffic = true use_remote_gateways = false allow_virtual_network_access = true }, ] Copy A similar configuration has to be done in the core Terraform to complete the peering. peering_config = [ { name = &quot;hub&quot; remote_virtual_network_id = &quot;/subscriptions/your-sub-id/resourceGroups/rg-prod-we-hub/providers/Microsoft.Network/virtualNetworks/vnet-prod-we-hub&quot; allow_forwarded_traffic = true use_remote_gateways = false allow_virtual_network_access = true }, ] Copy "},{"title":"CVE Information","type":0,"sectionRef":"#","url":"docs/xks/operator-guide/cve","content":"","keywords":""},{"title":"General​","type":1,"pageTitle":"CVE Information","url":"docs/xks/operator-guide/cve#general","content":""},{"title":"OpenSSL CVE-2022-3602/CVE-2022-3786 Spooky SSL​","type":1,"pageTitle":"CVE Information","url":"docs/xks/operator-guide/cve#openssl-cve-2022-3602cve-2022-3786-spooky-ssl","content":"2022-11-01 Limited impact due to openssl 3 not being broadly used in the ecosystem. No impact on our kubernetes nodes We recommend developers to verify if their container images or application runtimes are effected. To get a quick overview of impacted system you can start to look at https://github.com/NCSC-NL/OpenSSL-2022/blob/main/software/README.md. EKS specific information. AKS specific information. "},{"title":"Kubernetes​","type":1,"pageTitle":"CVE Information","url":"docs/xks/operator-guide/cve#kubernetes","content":""},{"title":"CVE-2022-3294 Node address isn't always verified when proxying​","type":1,"pageTitle":"CVE Information","url":"docs/xks/operator-guide/cve#cve-2022-3294-node-address-isnt-always-verified-when-proxying","content":"2022-11-10 Not a problem in AKS clusters due to the usage of Konnectivity. XKS users can't modify node objects so it shouldn't be a problem in EKS ether. "},{"title":"CVE-2022-3162 Unauthorized read of Custom Resources​","type":1,"pageTitle":"CVE Information","url":"docs/xks/operator-guide/cve#cve-2022-3162-unauthorized-read-of-custom-resources","content":"2022-11-10 Developers do not have cluster wide read access on any CRD in XKS. Due to this the CVE isn't an issue. "},{"title":"Blue Green Clusters","type":0,"sectionRef":"#","url":"docs/xks/operator-guide/blue-green","content":"","keywords":""},{"title":"Workflow​","type":1,"pageTitle":"Blue Green Clusters","url":"docs/xks/operator-guide/blue-green#workflow","content":"We assume that the workloads on the clusters are stateless and can run multiple instances. Set up a new cluster in the target environment using TerraformVerify that the new cluster is functioning as intended You will not be able to verify any ingressYou will not be able to use AZAD-proxy in the newly created cluster Change the TXT DNS records over to the newly created clusterVerify that the ingress traffic is migrated to the new cluster and it is working as intendedDestroy the old cluster using terraform "},{"title":"DNS migration​","type":1,"pageTitle":"Blue Green Clusters","url":"docs/xks/operator-guide/blue-green#dns-migration","content":"You can find a small small script bellow to make the migration of DNS easier. As always use at your own risk and make sure that you understand what the script does. Our recommendation is that you migrate one DNS record manually and verify that the ingress and the new cluster is working as intended, when you know that you can run the script. "},{"title":"Azure​","type":1,"pageTitle":"Blue Green Clusters","url":"docs/xks/operator-guide/blue-green#azure","content":"ENVIRONMENT=&quot;dev&quot; OLD_OWNER_ID=&quot;${ENVIRONMENT}-aks1&quot; NEW_OWNER_ID=&quot;${ENVIRONMENT}-aks2&quot; RESOURCE_GROUP_NAME=&quot;rg-${ENVIRONMENT}-we-aks&quot; ZONE_NAME=&quot;${ENVIRONMENT}.domain.se&quot; ZONE_RECORDS=$(az network dns record-set txt list -g ${RESOURCE_GROUP_NAME} -z ${ZONE_NAME} | jq -rc '.[]') ZONE_RECORDS_CSV_ARRAY=( $(jq -rc '. | [.name, (.txtRecords[0].value[0] | @base64)] | join(&quot;;&quot;)' &lt;&lt;&lt; &quot;${ZONE_RECORDS}&quot;) ) for ZONE_RECORD_CSV in &quot;${ZONE_RECORDS_CSV_ARRAY[@]}&quot;; do ZONE_RECORD_NAME=$(awk -F';' '{print $1}' &lt;&lt;&lt; $ZONE_RECORD_CSV) OLD_ZONE_TXT_VALUE=$(awk -F';' '{print $2}' &lt;&lt;&lt; $ZONE_RECORD_CSV | base64 -d) if [[ ${OLD_ZONE_TXT_VALUE} =~ &quot;owner=${OLD_OWNER_ID}&quot; ]]; then NEW_ZONE_TXT_VALUE=${OLD_ZONE_TXT_VALUE/owner=${OLD_OWNER_ID}/owner=${NEW_OWNER_ID}} echo Updating external-dns owner of ${ZONE_RECORD_NAME}: ${OLD_OWNER_ID} to ${NEW_OWNER_ID} az network dns record-set txt add-record --resource-group ${RESOURCE_GROUP_NAME} --zone-name ${ZONE_NAME} --record-set-name ${ZONE_RECORD_NAME} --value &quot;${NEW_ZONE_TXT_VALUE}&quot; 1&gt;/dev/null az network dns record-set txt remove-record --resource-group ${RESOURCE_GROUP_NAME} --zone-name ${ZONE_NAME} --record-set-name ${ZONE_RECORD_NAME} --value &quot;${OLD_ZONE_TXT_VALUE}&quot; 1&gt;/dev/null fi done Copy "},{"title":"XKF on Github","type":0,"sectionRef":"#","url":"docs/xks/operator-guide/github","content":"","keywords":""},{"title":"Terraform​","type":1,"pageTitle":"XKF on Github","url":"docs/xks/operator-guide/github#terraform","content":"How to run Terraform plan and apply through a GitHub action workflow. "},{"title":"Workflow​","type":1,"pageTitle":"XKF on Github","url":"docs/xks/operator-guide/github#workflow","content":"Just like in the Azure DevOps case we have created a basic pipeline for easy use. Below you can find an example pipeline that uses the Github Actions workflow. Read further down to see how to create the secrets needed to run the pipeline. You should store this GitHub action in your Terraform repository under .github/workflows/name.yaml name: terraform_core on: push: branches: - main paths: - core/** pull_request: paths: - core/** workflow_dispatch: inputs: OPA_BLAST_RADIUS: description: OPA Blast Radius required: true default: &quot;50&quot; jobs: terraform: uses: xenitab/azure-devops-templates/.github/workflows/terraform-docker.yaml@2021.10.1 with: DIR: core runs-on: '[&quot;self-hosted&quot;, &quot;linux&quot;]' # If you do not want to use the default ubuntu-latest ENVIRONMENTS: | { &quot;environments&quot;:[ {&quot;name&quot;:&quot;dev&quot;}, {&quot;name&quot;:&quot;qa&quot;}, {&quot;name&quot;:&quot;prod&quot;} ] } secrets: AZURE_CREDENTIALS_DEV: ${{ secrets.AZURE_CREDENTIALS_DEV }} AZURE_CREDENTIALS_QA: ${{ secrets.AZURE_CREDENTIALS_QA }} AZURE_CREDENTIALS_PROD: ${{ secrets.AZURE_CREDENTIALS_PROD }} Copy "},{"title":"Self-hosted runners​","type":1,"pageTitle":"XKF on Github","url":"docs/xks/operator-guide/github#self-hosted-runners","content":"It is currently not possible to use self-hosted runners hosted in GitHub organization X while calling on workflows in GitHub organization Y. To be able to use self-hosted runners you have to import (not fork) the repository to organization X the workflow repositoryand make it public. If you do not do this private repositories located in organization X will not be able to find the workflows. "},{"title":"Azure Service Principal​","type":1,"pageTitle":"XKF on Github","url":"docs/xks/operator-guide/github#azure-service-principal","content":"Create a Service Principal(SP) with the access that Terraform requires to perform all the tasks you want. You can read more about SP creation in our getting started guide The workflow is using Azure Login GitHub Actionto login to Azure. When uploading your SP to GitHub make sure to follow the formatting in the examples. This is to prevent unnecessary masking of { } in your logs which are in dictionary form. For example, do: {&quot;clientId&quot;: &quot;00000000-0000-0000-0000-000000000000&quot;, &quot;clientSecret&quot;: &quot;super-duper-secret-value&quot;, &quot;subscriptionId&quot;: &quot;00000000-0000-0000-0000-000000000000&quot;, &quot;tenantId&quot;: &quot;00000000-0000-0000-0000-000000000000&quot;} Copy instead of: { &quot;clientId&quot;: &quot;00000000-0000-0000-0000-000000000000&quot;, &quot;clientSecret&quot;: &quot;super-duper-secret-value&quot;, &quot;subscriptionId&quot;: &quot;00000000-0000-0000-0000-000000000000&quot;, &quot;tenantId&quot;: &quot;00000000-0000-0000-0000-000000000000&quot; } Copy Upload the entire JSON as your GitHub secret. The workflow uses one secret per environment and we recommend that you follow our naming standard. The secret name the workflow uses is AZURECREDENTIALS\\&lt;ENV&gt;, for example AZURE_CREDENTIALS_DEV. To upload the secret to GitHub you can use the GitHub UI or you can use the GitHub CLI to upload secrets to GitHub. Assuming that you are storing the SP JSON data in a file you could do: gh secret -R ORG/xks-terraform set AZURE_CREDENTIALS_DEV &lt; dev-secrets.json gh secret -R ORG/xks-terraform set AZURE_CREDENTIALS_QA &lt; qa-secrets.json gh secret -R ORG/xks-terraform set AZURE_CREDENTIALS_PROD &lt; prod-secrets.json Copy "},{"title":"EKS","type":0,"sectionRef":"#","url":"docs/xks/operator-guide/kubernetes/eks","content":"","keywords":""},{"title":"Differences​","type":1,"pageTitle":"EKS","url":"docs/xks/operator-guide/kubernetes/eks#differences","content":"To setup XKF using EKS you still need an Azure environment. XKF is heavily relying on Azure AD (AAD) and we have developed our own tool to manage access to our clusters called azad-kube-proxy. Our governance solution is still fully located in Azure together with our Terraform state. "},{"title":"Repo structure​","type":1,"pageTitle":"EKS","url":"docs/xks/operator-guide/kubernetes/eks#repo-structure","content":"This is how an AWS repo structure can look like: ├── Makefile ├── README.md ├── aws-core │ ├── main.tf │ ├── outputs.tf │ ├── variables │ │ ├── common.tfvars │ │ ├── dev.tfvars │ │ ├── prod.tfvars │ │ └── qa.tfvars │ └── variables.tf ├── aws-eks │ ├── main.tf │ ├── outputs.tf │ ├── variables │ │ ├── common.tfvars │ │ ├── dev.tfvars │ │ ├── prod.tfvars │ │ └── qa.tfvars │ └── variables.tf ├── azure-governance │ ├── main.tf │ ├── outputs.tf │ ├── variables │ │ ├── common.tfvars │ │ ├── dev.tfvars │ │ ├── prod.tfvars │ │ └── qa.tfvars │ └── variables.tf ├── global.tfvars Copy "},{"title":"EKS​","type":1,"pageTitle":"EKS","url":"docs/xks/operator-guide/kubernetes/eks#eks","content":"Just like in AKS we use Calico as our CNI. AWS CNI does not support network policiesAWS CNI heavily limits how many pods we can run on a single nodeWe want to be consistent with AKS Just after setting up the EKS cluster we use a null_resource to first delete the AWS CNI daemon set and then install calico. This is all done before we add a single node to the cluster. After this we add an EKS node group and Calico starts. "},{"title":"IRSA​","type":1,"pageTitle":"EKS","url":"docs/xks/operator-guide/kubernetes/eks#irsa","content":"In AKS we use AAD Pod Identity to support access to Azure resources. We support the same thing in EKS but use IAM roles for service accounts IRSA. To make it easier to use IRSA we have developed a small terraform module. "},{"title":"Bootstrap​","type":1,"pageTitle":"EKS","url":"docs/xks/operator-guide/kubernetes/eks#bootstrap","content":"By default AWS CNI limits the amount of pods that you can have on a single node. Since we are using Calico we do not have this limit, but when setting up a default EKS environment the EKS bootstrap scriptdefines a pod limit. To remove this limit we have created our own AWS launch template for our EKS node group. It sets --use-max-pods false and some needed Kubernetes node labels. If these labels are not set the EKS cluster is unable to &quot;find&quot; the nodes in the node group. "},{"title":"Tenants account peering​","type":1,"pageTitle":"EKS","url":"docs/xks/operator-guide/kubernetes/eks#tenants-account-peering","content":"In Azure we separates XKF and our tenants by using Resource Groups, in AWS we use separate accounts. To setup a VPC peering you need to know the target VPC id, this creates a chicken and egg problem. To workaround this problem we sadly have to run the eks/core module multiple times on both the XKF side and the tenant side. Run Terraform in the following order: XKF core without any vpc_peering_config_requester defined.Tenant core without any vpc_peering_config_accepter defined.XKF core defines vpc_peering_config_requester, manually getting the needed information from the tenant account.Tenant core defines vpc_peering_config_accepter, manually getting the needed information from the XKF account. Make sure that you only have one peering request open at the same time, else the accepter side will not be able to find a unique request. Now you should be able to see the VPC peering connected on both sides. "},{"title":"Update cluster version​","type":1,"pageTitle":"EKS","url":"docs/xks/operator-guide/kubernetes/eks#update-cluster-version","content":"Updating the EKS cluster version can not be done by updating Terraform code only, it also involves the AWS CLI and kubectl. Find your EKS version to upgrade to here: EKS versions For further information on the AWS CLI commands used in this section, please refer to the AWS EKS CLI documentation. "},{"title":"Update the control plane using AWS CLI​","type":1,"pageTitle":"EKS","url":"docs/xks/operator-guide/kubernetes/eks#update-the-control-plane-using-aws-cli","content":"Get the name of the cluster to update: aws eks list-clusters --region eu-west-1 Copy Update the control plane version by running the following command: aws eks update-cluster-version --region eu-west-1 --name &lt;cluster-name&gt; --kubernetes-version &lt;version&gt; Copy The above command provides an id that can be use to check the status of the update: aws eks describe-update --region eu-west-1 --name &lt;cluster-name&gt; --update-id &lt;id&gt; Copy The update is finished when status is Successful. Previous updates have taken approximately 45 minutes. In the aws-eks/variables/&lt;environment&gt;.tfvars Terraform file that corresponds to the actual environment, update the kubernetes_version in eks_config and make a terraform plan. No difference in the plan output is expected. Also perform a terraform apply just to make sure state the state is updated (might not be needed). "},{"title":"Update the control plane using Terraform​","type":1,"pageTitle":"EKS","url":"docs/xks/operator-guide/kubernetes/eks#update-the-control-plane-using-terraform","content":"TBD "},{"title":"Update the nodes​","type":1,"pageTitle":"EKS","url":"docs/xks/operator-guide/kubernetes/eks#update-the-nodes","content":"In the aws-eks/variables/&lt;environment&gt;.tfvars Terraform file that corresponds to the actual environment, add a new node group in eks_config. The example below shows a node upgrade from 1.20 to 1.21 where standard2 is the new node group. The value of release_version must match an AMI version (preferrably the latest) for the actual Kubernetes version (can be found here): eks_config = { kubernetes_version = &quot;1.21&quot; cidr_block = &quot;10.100.64.0/18&quot; node_groups = [ { name = &quot;standard&quot; release_version = &quot;1.20.4-20210621&quot; min_size = 3 max_size = 4 instance_types = [&quot;t3.large&quot;] }, { name = &quot;standard2&quot; release_version = &quot;1.21.5-20220123&quot; min_size = 3 max_size = 4 instance_types = [&quot;t3.large&quot;] }, ] } Copy When this change is applied, there will be a new set of nodes running the new version added to the cluster. The following command will show all nodes and their versions: kubectl get nodes Copy Now it is time to drain the old nodes one by one with: kubectl drain &lt;node-name&gt; --ignore-daemonsets --delete-emptydir-data Copy When all nodes are drained, remove the old node group in eks_config. From the example above: eks_config = { kubernetes_version = &quot;1.21&quot; cidr_block = &quot;10.100.64.0/18&quot; node_groups = [ { name = &quot;standard2&quot; release_version = &quot;1.21.5-20220123&quot; min_size = 3 max_size = 4 instance_types = [&quot;t3.large&quot;] }, ] } Copy When applied, the old nodes are removed. The update is now complete. "},{"title":"Command examples​","type":1,"pageTitle":"EKS","url":"docs/xks/operator-guide/kubernetes/eks#command-examples","content":"The following AWS CLI commands are an example of an update from 1.20 to 1.21: Control plane: aws eks list-clusters --region eu-west-1 aws eks update-cluster-version --region eu-west-1 --name qa-eks2 --kubernetes-version 1.21 aws eks describe-update --region eu-west-1 --name qa-eks2 --update-id 25b9f04f-0be3-40ca-bc37-aaf841070012 Copy "},{"title":"Break glass​","type":1,"pageTitle":"EKS","url":"docs/xks/operator-guide/kubernetes/eks#break-glass","content":"We are very dependent on azad-proxy to work but if something happens with the ingress, azad-proxy or the AAD we need to have ways of reaching the cluster: aws eks --region eu-west-1 update-kubeconfig --name dev-eks1 --alias dev-eks1 --role-arn arn:aws:iam::111111111111:role/xkf-eu-west-1-dev-eks-admin Copy "},{"title":"EKS resources​","type":1,"pageTitle":"EKS","url":"docs/xks/operator-guide/kubernetes/eks#eks-resources","content":"To get a quick overview of what is happening in EKS you can look at its changelog. When upgrading node groups you need to correlate with your Kubernetes release, you can find which node group is available to which node group. AWS general security information Public containers roadmap. "},{"title":"Getting Started","type":0,"sectionRef":"#","url":"docs/xks/operator-guide/getting-started","content":"","keywords":""},{"title":"Bootstrap​","type":1,"pageTitle":"Getting Started","url":"docs/xks/operator-guide/getting-started#bootstrap","content":""},{"title":"Add New Tenant​","type":1,"pageTitle":"Getting Started","url":"docs/xks/operator-guide/getting-started#add-new-tenant","content":"When creating a new tenant there are a number of (for now manual) processes to perform. In this scenario we are assuming that you are using Azure DevOps and that you have already created a project and organization. In many places in the text we have provided names, they are just examples, all of these names can be exchanged to fit your needs. In this case let us call it project1. In the future we should manually import a repository.https://github.com/XenitAB/azure-devops-templates "},{"title":"Import azure-devops-templates pipeline​","type":1,"pageTitle":"Getting Started","url":"docs/xks/operator-guide/getting-started#import-azure-devops-templates-pipeline","content":"To make sure that the azure-devops-templates repo is up to date we have an automatic CI that fetches updates from upstream to your local Azure DevOps clone. Go to pipelines -&gt; New pipeline -&gt; Azure Repos Git -&gt; azure-devops-templates -&gt; Existing Azure Pipelines YAML file Import the pipeline from the following path: /.ci/pipeline.yaml "},{"title":"Setup XKS​","type":1,"pageTitle":"Getting Started","url":"docs/xks/operator-guide/getting-started#setup-xks","content":"In this case we will only setup a single XKS cluster in one environment, in our case dev. It is easy to add more environments when you have created your first one. At Xenit we are using Terraform modules that we share upstream To setup XKS we will utilize 4 modules: governance-globalgovernance-regionalcoreaks "},{"title":"Create Terraform repo​","type":1,"pageTitle":"Getting Started","url":"docs/xks/operator-guide/getting-started#create-terraform-repo","content":"Of course we need a place to store our Terraform code so create one in your Azure DevOps organization. TODO create a example repo that uses our Terraform modules. You can today see a example of the Makefile. This is how we normally structure our tenant repo. ├── Makefile ├── README.md ├── aks │ ├── main.tf │ ├── outputs.tf │ ├── variables │ │ ├── common.tfvars │ │ ├── dev.tfvars │ │ ├── prod.tfvars │ │ └── qa.tfvars │ └── variables.tf ├── core │ ├── main.tf │ ├── outputs.tf │ ├── variables │ │ ├── common.tfvars │ │ ├── dev.tfvars │ │ ├── prod.tfvars │ │ └── qa.tfvars │ └── variables.tf ├── global.tfvars ├── governance │ ├── main.tf │ ├── outputs.tf │ ├── variables │ │ ├── common.tfvars │ │ ├── dev.tfvars │ │ ├── prod.tfvars │ │ └── qa.tfvars │ └── variables.tf Copy "},{"title":"Update repo​","type":1,"pageTitle":"Getting Started","url":"docs/xks/operator-guide/getting-started#update-repo","content":"We need to update a number of settings in a number of places in your Terraform repo. Generate a SUFFIX that should be tfstate + a few random numbers, for example tfstate1234. Update the Makefile SUFFIX variable with the suffix and the random number. Also update global.tfvars with the same random number. "},{"title":"Create Terraform storage​","type":1,"pageTitle":"Getting Started","url":"docs/xks/operator-guide/getting-started#create-terraform-storage","content":"In order to store a Terraform state we need to prepare that. We have written a small Go tool that will help out with that. Instead of running these scripts manually we will use the makefile. We use one Terrafrom state per DIR and ENV. Lets create the first Terraform state, in this case governance. make prepare ENV=dev DIR=governance You will need to run the prepare command for each separate Terraform folder. "},{"title":"Configure governance​","type":1,"pageTitle":"Getting Started","url":"docs/xks/operator-guide/getting-started#configure-governance","content":"After defining the variables and you have have applied your config: make plan ENV=dev DIR=governance # If everything looks good make apply ENV=dev DIR=governance Copy By default you don't get access to the key vaults that governance creates. You should probably give the access to a group that you and your team have access to but to get started you can run the following. AZ_ID=$(az ad user show --id &quot;your@email&quot; --output tsv --query id) KEYVAULTNAME=kv-favorite-name az keyvault set-policy --name $KEYVAULTNAME --object-id $AZ_ID --secret-permissions backup delete get list purge recover restore set --key-permissions backup create decrypt delete encrypt get import list purge recover restore sign unwrapKey update verify wrapKey --certificate-permissions backup create delete deleteissuers get getissuers import list listissuers managecontacts manageissuers purge recover restore setissuers update Copy "},{"title":"Configure core​","type":1,"pageTitle":"Getting Started","url":"docs/xks/operator-guide/getting-started#configure-core","content":"Get a CIDR network for your AKS env per env. Define in core/variables/env.tfvars. "},{"title":"Configure AKS cluster​","type":1,"pageTitle":"Getting Started","url":"docs/xks/operator-guide/getting-started#configure-aks-cluster","content":"How big should your cluster be?Which version of Kubernetes should you run?What DNS zone should you use?What SKU tier should your cluster have?What size should your k8s nodes have? All of this is configured under aks/variables/prod.tfvars. environment = &quot;prod&quot; dns_zone = &quot;prod.aks.xenit.io&quot; aks_config = { kubernetes_version = &quot;1.20.7&quot; sku_tier = &quot;Free&quot; default_node_pool = { orchestrator_version = &quot;1.20.7&quot; vm_size = &quot;Standard_D2as_v4&quot; min_count = 1 max_count = 1 node_labels = {} }, additional_node_pools = [ { name = &quot;standard2&quot; orchestrator_version = &quot;1.20.7&quot; vm_size = &quot;Standard_D2as_v4&quot; min_count = 1 max_count = 4 node_labels = {} node_taints = [] }, ] } Copy Notice the vm_size = Standard_D2as_v4 "},{"title":"GitOps using Flux​","type":1,"pageTitle":"Getting Started","url":"docs/xks/operator-guide/getting-started#gitops-using-flux","content":"If you want Flux to manage your GitOps repo from the get go you can enable this in aks/variables/common.tfvars. In my case I will have Flux manage a namespace called monitor and sync a repo under monitor-gitops. You need to create the repository in Azure Devops that you link to before applying this Terraform. The repository can be empty. You will also need to create a separate repository for fleet-infra, this repo is used to store Flux config. This repo cannot be empty and needs a README file or something similar to work as intended before you run Terraform. In the example below we are using Azure DevOps as our CSM system, but we also support GitHub. If you want to use GitHub just fill in its config and make azure_devops empty instead. namespaces = [ { name = &quot;monitor&quot; delegate_resource_group = true labels = { &quot;terraform&quot; = &quot;true&quot; } flux = { enabled = true azure_devops = { org = &quot;organization1&quot; proj = &quot;project1&quot; repo = &quot;monitor-gitops&quot; } github = { repo = &quot;&quot; } } } ] Copy "},{"title":"Terraform CI/CD​","type":1,"pageTitle":"Getting Started","url":"docs/xks/operator-guide/getting-started#terraform-cicd","content":"We have one CI/CD pipeline per terraform directory. You can find ready to use pipelines under .ci/ in the Terraform repo. "},{"title":"Configure service principal​","type":1,"pageTitle":"Getting Started","url":"docs/xks/operator-guide/getting-started#configure-service-principal","content":"There are a few manual steps that you need to perform before we can start to configure the CI/CD pipeline. Service principal access​ A couple of manual steps are required before Terraform can be applied. A main service principal has to be created in the tenants Azure AD, as it will be used by the Terraform modules. Create the new service principal. It should have a name with the format sp-sub-&lt;subscription_name&gt;-all-owner. Most likely the subscription name will be xks. AZ_APP_NAME=&quot;sp-sub-&lt;subscription_name&gt;-all-owner&quot; AZ_APP_ID=$(az ad app create --display-name ${AZ_APP_NAME} --sign-in-audience AzureADMyOrg --query appId -o tsv) AZ_APP_OBJECT_ID=$(az ad app show --id ${AZ_APP_ID} --output tsv --query id) az ad sp create --id ${AZ_APP_OBJECT_ID} Copy Grant the service principal additional permissions in the App Registration. The permissions Group.ReadWrite.All and Application.ReadWrite.All in Microsoft Graph should be added. After the permissions are added grant admin consent for the Tenant. Make the service principal Owner of all the XKS subscriptions. This is done in the IAM settings of each individual subscription. Additionaly the service principal also needs to be member of the User administrator role. Create three Azure AD groups. These will be used to assing users a owner, contributor, or reader role for all resources.. az ad group create --display-name az-sub-&lt;subscription_name&gt;-all-owner --mail-nickname az-sub-&lt;subscription_name&gt;-all-owner az ad group create --display-name az-sub-&lt;subscription_name&gt;-all-contributor --mail-nickname az-sub-&lt;subscription_name&gt;-all-contributor az ad group create --display-name az-sub-&lt;subscription_name&gt;-all-reader --mail-nickname az-sub-&lt;subscription_name&gt;-all-reader Copy You can also view to original documentation on how to create a SP. In some cases it might be useful to create a group where both a admin group for you as an admin the SP can use and assign the group the needed access. Depending on how your global.tfvars looks like it will be called something like: az-mg-lz-xks-owner. Create Service principal key​ The SP that we use is generated by Terraform but we do not store the key anywhere, so this is among the few times that we have to do something manual. First find the SP that you will use, this will depend on your Terraform config. There is no CLI command to create a new key so it is done through the portal. AAD -&gt; App registrations -&gt; All applications -&gt; search for the application -&gt; Certificates &amp; secrets -&gt; New client secret The key is only shown once, so copy it some where safe for long-term storage. This key will be used when creating the service connection in Azure DevOps. "},{"title":"Setup Service Connection​","type":1,"pageTitle":"Getting Started","url":"docs/xks/operator-guide/getting-started#setup-service-connection","content":"To be able to talk from Azure DevOps to Azure and be able to run Terraform on push we need to configure service connections. Now you will also need the key that we created in the SP earlier. Get the config: # Service Principal Id APP_ID=$(az ad sp list --display-name sp-sub-xks-all-owner -o tsv --query '[].appId') # Tenant ID TENANT_ID=$(az account show -o tsv --query tenantId) # Subscription Id SUB_ID=$(az account show -o tsv --query id) Copy In Azure DevOps: Project settings -&gt; Service connections -&gt; New service connection -&gt; Azure Resource Manager -&gt; Service principal (manual) Subscription Id = $SUB_IDService Principal Id = $APP_IDService principal key = The key created in the earlier stepTenant ID = $TENANT_IDService connection name = xks-${environment}-owner "},{"title":"Update pipelines​","type":1,"pageTitle":"Getting Started","url":"docs/xks/operator-guide/getting-started#update-pipelines","content":"Update the variable azureSubscriptionTemplate. You can find the value under Project settings -&gt; Service Connections  In my case sp-sub-project1-xks: name: $(Build.BuildId) variables: - name: azureSubscriptionTemplate value: &quot;sp-sub-project1-xks-{0}-owner&quot; - name: terraformFolder value: &quot;governance&quot; - name: sourceBranch value: &quot;refs/heads/main&quot; Copy Also update the project path in &quot;name&quot;. Also notice the ref, the ref points to which version of the module that you are using: resources: repositories: - repository: templates type: git name: project1/azure-devops-templates ref: refs/tags/2021.03.1 Copy "},{"title":"Add CI/CD pipelines​","type":1,"pageTitle":"Getting Started","url":"docs/xks/operator-guide/getting-started#add-cicd-pipelines","content":"Once again add a pipeline. Assuming that you named your repository to Terraform Pipelines -&gt; New pipeline -&gt; Azure Repos Git -&gt; Terraform -&gt; Existing Azure Pipelines YAML file Import the pipeline from the following path: .ci/pipeline-governance.yaml Hopefully after adding the pipeline the pipeline should automatically trigger and the plan and apply stages should go through without any problems. "},{"title":"Create PAT secret​","type":1,"pageTitle":"Getting Started","url":"docs/xks/operator-guide/getting-started#create-pat-secret","content":"To make it possible for flux to clone repos from azure devops we need to create a Personal Access Token(PAT). User Settings -&gt; Personal access tokens -&gt; New Token  Create a PAT  Copy the generated key, we will need it for the next step. "},{"title":"Add PAT to Azure Key Vaults​","type":1,"pageTitle":"Getting Started","url":"docs/xks/operator-guide/getting-started#add-pat-to-azure-key-vaults","content":"To make it possible for terraform to reach the PAT in a easy and secure way we have chosen to store the PAT in Azure Key Vaults which you need to add manually. Azure CLI​ You can add the secret using the az CLI. Call the secret azure-devops-pat and the value should be the token you created in Azure DevOps. # List all key vaults az keyvault list # Create the secret az keyvault secret set --vault-name kv-dev-we-core-1234 --name azure-devops-pat --value &quot;SuperSecretToken123&quot; Copy Azure portal​ Or if you prefer use the UI. In the Azure portal search for &quot;Key vaults&quot; and pick the core one that matches the unique_suffix that you have specified in global.tfvars, in our case 1234. Key vaults -&gt; core-1234 -&gt; Secrets -&gt; Generate/Import  Call the secret azure-devops-pat and add the PAT key that you created in the previous step. "},{"title":"Admin and developer access​","type":1,"pageTitle":"Getting Started","url":"docs/xks/operator-guide/getting-started#admin-and-developer-access","content":"Hopefully you should now have one XKS cluster up and running, but currently no developer can actually reach the cluster. In XKF we see clusters as cattle and at any time we can decide to recreate an XKS cluster. To be able to do this without our developers even knowing we use blue green clusters. TODO write a document on how blue green clusters works and link. We use GitOps together with DNS to be able to migrate applications without any impact to end-users assuming that our developers have written 12 step applications. To store state we utilize the cloud services available in the different clouds that XKF supports. To make sure that our developers do not notice when we change our the cluster we have written a Kubernetes API Proxy called azad-kube-proxy. "},{"title":"Azure AD Kubernetes Proxy​","type":1,"pageTitle":"Getting Started","url":"docs/xks/operator-guide/getting-started#azure-ad-kubernetes-proxy","content":"AZAD as we also call it, is a deployment that runs inside XKS and sits in front of the Kubernetes API. We also supply a krew/kubectl plugin to make it easy for our developers to use AZAD. For instructions on how to setup and configure this see. AZAD Usage​ Install krew: https://krew.sigs.k8s.io/docs/user-guide/setup/install/#windowsInstall the azad-proxy plugin: kubectl krew install azad-proxyLogin with the Azure CLI (a valid session with azure cli is always required): az loginList all the available clusters: kubectl azad-proxy menu You can also use the discover function: kubectl azad-proxy discover kubectl azad-proxy generate --cluster-name dev-cluster --proxy-url https://dev.example.com --resource https://dev.example.com Copy "},{"title":"AAD groups​","type":1,"pageTitle":"Getting Started","url":"docs/xks/operator-guide/getting-started#aad-groups","content":"To make it possible for our developers and admins to actually login to the cluster we need to add them to a AAD group. If you are a AAD guest user you need to add the AAD Role: Directory Reader to your user account. AZAD proxy parses the AAD and that is why the user needs Directory Reader. No subscription​ If you have not gotten any of the RG groups that XKF generates and perform az login you might see an error saying that you do not have any subscriptions. This is more likely if you are running XKF in AWS but also possible in Azure. Do as the error suggest and use the --allow-no-subscription flag. # The variable TENANT_ID = your tenant id az login $TENANT_ID --allow-no-subscription Copy AZAD proxy should still work. Developer groups​ Depending on what configuration you did in global.tfvars this will differ but the group name should be something like bellow. This group will give your developers contributor access in the namespaces where they have access. &lt;azure_ad_group_prefix&gt;-rg-xks-&lt;cluster-env&gt;-aks-contributor Example: az-rg-xks-dev-aks-contributor Admin groups​ To make it easy for you as a admin you should also use AZAD. To give yourself cluster-admin access: &lt;azure_ad_group_prefix&gt;-xks-&lt;cluster-env&gt;-clusteradmin Example: aks-xks-dev-clusteradmin Verify access​ There is a flag in Kubernetes called --as, which enables you to see if a specific user got access to a specific resource. Note this will not work if you are connecting to the cluster using AZAD-proxy due to it using the --as flag to run the commands for you. Since we are using OIDC we also need to provide the group id, you can find the group id in AAD. You can find the UUID of the group in AAD. kubectl get pods --as-group=12345678-1234-1234-1234-00000000000 --as=&quot;fake&quot; If you already have a rolebinding where a existing UUID exist you can run the following command: kubectl get pods --as-group=$(kubectl get rolebinding &lt;rolebiding-name&gt; -o jsonpath='{.subjects[0].name}') --as=&quot;fake&quot; "},{"title":"Authorized IPs​","type":1,"pageTitle":"Getting Started","url":"docs/xks/operator-guide/getting-started#authorized-ips","content":"To minimize the exposure of the XKS clusters we define a list of authorized IP:s that is approved to connect the Kubernetes cluster API. We need to approve multiple infrastructure networks and user networks. If you are using the HUB module and you are running VMSS Azure Devops Agent you need to approve those IP:s as authorized.The AKS public IPYour developers' public IP A recommendation is to add a comment with what IP you have added. aks_authorized_ips = [ &quot;8.8.8.8/32&quot;, # google dns &quot;1.2.3.4/32&quot;, # developer x ip &quot;2.3.4.5/30&quot;, # AKS0 dev ] "},{"title":"Networking","type":0,"sectionRef":"#","url":"docs/xks/operator-guide/networking","content":"","keywords":""},{"title":"Kubernetes​","type":1,"pageTitle":"Networking","url":"docs/xks/operator-guide/networking#kubernetes","content":"TBD "},{"title":"Node Local DNS​","type":1,"pageTitle":"Networking","url":"docs/xks/operator-guide/networking#node-local-dns","content":"To lower DNS query latency and a number of other reasonswe are using NodeLocal DNS in XKS. Node Local DNS is an application that runs on each node and creates a loopback interface on each node together with a number of iptables rules. The iptables rules intercepts all the DNS traffic from all pods that is sent to the clusters DNS server. Node Local DNS Configuration​ To configure Node Local DNS you need to provide two values. The IP of the central DNS server in your cluster, you can find this by running: kubectl get svc kube-dns -n kube-system -o jsonpath={.spec.clusterIP}. The second value is a random IP that you know that nothing else in the cluster is ever going to use, in our case we used the example ip 169.254.20.10. These values are defined for you in XKS but it's good to know about them and where to find them. Here you can view the example configuration provided in the docs. Node Local DNS is built on top of CoreDNS and is plugin based. Depending on your needs you can easily enable new features. By default NodeLocal DNS don't log the DNS requests it gets but it can make it hard to debug. In XKS we haven't enabled any debug logs ether but if you need to enable it all you need to do is to add log as part of the plugins defined in your yaml. For example: data: Corefile: | .:53 { errors log cache 30 reload loop bind 169.254.20.10 10.0.0.10 forward . __PILLAR__UPSTREAM__SERVERS__ prometheus :9253 } Copy For you as a XKS administrator the biggest chance to change is in the cache plugin. Instead of me trying to rewrite the docs I recommend you to read it but we have changed the default value and at the time of writing we use the following config: data: Corefile: | .:53 { log errors cache { success 9984 30 denial 9984 10 prefetch 20 60s 15% } reload loop bind 169.254.20.10 10.0.0.10 forward . /etc/resolv.conf prometheus :9253 } Copy The prefetch feature allows us to automatically get DNS entries that is in the cache and automatically update it before the DNS TTL ends. Remember that the cache TTL won't change the TTL of your cached DNS entries. If the DNS entry have a TTL of 1 minute and the cache have a TTL of 5 minutes the DNS entry will disappear after 1 minute. If you for example define a cache without setting success and denial but set the prefetch config the default TTL cache value will still be applied. data: Corefile: | .:53 { log errors cache { prefetch 20 60s 15% } reload loop bind 169.254.20.10 10.0.0.10 forward . /etc/resolv.conf prometheus :9253 } Copy Node local DNS networkpolicy​ Sadly when using NodeLocal DNS together with Networkpolicy and the Calico CNI you need to write a networkpolicy that instead of using label selectors on a pod level you need to write a ruler that will work on the node levelWhat it doesn't say in the docs is that you need to define the internal vnet IP as well. These are the same values that was defined when doing the configuration. The default values on XKS AKS is 169.254.20.10 and 10.0.0.10 and on AWS it's 169.254.20.10 and 172.20.0.10. The needed networkpolicy exist by default in all the tenant namespaces and is called default-deny and is managed by terraform. To view the rule run: kubectl get networkpolicies default-deny -n &lt;tenant&gt; "},{"title":"Azure​","type":1,"pageTitle":"Networking","url":"docs/xks/operator-guide/networking#azure","content":"XKS in Azure uses a single VNET with a single subnet per AKS cluster. The VNET and subnets are created by the core module. Additionally each AKS cluster also creates a load balancer. The load balancer is used for both ingress and egress traffic. When a Kubernetes service of type LoadBalancer is created a new IP is attached to the load balancer. An Azure load balancer can have multiple IPs attached to it so unlike AWS it does not have to create a new load balancer. During the creation of the AKS cluster a public IP prefix is attached to the load balancer for egress traffic. This ensures that all traffic egress with the same source IP, enabling the use of IP white listing in external sources. This does however mean that all outbound traffic will also go through the same load balancer as the incoming traffic. There is currently work underway to enable the use of managed NAT gateways for egress traffic in AKS, but it is currently in preview right now. "},{"title":"SNAT Exhaustion​","type":1,"pageTitle":"Networking","url":"docs/xks/operator-guide/networking#snat-exhaustion","content":"Applications making large numbers of outgoing TCP or UDP connections to the same IP and port can cause an issue known as SNAT port exhaustion. This is mostly due to the network architecture in Azure and AKS. All of the outgoing traffic from AKS goes through the load balancer, and for each outgoing request the load balancer needs to allocate an SNAT port to receive the response. Each Azure load balancer will allocate 64000 SNAT ports. This may seem like a lot, but there is a caveat as AKS will limit the amount of SNAT ports per node. The amount of SNAT ports available per node depends on the amount of nodes per cluster. Node Count\tSNAT Ports per Node1-50\t1024 51-100\t512 101-200\t256 201-400\t128 401-800\t64 801-1000\t32 A symptom of exhausting the SNAT ports is that outgoing requests will just fail. This is of course not a good situation, and may be hard to debug as a failing request could be caused by many different factors. Links​ https://docs.microsoft.com/en-us/azure/aks/load-balancer-standard#troubleshooting-snathttps://docs.microsoft.com/en-us/azure/load-balancer/troubleshoot-outbound-connectionhttps://www.danielstechblog.io/detecting-snat-port-exhaustion-on-azure-kubernetes-service/https://medium.com/asos-techblog/an-aks-performance-journey-part-1-sizing-everything-up-ee6d2346ea99https://medium.com/asos-techblog/an-aks-performance-journey-part-2-networking-it-out-e253f5bb4f69 "},{"title":"AWS​","type":1,"pageTitle":"Networking","url":"docs/xks/operator-guide/networking#aws","content":"TBD "},{"title":"AKS","type":0,"sectionRef":"#","url":"docs/xks/operator-guide/kubernetes/aks","content":"","keywords":""},{"title":"System Node Pool​","type":1,"pageTitle":"AKS","url":"docs/xks/operator-guide/kubernetes/aks#system-node-pool","content":"AKS requires the configuration of a system node pool when creating a cluster. This system node pool is not like the other additional node pools. It is tightly coupled to the AKS cluster. It is not possible without manual intervention to change the instance type or taints on this node pool without recreating the cluster. Additionally the system node pool cannot scale down to zero, for AKS to work there has to be at least one instance present. This is because critical system pods like Tunnelfront and CoreDNS will by default run on the system node pool. For more information about AKS system node pool refer to the official documentation. XKS follows the Azure recommendation and runs only system critical applications on the system node pool. Doing this protects services like CoreDNS from starvation or memory issues caused by user applications running on the same nodes. This is achieved by adding the taint CriticalAddonsOnly to all of the system nodes. "},{"title":"Sizing Nodes​","type":1,"pageTitle":"AKS","url":"docs/xks/operator-guide/kubernetes/aks#sizing-nodes","content":"Smaller AKS clusters can survive with a single node as the load on the system applications will be moderately low. In larger clusters and production clusters it is recommended to run at least three system nodes that may be larger in size. This section aims to describe how to properly size the system nodes. The minimum requirement for a system node is a VM with at least 2 vCPUs and 4GB of memory. Burstable B series VMs are not recommended. A good starting point for all clusters are the D series node types which have a balance of CPU and memory resources. A good starting point is a node of type Standard_D2as_v4. More work has to be done in this area regarding sizing and scaling of the system node pools to achieve a standardized solution. "},{"title":"Modifying Nodes​","type":1,"pageTitle":"AKS","url":"docs/xks/operator-guide/kubernetes/aks#modifying-nodes","content":"There may come times when Terraform wants to recreate the AKS cluster when the system node pool has been updated. This happens when updating certain properties in the system node pool. It is still possible to do these updates without recreating the cluster, but it requires some manual intervention. AKS requires at least one system node pool but does not have an upper limit. This makes it possible to manually add a new temporary system node pool. Remove the existing default node pool created by Terraform. Create a new system node pool with the same name but with the updated parameters. Finally remove the temporary node pool. Terraform will just assume that the changes have already been applied and import the new state without any other complaints. Start off with creating a temporary system pool. Make sure to replace the cluster name and resource groups to the correct values. az aks nodepool add --cluster-name aks-dev-we-aks1 --resource-group rg-dev-we-aks --name temp --mode &quot;System&quot; --node-count 1 Copy It may not be possible to create a new node pool with the current Kubernetes version if the cluster has not been updated in a while. Azure will remove minor versions as new versions are released. In that case you will need to upgrade the cluster to the latest minor version before making changes to the system pool, as AKS will not allow a node with a newer version than the control plane. Delete the system node pool created by Terraform: az aks nodepool delete --cluster-name aks-dev-we-aks1 --resource-group rg-dev-we-aks --name default Copy Create a new node pool with the new configuration. In this case it is setting a new instance type and adding a taint: az aks nodepool add --cluster-name aks-dev-we-aks1 --resource-group rg-dev-we-aks --name default --mode &quot;System&quot; --zones 1 2 3 --node-vm-size &quot;Standard_D2as_v4&quot; --node-taints &quot;CriticalAddonsOnly=true:NoSchedule&quot; --node-count 1 Copy Delete the temporary pool: az aks nodepool delete --cluster-name aks-dev-we-aks1 --resource-group rg-dev-we-aks --name temp Copy For additional information about updating the system nodes refer to this blog post. "},{"title":"Update AKS cluster​","type":1,"pageTitle":"AKS","url":"docs/xks/operator-guide/kubernetes/aks#update-aks-cluster","content":""},{"title":"Useful commands in Kubernetes​","type":1,"pageTitle":"AKS","url":"docs/xks/operator-guide/kubernetes/aks#useful-commands-in-kubernetes","content":"When patching an AKS cluster or just upgrading nodes it can be useful to watch your resources in Kubernetes. # Show node version kubectl get nodes -o jsonpath='{range .items[*]}{.metadata.name}{&quot;\\t&quot;}{.metadata.labels.kubernetes\\.azure\\.com\\/node-image-version}{&quot;\\n&quot;}{end}' # Watch nodes watch kubectl get nodes # Check the status of all pods in the cluster kubectl get pods -A Copy "},{"title":"Terraform update Kubernetes version​","type":1,"pageTitle":"AKS","url":"docs/xks/operator-guide/kubernetes/aks#terraform-update-kubernetes-version","content":"TBD "},{"title":"CLI update Kubernetes version​","type":1,"pageTitle":"AKS","url":"docs/xks/operator-guide/kubernetes/aks#cli-update-kubernetes-version","content":"export RG=rg1 export POOL_NAME=default export CLUSTER_NAME=cluster1 export AZURE_LOCATION=westeurope export KUBE_VERSION=1.21.9 Copy What AKS versions can I pick in this Azure location: az aks get-versions --location $AZURE_LOCATION -o table Copy az aks get-upgrades --resource-group $RG --name $CLUSTER_NAME --output table Copy We recommend to only upgrade control-plane separately and then upgrade the nodes. az aks upgrade --resource-group $RG --name $CLUSTER_NAME --kubernetes-version $KUBE_VERSION --control-plane-only Copy az aks nodepool upgrade --resource-group $RG --cluster-name $CLUSTER_NAME --name $POOL_NAME --kubernetes-version $KUBE_VERSION Copy "},{"title":"Upgrading node pools without upgrading cluster​","type":1,"pageTitle":"AKS","url":"docs/xks/operator-guide/kubernetes/aks#upgrading-node-pools-without-upgrading-cluster","content":"From time to time you might want to upgrade your Node Pools without upgrading the Kubernetes version. We always recommend to look at the official documentationas well. The node pool will spin up a new node and drain the existing one. When this is done the old node will be deleted. The below command works great for smaller clusters. If you want to upgrade more nodes faster it is possible to do so. Read the documentation for more information. export RG=rg1 export POOL_NAME=default export CLUSTER_NAME=cluster1 Copy Get the latest available node versions for your node pool: az aks nodepool get-upgrades --nodepool-name $POOL_NAME --cluster-name $CLUSTER_NAME --resource-group $RG Copy Upgrade the image on the specified node pool: az aks nodepool upgrade --resource-group $RG --cluster-name $CLUSTER_NAME --name $POOL_NAME --node-image-only Copy "},{"title":"Change vm size through Terraform​","type":1,"pageTitle":"AKS","url":"docs/xks/operator-guide/kubernetes/aks#change-vm-size-through-terraform","content":"If you want to use terraform to change the your node pools VM size you can't just change the vm_size in the additional_node_pools config. This will tell Azure to drain all the nodes and then delete the existing ones, then Azure will spin up a new node pool after the existing one is gone. This might be fine if you already have multiple additional node pools and you pods don't have specific node affinities. But if that isn't the case terraform will most likely run for ever since it won't be able to destroy the nodes that you already have workload on. Or even worse it will destroy the existing node and you won't have any node pools in your cluster to manage your workloads. Instead you have to add a second additional node pool in to your cluster. For example:  additional_node_pools = [ { name = &quot;standard&quot; orchestrator_version = &quot;1.21.2&quot; vm_size = &quot;Standard_E2s_v4&quot; min_count = 1 max_count = 5 node_labels = {} node_taints = [] spot_enabled = false spot_max_price = null }, { name = &quot;standard2&quot; orchestrator_version = &quot;1.21.2&quot; vm_size = &quot;Standard_F4s_v2&quot; min_count = 1 max_count = 5 node_labels = {} node_taints = [] spot_enabled = false spot_max_price = null } ] Copy Run terraform and see that standard2 is up and running. Now you can remove the standard node pool and standard2 should be able to handle the new load. Azure will automatically drain all the data from the old standard node pool. Remember to set min_count so that your current workload fits, you can always reduce min_count later. The cluster autoscaler will scale up new vm:s of standard2 but it will take time. During the creation of more standard2 nodes much of your workload might become pending. "},{"title":"AKS resources​","type":1,"pageTitle":"AKS","url":"docs/xks/operator-guide/kubernetes/aks#aks-resources","content":"To get a quick overview of what is happening in AKS you can look at its changelog. "},{"title":"VMSS inject commands​","type":1,"pageTitle":"AKS","url":"docs/xks/operator-guide/kubernetes/aks#vmss-inject-commands","content":"In Azure you can inject commands to VMSS instances using the Azure Linux VM agent. All you need to do is to get the RG of the VMSS, the node pool name and the instance ID. You can find the instance ID by running az vmss list-instances -g rg1 -n nodepool1 and look for instanceId. az vmss run-command invoke -g &lt;RG&gt; -n &lt;node-pool-name&gt; --command-id RunShellScript --instance-id &lt;instance id&gt; --scripts &quot; nc -vz mcr.microsoft.com 443 &quot; # example az vmss run-command invoke -g rg1 -n nodepool1 --command-id RunShellScript --instance-id 1 --scripts &quot; nc -vz mcr.microsoft.com 443 &quot; Copy "}]